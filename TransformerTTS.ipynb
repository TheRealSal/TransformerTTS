{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zatRhfCf-Wq4"
   },
   "source": [
    "# TransformerTTS: Neural Speech Synthesis with Transformer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxMuDjEm-Rqx"
   },
   "source": [
    "**Salman Sami Hussain Ali 40161786**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper presents the evolution of text-to-speech (TTS) technology, focusing on the transition from traditional concatenative and parametric methods to end-to-end neural network models like FastSpeech2 and Tacotron2. TransformerTTS models simplify the TTS pipeline by directly generating mel spectrograms from text inputs and then synthesizing audio using vocoders like WaveNet.\n",
    "\n",
    "The paper proposes a novel end-to-end TTS model that combines the strengths of Tacotron2 and Transformer architecture. This model replaces the recurrent neural networks (RNNs) in Tacotron2 with multi-head attention mechanisms from the Transformer model. By doing so, it improves parallelization capabilities and addresses long-distance dependency issues.\n",
    "\n",
    "Experiments conducted with a professional speech dataset(LJSpeech) demonstrate that the proposed Transformer TTS model outperforms Tacotron2 in terms of subjective evaluation metrics like CMOS and MOS. Additionally, the Transformer TTS model accelerates the training process significantly compared to Tacotron2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LJSpeech is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.\n",
    "\n",
    "The texts were published between 1884 and 1964, and are in the public domain. The audio was recorded in 2016-17 by the LibriVox project and is also in the public domain.\n",
    "\n",
    "**File Format**\n",
    "Metadata is provided in transcripts.csv. This file consists of one record per line, delimited by the pipe character (0x7c). The fields are:\n",
    "\n",
    "ID: this is the name of the corresponding .wav file\n",
    "Transcription: words spoken by the reader (UTF-8)\n",
    "Normalized Transcription: transcription with numbers, ordinals, and monetary units expanded into full words (UTF-8).\n",
    "Each audio file is a single-channel 16-bit PCM WAV with a sample rate of 22050 Hz.\n",
    "\n",
    "After processing with ljspeech_prepare.py, the json dataset files include:\n",
    "- ID\n",
    "- Transcription\n",
    "- Normalized Transcription\n",
    "- Phonemes generated using GraphemeToPhoneme\n",
    "\n",
    "**Miscellaneous**\n",
    "The audio clips range in length from approximately 1 second to 10 seconds. They were segmented automatically based on silences in the recording. Clip boundaries generally align with sentence or clause boundaries, but not always.\n",
    "The text was matched to the audio manually, and a QA pass was done to ensure that the text accurately matched the words spoken in the audio.\n",
    "The original LibriVox recordings were distributed as 128 kbps MP3 files. As a result, they may contain artifacts introduced by the MP3 encoding.\n",
    "The following abbreviations appear in the text. They may be expanded as follows:\n",
    "\n",
    "| Abbreviation | Expansion   |\n",
    "|--------------|-------------|\n",
    "| Mr.          | Mister      |\n",
    "| Mrs.         | Misess (*)  |\n",
    "| Dr.          | Doctor      |\n",
    "| No.          | Number      |\n",
    "| St.          | Saint       |\n",
    "| Co.          | Company     |\n",
    "| Jr.          | Junior      |\n",
    "| Maj.         | Major       |\n",
    "| Gen.         | General     |\n",
    "| Drs.         | Doctors     |\n",
    "| Rev.         | Reverend    |\n",
    "| Lt.          | Lieutenant  |\n",
    "| Hon.         | Honorable   |\n",
    "| Sgt.         | Sergeant    |\n",
    "| Capt.        | Captain     |\n",
    "| Esq.         | Esquire     |\n",
    "| Ltd.         | Limited     |\n",
    "| Col.         | Colonel     |\n",
    "| Ft.          | Fort        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1JYfBLWSj0vCSz_uVniMuIYOW8T49oSGV\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "(Li, N. et al.)\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "The core module of this model is the Transformer proposed by Vaswami et Al.[2]. The Transformer model operates as a sequence-to-sequence network that relies exclusively on attention mechanisms, eliminating the need for recurrent and convolutional layers entirely. Studies have demonstrated the remarkable performance of the Transformer, surpassing numerous RNN-based models in neural machine translation (NMT). Its architecture comprises two main components: an encoder and a decoder, each constructed from stacks of multiple identity blocks. Within each encoder block, there are two subnetworks: a multi-head attention mechanism and a feed-forward network. Conversely, each decoder block includes an additional masked multi-head attention mechanism compared to the encoder block. Both encoder and decoder blocks feature residual connections and layer normalizations to facilitate effective information flow and training stability.\n",
    "\n",
    "<br>\n",
    "\n",
    "A description of each of the other modules are provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AveEbVMELhlJ",
    "tags": []
   },
   "source": [
    "## PyTorch Modules and Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7T0cMHHbmql",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Scaled Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcmsNbL3b-2v"
   },
   "source": [
    "Positional encoding for transformers is defined as follows:\n",
    "\\begin{align}\n",
    "PE(pos, 2i) &= \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\label{eq:pe1} \\\\\n",
    "PE(pos, 2i + 1) &= \\cos\\left(\\frac{pos}{10000^{(2i + 1)/d_{\\text{model}}}}\\right) \\label{eq:pe2}\n",
    "\\end{align}\n",
    "where $pos$ is the time step index, $2i$ and $2i+1$ are the channel indices, and $d_{\\text{model}}$ is the vector dimension of each frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DrTQJu4byR-"
   },
   "source": [
    "We employ these triangle positional embeddings with a trainable weight, so that these embedding can\n",
    "adaptively fit the scales of both encoder and decoder prenets’ output, as shown in Eq. 8:\n",
    "xi = prenet(phoneme_i\n",
    ") + αP E(i) (8)\n",
    "where α is the trainable weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHN17ZEWKVQw"
   },
   "source": [
    "The following is an adaptation of the PositionalEncoding class in speechbrain.lobes.models.transformer.Transformer, but adapted to include the learnable weight α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6loqPjTbx3C"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"This class implements the absolute sinusoidal positional encoding function.\n",
    "    PE(pos, 2i)   = sin(pos/(10000^(2i/dmodel)))\n",
    "    PE(pos, 2i+1) = cos(pos/(10000^(2i/dmodel)))\n",
    "\n",
    "    Based on Cornell & Zhong's implementation in Transformer.py\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    input_size: int\n",
    "        Embedding dimension.\n",
    "    max_len : int, optional\n",
    "        Max length of the input sequences (default 2500).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> a = torch.rand((8, 120, 512))\n",
    "    >>> enc = PositionalEncoding(input_size=a.shape[-1])\n",
    "    >>> b = enc(a)\n",
    "    >>> b.shape\n",
    "    torch.Size([1, 120, 512])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, max_len=2500):\n",
    "        super().__init__()\n",
    "        if input_size % 2 != 0:\n",
    "            raise ValueError(\n",
    "                f\"Cannot use sin/cos positional encoding with odd channels (got channels={input_size})\"\n",
    "            )\n",
    "        self.max_len = max_len\n",
    "        pe = torch.zeros(self.max_len, input_size, requires_grad=False)\n",
    "        positions = torch.arange(0, self.max_len).unsqueeze(1).float()\n",
    "        denominator = torch.exp(\n",
    "            torch.arange(0, input_size, 2).float()\n",
    "            * -(math.log(10000.0) / input_size)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(positions * denominator)\n",
    "        pe[:, 1::2] = torch.cos(positions * denominator)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "        # Define learnable scaling parameter\n",
    "        self.alpha = nn.Parameter(torch.Tensor(1))  # 1-dimensional tensor\n",
    "\n",
    "        # Initialize alpha parameter\n",
    "        nn.init.normal_(self.alpha)  # Initialize with random values\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Input feature shape (batch, time, fea)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The positional encoding.\n",
    "        \"\"\"\n",
    "        scaled_pos_embedding = self.alpha * self.pe[:, : x.size(1)].clone().detach()\n",
    "        return scaled_pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KS8G6bH89x1L",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Linear and Convolutional Normalization Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqCK1ch692or"
   },
   "outputs": [],
   "source": [
    "class LinearNorm(torch.nn.Module):\n",
    "    \"\"\"A linear layer with Xavier initialization\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_dim: int\n",
    "        the input dimension\n",
    "    out_dim: int\n",
    "        the output dimension\n",
    "    bias: bool\n",
    "        whether or not to use a bias\n",
    "    w_init_gain: linear\n",
    "        the weight initialization gain type (see torch.nn.init.calculate_gain)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.Tacotron2 import LinearNorm\n",
    "    >>> layer = LinearNorm(in_dim=5, out_dim=3)\n",
    "    >>> x = torch.randn(3, 5)\n",
    "    >>> y = layer(x)\n",
    "    >>> y.shape\n",
    "    torch.Size([3, 3])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain=\"linear\"):\n",
    "        super().__init__()\n",
    "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.linear_layer.weight,\n",
    "            gain=torch.nn.init.calculate_gain(w_init_gain),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            a (batch, features) input tensor\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the linear layer output\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2z3iFiGI95VT"
   },
   "outputs": [],
   "source": [
    "class ConvNorm(torch.nn.Module):\n",
    "    \"\"\"A 1D convolution layer with Xavier initialization\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_channels: int\n",
    "        the number of input channels\n",
    "    out_channels: int\n",
    "        the number of output channels\n",
    "    kernel_size: int\n",
    "        the kernel size\n",
    "    stride: int\n",
    "        the convolutional stride\n",
    "    padding: int\n",
    "        the amount of padding to include. If not provided, it will be calculated\n",
    "        as dilation * (kernel_size - 1) / 2\n",
    "    dilation: int\n",
    "        the dilation of the convolution\n",
    "    bias: bool\n",
    "        whether or not to use a bias\n",
    "    w_init_gain: linear\n",
    "        the weight initialization gain type (see torch.nn.init.calculate_gain)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.Tacotron2 import ConvNorm\n",
    "    >>> layer = ConvNorm(in_channels=10, out_channels=5, kernel_size=3)\n",
    "    >>> x = torch.randn(3, 10, 5)\n",
    "    >>> y = layer(x)\n",
    "    >>> y.shape\n",
    "    torch.Size([3, 5, 5])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=None,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "            w_init_gain=\"linear\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            assert kernel_size % 2 == 1\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain)\n",
    "        )\n",
    "\n",
    "    def forward(self, signal):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        signal: torch.Tensor\n",
    "            the input to the convolutional layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        return self.conv(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC9J7YQuMAeX",
    "tags": []
   },
   "source": [
    "#### Pre-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfgSjglvMDjB",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Encoder Pre-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Yfej3PLOjL9"
   },
   "source": [
    "In our Transformer TTS model, we\n",
    "input the phoneme sequence into the same network, which\n",
    "is called ”encoder pre-net”. Each phoneme has a trainable\n",
    "embedding of 512 dims, and the output of each convolution\n",
    "layer has 512 channels, followed by a batch normalization\n",
    "and ReLU activation, and a dropout layer as well. In addition, we add a linear projection after the final ReLU activation, since the output range of ReLU is [0, +∞), while\n",
    "each dimension of these triangle positional embeddings is in\n",
    "[−1, 1]. Adding 0-centered positional information onto nonnegative embeddings will result in a fluctuation not centered\n",
    "on the origin and harm model performance, which will be\n",
    "demonstrated in our experiment. Hence we add a linear projection for center consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS4-MC9MRKxc"
   },
   "source": [
    "Block containing the sequence of Conv, Batch Normalization, ReLU, and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7Ulb5EwREJH"
   },
   "outputs": [],
   "source": [
    "class EncoderPreNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        A block for preprocessing text inputs in an encoder network, consisting of a 1D convolutional layer followed by batch normalization, ReLU activation, dropout, and linear transformation.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        in_dim: int\n",
    "            Number of input channels.\n",
    "        kernel_size: int\n",
    "            Size of the convolutional kernel.\n",
    "        stride: int\n",
    "            Stride of the convolution.\n",
    "        padding: int, optional\n",
    "            Amount of padding to include. If not provided, it will be calculated as dilation * (kernel_size - 1) / 2.\n",
    "        dilation: int, optional\n",
    "            Dilation factor of the convolution.\n",
    "        bias: bool, optional\n",
    "            Whether to include bias in the convolutional layer.\n",
    "        dropout: float, optional\n",
    "            Dropout probability.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> import torch\n",
    "        >>> from speechbrain.lobes.models.TransformerTTS import EncoderPreNetBlock\n",
    "        >>> block = EncoderPreNetBlock(in_dim=10, kernel_size=3)\n",
    "        >>> x = torch.randn(3, 10, 5)  # Input tensor shape: (batch_size, in_dim, sequence_length)\n",
    "        >>> y = block(x)\n",
    "        >>> y.shape  # Output shape: (batch_size, in_dim, sequence_length)\n",
    "        torch.Size([3, 10, 5])\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim=512,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=None,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "            dropout=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            assert kernel_size % 2 == 1\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = ConvNorm(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "\n",
    "        self.batch = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        text: torch.Tensor\n",
    "            the input to the convolutional layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        out = self.conv(text)\n",
    "        out = self.batch(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIi7ws_RMAKY"
   },
   "outputs": [],
   "source": [
    "class EncoderPrenet(nn.Module):\n",
    "    \"\"\"The TransformerTTS pre-net module consisting of a specified number of\n",
    "    normalized (Xavier-initialized) linear layers\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_dim: int\n",
    "        the input dimensions\n",
    "    sizes: int\n",
    "        the dimension of the hidden layers/output\n",
    "    dropout: float\n",
    "        the dropout probability\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.TransformerTTS import EncoderPrenet\n",
    "    >>> layer = Prenet()\n",
    "    >>> x = torch.randn(862, 2, 80)\n",
    "    >>> output = layer(x)\n",
    "    >>> output.shape\n",
    "    torch.Size([862, 2, 256])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_dim=512,\n",
    "            hidden_dim=256,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=None,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "            num_layers=3,\n",
    "            dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderPreNetBlock(emb_dim, kernel_size, stride, padding, dilation, bias) for i in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(emb_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass for the prenet\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            the prenet inputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vTV4TEGSB_d",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Decoder Pre-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mel spectrogram is initially processed by a neural network called \"decoder pre-net,\" which consists of two fully connected layers, each with 256 hidden units and ReLU activation. Trainable embeddings are used for phonemes, allowing their subspace to adapt, while the subspace for mel spectrograms remains fixed. The decoder pre-net is responsible for mapping mel spectrograms into the same subspace as phoneme embeddings. This alignment enables the attention mechanism to measure the similarity between phonemes and mel frames. Additionally, an extra linear projection is employed to ensure center consistency and to achieve the same dimensionality as the triangular positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Oz_FApCSPbN"
   },
   "outputs": [],
   "source": [
    "class DecoderPrenet(nn.Module):\n",
    "    \"\"\"The TransformerTTS pre-net module consisting of a specified number of\n",
    "        normalized (Xavier-initialized) linear layers\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        in_dim: int\n",
    "            the input dimensions\n",
    "        sizes: int\n",
    "            the dimension of the hidden layers/output\n",
    "        dropout: float\n",
    "            the dropout probability\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> import torch\n",
    "        >>> from speechbrain.lobes.models.TransformerTTS import DecoderPrenet\n",
    "        >>> layer = DecoderPrenet()\n",
    "        >>> x = torch.randn(862, 2, 80)\n",
    "        >>> output = layer(x)\n",
    "        >>> output.shape\n",
    "        torch.Size([862, 2, 256])\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mel_dims=80,\n",
    "            hidden_dims=256,\n",
    "            d_model=512,\n",
    "            dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(mel_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Linear layer used to project the targets to the same dimensional space as the phoneme's embeddings\n",
    "        self.linear = nn.Linear(hidden_dims, d_model)\n",
    "\n",
    "    def forward(self, mel):\n",
    "        \"\"\"Computes the forward pass for the prenet\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        mel: torch.Tensor\n",
    "            the mel spectrogram inputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        out = self.layers(mel)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xlPrtCY9NYF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Post-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as Tacotron2, we use two different linear projections to predict the mel spectrogram and the stop token respectively, and use a 5-layer CNN to produce a residual to refine the reconstruction of the mel spectrogram. For the stop linear, there is only one positive sample at the end of each sequence which means ”stop”, while hundreds of negative samples for other frames. This imbalance may result in unstoppable inference.\n",
    "\n",
    "This issue is fixed by giving the stop token a positive weight (5.0 ~ 8.0), which is done in the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrPBSd719Pgy"
   },
   "outputs": [],
   "source": [
    "class Postnet(nn.Module):\n",
    "    \"\"\"The TransformerTTS postnet consists of a number of 1-d convolutional layers\n",
    "    with Xavier initialization and a tanh activation, with batch normalization.\n",
    "    Depending on configuration, the postnet may either refine the MEL spectrogram\n",
    "    or upsample it to a linear spectrogram. It has the same architecture as Tacotron2's Post-Net\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    n_mel_channels: int\n",
    "        the number of MEL spectrogram channels\n",
    "    postnet_embedding_dim: int\n",
    "        the postnet embedding dimension\n",
    "    postnet_kernel_size: int\n",
    "        the kernel size of the convolutions within the decoders\n",
    "    postnet_n_convolutions: int\n",
    "        the number of convolutions in the postnet\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.TransformerTTS import Postnet\n",
    "    >>> layer = Postnet()\n",
    "    >>> x = torch.randn(2, 80, 861)\n",
    "    >>> output = layer(x)\n",
    "    >>> output.shape\n",
    "    torch.Size([2, 80, 861])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_mel_channels=80,\n",
    "            postnet_embedding_dim=512,\n",
    "            postnet_kernel_size=5,\n",
    "            postnet_n_convolutions=5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(\n",
    "                    n_mel_channels,\n",
    "                    postnet_embedding_dim,\n",
    "                    kernel_size=postnet_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=int((postnet_kernel_size - 1) / 2),\n",
    "                    dilation=1,\n",
    "                    w_init_gain=\"tanh\",\n",
    "                ),\n",
    "                nn.BatchNorm1d(postnet_embedding_dim),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for i in range(1, postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    ConvNorm(\n",
    "                        postnet_embedding_dim,\n",
    "                        postnet_embedding_dim,\n",
    "                        kernel_size=postnet_kernel_size,\n",
    "                        stride=1,\n",
    "                        padding=int((postnet_kernel_size - 1) / 2),\n",
    "                        dilation=1,\n",
    "                        w_init_gain=\"tanh\",\n",
    "                    ),\n",
    "                    nn.BatchNorm1d(postnet_embedding_dim),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(\n",
    "                    postnet_embedding_dim,\n",
    "                    n_mel_channels,\n",
    "                    kernel_size=postnet_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=int((postnet_kernel_size - 1) / 2),\n",
    "                    dilation=1,\n",
    "                    w_init_gain=\"linear\",\n",
    "                ),\n",
    "                nn.BatchNorm1d(n_mel_channels),\n",
    "            )\n",
    "        )\n",
    "        self.n_convs = len(self.convolutions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass of the postnet\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            the postnet input (usually a MEL spectrogram)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the postnet output (a refined MEL spectrogram or a\n",
    "            linear spectrogram depending on how the model is\n",
    "            configured)\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        for conv in self.convolutions:\n",
    "            if i < self.n_convs - 1:\n",
    "                x = F.dropout(torch.tanh(conv(x)), 0.5, training=self.training)\n",
    "            else:\n",
    "                x = F.dropout(conv(x), 0.5, training=self.training)\n",
    "            i += 1\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acA5yMI2So75",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### TransformerTTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsb8LEf8S1x1"
   },
   "source": [
    "The main module that brings everything together. It is the main entry point for the model, which is responsible for instantiating all submodules. \n",
    "\n",
    "Its simplified structure is: input -> phonemes, mel -> phoneme embedding -> encoder & decoder pre-net -> scaled positional embedding -> encoder & decoder(nn.Transformer) -> post-net -> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXBaxumqSuoL"
   },
   "outputs": [],
   "source": [
    "class TransformerTTS(nn.Module):\n",
    "    \"\"\"The Transformer text-to-speech model, based on the NVIDIA implementation.\n",
    "\n",
    "    This class is the main entry point for the model, which is responsible\n",
    "    for instantiating all submodules, which, in turn, manage the individual\n",
    "    neural network layers\n",
    "\n",
    "    Simplified STRUCTURE: input->word embedding ->encoder ->attention \\\n",
    "    ->decoder(+prenet) -> postnet ->output\n",
    "\n",
    "    prenet(input is decoder previous time step) output is input to decoder\n",
    "    concatenated with the attention output\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    mask_padding: bool\n",
    "        whether or not to mask pad-outputs of TransformerTTS\n",
    "    n_mel_channels: int\n",
    "        number of mel channels for constructing spectrogram\n",
    "    n_symbols:  int=128\n",
    "        number of accepted char symbols defined in textToSequence\n",
    "    symbols_embedding_dim: int\n",
    "        number of embedding dimension for symbols fed to nn.Embedding\n",
    "    encoder_prenet_kernel_size: int\n",
    "        size of kernel processing the embeddings\n",
    "    encoder_prenet_n_convolutions: int\n",
    "        number of convolution layers in encoder\n",
    "    encoder_prenet_embedding_dim: int\n",
    "        number of kernels in encoder, this is also the dimension\n",
    "        of the bidirectional LSTM in the encoder\n",
    "    n_frames_per_step: int=1\n",
    "        only 1 generated mel-frame per step is supported for the decoder as of now.\n",
    "    decoder_rnn_dim: int\n",
    "        number of 2 unidirectional stacked LSTM units\n",
    "    prenet_dim: int\n",
    "        dimension of linear prenet layers\n",
    "    max_decoder_steps: int\n",
    "        maximum number of steps/frames the decoder generates before stopping\n",
    "    gate_threshold: int\n",
    "        cut off level any output probability above that is considered\n",
    "        complete and stops generation so we have variable length outputs\n",
    "    p_attention_dropout: float\n",
    "        attention drop out probability\n",
    "    p_decoder_dropout: float\n",
    "        decoder drop  out probability\n",
    "    postnet_embedding_dim: int\n",
    "        number os postnet dfilters\n",
    "    postnet_kernel_size: int\n",
    "        1d size of posnet kernel\n",
    "    postnet_n_convolutions: int\n",
    "        number of convolution layers in postnet\n",
    "    decoder_no_early_stopping: bool\n",
    "        determines early stopping of decoder\n",
    "        along with gate_threshold . The logical inverse of this is fed to the decoder\n",
    "    paddig_idx: int\n",
    "        represents the number given to the padding\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> _ = torch.manual_seed(213312)\n",
    "    >>> from speechbrain.lobes.models.TransformerTTS import TransformerTTS\n",
    "    >>> model = TransformerTTS(\n",
    "    ...    mask_padding=True,\n",
    "    ...    n_mel_channels=80,\n",
    "    ...    n_symbols=148,\n",
    "    ...    symbols_embedding_dim=512,\n",
    "    ...    encoder_prenet_kernel_size=5,\n",
    "    ...    encoder_prenet_n_convolutions=3,\n",
    "    ...    encoder_prenet_embedding_dim=512,\n",
    "    ...    attention_rnn_dim=1024,\n",
    "    ...    attention_dim=128,\n",
    "    ...    attention_location_n_filters=32,\n",
    "    ...    attention_location_kernel_size=31,\n",
    "    ...    n_frames_per_step=1,\n",
    "    ...    decoder_rnn_dim=1024,\n",
    "    ...    prenet_dim=256,\n",
    "    ...    max_decoder_steps=32,\n",
    "    ...    gate_threshold=0.5,\n",
    "    ...    p_attention_dropout=0.1,\n",
    "    ...    p_decoder_dropout=0.1,\n",
    "    ...    postnet_embedding_dim=512,\n",
    "    ...    postnet_kernel_size=5,\n",
    "    ...    postnet_n_convolutions=5,\n",
    "    ...    decoder_no_early_stopping=False,\n",
    "    ...    padding_idx=0,\n",
    "    ... )\n",
    "    >>> _ = model.eval()\n",
    "    >>> inputs = torch.tensor([\n",
    "    ...     [13, 12, 31, 14, 19],\n",
    "    ...     [31, 16, 30, 31, 0],\n",
    "    ... ])\n",
    "    >>> input_lengths = torch.tensor([5, 4])\n",
    "    >>> outputs, output_lengths = model.infer(inputs, input_lengths)\n",
    "    >>> outputs.shape, output_lengths.shape\n",
    "    (torch.Size([2, 80, 1]), torch.Size([2]))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mask_padding=True,\n",
    "            # mel generation parameter in data io\n",
    "            n_mel_channels=80,\n",
    "            # symbols\n",
    "            n_symbols=52,\n",
    "            symbols_embedding_dim=512,\n",
    "            # Encoder Pre-Net parameters\n",
    "            encoder_prenet_kernel_size=5,\n",
    "            encoder_prenet_n_convolutions=3,\n",
    "            encoder_prenet_padding=None,\n",
    "            encoder_prenet_dilation=1,\n",
    "            encoder_prenet_bias=True,\n",
    "            encoder_prenet_dropout=0.5,\n",
    "            encoder_prenet_stride=1,\n",
    "            # Decoder Pre-Net parameters\n",
    "            n_frames_per_step=1,\n",
    "            decoder_prenet_hidden_dims=256,\n",
    "            decoder_prenet_dropout=0.15,\n",
    "            # Transformer Parameters\n",
    "            d_model=256,\n",
    "            transformer_nhead=8,\n",
    "            transformer_num_encoder_layers=6,\n",
    "            transformer_num_decoder_layers=6,\n",
    "            transformer_d_ffn=2048,\n",
    "            transformer_dropout=0.1,\n",
    "            transformer_activation=\"relu\",\n",
    "            custom_encoder_module=None,\n",
    "            custom_decoder_module=None,\n",
    "            batch_first=False,\n",
    "            norm_first=False,\n",
    "            layer_norm_eps=1e-5,\n",
    "            # Mel-post processing network parameters\n",
    "            postnet_embedding_dim=512,\n",
    "            postnet_kernel_size=5,\n",
    "            postnet_n_convolutions=5,\n",
    "            gate_threshold=0.5,\n",
    "            max_decoder_steps=32,\n",
    "            early_stopping=False,\n",
    "            padding_idx=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask_padding = mask_padding\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "        self.max_decoder_steps = max_decoder_steps\n",
    "        self.early_stopping = early_stopping\n",
    "\n",
    "        self.gate_threshold = gate_threshold\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(n_symbols, symbols_embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        std = sqrt(2.0 / (n_symbols + d_model))\n",
    "        val = sqrt(3.0) * std  # uniform bounds for std\n",
    "        self.encoder_embedding.weight.data.uniform_(-val, val)\n",
    "\n",
    "        if custom_encoder_module is None:\n",
    "            encoder_block = torch.nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                             nhead=transformer_nhead,\n",
    "                                                             dim_feedforward=transformer_d_ffn,\n",
    "                                                             dropout=transformer_dropout,\n",
    "                                                             activation=transformer_activation,\n",
    "                                                             batch_first=batch_first,\n",
    "                                                             norm_first=norm_first,\n",
    "                                                             layer_norm_eps=layer_norm_eps)\n",
    "        else:\n",
    "            encoder_block = custom_encoder_module\n",
    "\n",
    "        if custom_decoder_module is None:\n",
    "            decoder_block = torch.nn.TransformerDecoderLayer(d_model=d_model,\n",
    "                                                             nhead=transformer_nhead,\n",
    "                                                             dim_feedforward=transformer_d_ffn,\n",
    "                                                             dropout=transformer_dropout,\n",
    "                                                             activation=transformer_activation,\n",
    "                                                             batch_first=batch_first,\n",
    "                                                             norm_first=norm_first,\n",
    "                                                             layer_norm_eps=layer_norm_eps)\n",
    "        else:\n",
    "            decoder_block = custom_decoder_module\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_block, num_layers=transformer_num_encoder_layers)\n",
    "\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_block, num_layers=transformer_num_decoder_layers)\n",
    "\n",
    "        self.encoder_prenet = EncoderPrenet(\n",
    "            emb_dim=symbols_embedding_dim,\n",
    "            hidden_dim=d_model,\n",
    "            kernel_size=encoder_prenet_kernel_size,\n",
    "            num_layers=encoder_prenet_n_convolutions,\n",
    "            stride=encoder_prenet_stride,\n",
    "            padding=encoder_prenet_padding,\n",
    "            dilation=encoder_prenet_dilation,\n",
    "            bias=encoder_prenet_bias,\n",
    "            dropout=encoder_prenet_dropout)\n",
    "\n",
    "        self.decoder_prenet = DecoderPrenet(\n",
    "            mel_dims=n_mel_channels,\n",
    "            hidden_dims=decoder_prenet_hidden_dims,\n",
    "            dropout=decoder_prenet_dropout,\n",
    "            d_model=d_model\n",
    "        )\n",
    "\n",
    "        self.postnet = Postnet(\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            postnet_embedding_dim=postnet_embedding_dim,\n",
    "            postnet_kernel_size=postnet_kernel_size,\n",
    "            postnet_n_convolutions=postnet_n_convolutions,\n",
    "        )\n",
    "\n",
    "        self.encoder_positional_encoding = PositionalEncoding(input_size=d_model)\n",
    "        self.decoder_positional_encoding = PositionalEncoding(input_size=d_model)\n",
    "\n",
    "        self.mel_linear = LinearNorm(d_model, n_mel_channels)\n",
    "        self.stop_linear = LinearNorm(d_model, 1, w_init_gain='sigmoid')\n",
    "\n",
    "    def parse_output(self, outputs, output_lengths):\n",
    "        \"\"\"\n",
    "        Masks the padded part of output\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        outputs: list\n",
    "            a list of tensors - raw outputs\n",
    "        output_lengths: torch.Tensor\n",
    "            a tensor representing the lengths of all outputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs: torch.Tensor\n",
    "        mel_outputs_postnet: torch.Tensor\n",
    "        gate_outputs: torch.Tensor\n",
    "        \"\"\"\n",
    "        mel_outputs, mel_outputs_postnet, gate_outputs = outputs\n",
    "\n",
    "        if self.mask_padding and output_lengths is not None:\n",
    "            mask = get_mask_from_lengths(\n",
    "                output_lengths, max_len=mel_outputs.size(-1)\n",
    "            )\n",
    "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "\n",
    "            mel_outputs.clone().masked_fill_(mask, 0.0)\n",
    "            mel_outputs_postnet.masked_fill_(mask, 0.0)\n",
    "            gate_outputs.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
    "\n",
    "        return mel_outputs, mel_outputs_postnet, gate_outputs\n",
    "\n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs):\n",
    "        \"\"\"Prepares decoder outputs for output\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        mel_outputs: torch.Tensor\n",
    "            MEL-scale spectrogram outputs\n",
    "        gate_outputs: torch.Tensor\n",
    "            gate output energies\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs: torch.Tensor\n",
    "            MEL-scale spectrogram outputs\n",
    "        gate_outputs: torch.Tensor\n",
    "            gate output energies\n",
    "        \"\"\"\n",
    "        # (T_out, B) -> (B, T_out)\n",
    "        if gate_outputs.dim() == 1:\n",
    "            gate_outputs = gate_outputs.unsqueeze(0)\n",
    "        else:\n",
    "            gate_outputs = gate_outputs.transpose(0, 1).contiguous()\n",
    "\n",
    "        # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
    "        mel_outputs = mel_outputs.transpose(0, 1).contiguous()\n",
    "        # decouple frames per step\n",
    "        shape = (mel_outputs.shape[0], -1, self.n_mel_channels)\n",
    "        mel_outputs = mel_outputs.view(*shape)\n",
    "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "        return mel_outputs, gate_outputs\n",
    "\n",
    "    def get_go_frame(self, memory):\n",
    "        \"\"\"Gets all zeros frames to use as first decoder input\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        memory: torch.Tensor\n",
    "            decoder outputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        decoder_input: torch.Tensor\n",
    "            all zeros frames\n",
    "        \"\"\"\n",
    "        B = memory.size(0)\n",
    "        dtype = memory.dtype\n",
    "        device = memory.device\n",
    "        decoder_input = torch.zeros(\n",
    "            B,\n",
    "            self.n_mel_channels * self.n_frames_per_step,\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "        )\n",
    "        return decoder_input\n",
    "\n",
    "    def forward(self, inputs, masks):\n",
    "        \"\"\"Decoder forward pass for training\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        inputs: tuple\n",
    "            batch object\n",
    "        masks: tuple\n",
    "            contains the lookahead target mask, and the src and target key padding mask\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs: torch.Tensor\n",
    "            mel outputs from the decoder\n",
    "        mel_outputs_postnet: torch.Tensor\n",
    "            mel outputs from postnet\n",
    "        gate_outputs: torch.Tensor\n",
    "            gate outputs from the decoder\n",
    "        output_lengths: torch.Tensor\n",
    "            length of the output without padding\n",
    "        \"\"\"\n",
    "        inputs, input_lengths, mel_padded, max_len, mel_len = inputs  # (text_padded, input_lengths, mel_padded, max_len, output_lengths)\n",
    "        tgt_mask, src_mask, src_key_padding_mask, tgt_key_padding_mask = masks\n",
    "\n",
    "        # Generate Embeddings\n",
    "        embedded_inputs = self.encoder_embedding(inputs).transpose(1, 2)\n",
    "\n",
    "        # Pass through encoder pre-net\n",
    "        encoder_prenet_outputs = self.encoder_prenet(embedded_inputs)\n",
    "\n",
    "        # Pass through decoder pre-net\n",
    "        decoder_prenet_outputs = self.decoder_prenet(mel_padded.transpose(1, 2))\n",
    "\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs.transpose(0, 1)\n",
    "        decoder_prenet_outputs = decoder_prenet_outputs.transpose(0, 1)\n",
    "\n",
    "        # Add Scaled Positional Embeddings to encoder and decoder pre-nets\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs + self.encoder_positional_encoding(encoder_prenet_outputs)\n",
    "        decoder_prenet_outputs = decoder_prenet_outputs + self.decoder_positional_encoding(decoder_prenet_outputs)\n",
    "\n",
    "        # Input embedded phonemes into transformer's encoder\n",
    "        memory = self.transformer_encoder(\n",
    "            src=encoder_prenet_outputs,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        # Input memory and mel spectograms into transformer's decoder\n",
    "        mel_outputs = self.transformer_decoder(tgt=decoder_prenet_outputs,\n",
    "                                               memory=memory,\n",
    "                                               tgt_mask=tgt_mask,\n",
    "                                               tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                               memory_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # Calculate mel linear, mel stop, and post-net\n",
    "        # Stop Linear\n",
    "        stop_token = self.stop_linear(mel_outputs).squeeze()\n",
    "\n",
    "        # Mel Linear and Post-Net\n",
    "        mel_outputs = self.mel_linear(mel_outputs)\n",
    "\n",
    "        mel_outputs, stop_token = self.parse_decoder_outputs(mel_outputs, stop_token)\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, stop_token], mel_len)\n",
    "\n",
    "    def infer(self, inputs, input_lengths):\n",
    "        \"\"\"Produces outputs\n",
    "\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        inputs: torch.tensor\n",
    "            text or phonemes converted\n",
    "\n",
    "        input_lengths: torch.tensor\n",
    "            the lengths of input parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs_postnet: torch.Tensor\n",
    "            final mel output of TransformerTTS\n",
    "        mel_lengths: torch.Tensor\n",
    "            length of mels\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate Embeddings\n",
    "        embedded_inputs = self.encoder_embedding(inputs).transpose(1, 2)\n",
    "\n",
    "        # Pass through encoder pre-net\n",
    "        encoder_prenet_outputs = self.encoder_prenet(embedded_inputs)\n",
    "\n",
    "        # Add Scaled Positional Embeddings to encoder and decoder pre-nets\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs + self.encoder_positional_encoding(encoder_prenet_outputs)\n",
    "\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs.transpose(0, 1)\n",
    "\n",
    "        # Input embedded phonemes into transformer's encoder\n",
    "        src_key_padding_mask = get_mask_from_lengths(input_lengths).to(inputs.device, non_blocking=True)\n",
    "        memory = self.transformer_encoder(\n",
    "            src=encoder_prenet_outputs,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        decoder_input = self.get_go_frame(memory)\n",
    "\n",
    "        mask = get_mask_from_lengths(input_lengths)\n",
    "\n",
    "        mel_lengths = torch.zeros(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "        not_finished = torch.ones(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "\n",
    "        mel_outputs, gate_outputs = (\n",
    "            torch.zeros(1),\n",
    "            torch.zeros(1),\n",
    "        )\n",
    "\n",
    "        first_iter = True\n",
    "        while True:\n",
    "            decoder_input = self.decoder_prenet(decoder_input)\n",
    "            if len(decoder_input.shape) != 3:\n",
    "                decoder_input = decoder_input.unsqueeze(1)\n",
    "\n",
    "            decoder_output = self.transformer_decoder(memory=memory, tgt=decoder_input)\n",
    "\n",
    "            # Calculate mel linear and stop token\n",
    "            # Stop Linear\n",
    "            gate_output = self.stop_linear(decoder_output).squeeze()\n",
    "\n",
    "            # Mel Linear and Post-Net\n",
    "            mel_output = self.mel_linear(decoder_output)\n",
    "\n",
    "            if first_iter:\n",
    "                mel_outputs = mel_output\n",
    "                gate_outputs = gate_output\n",
    "                first_iter = False\n",
    "            else:\n",
    "                mel_outputs = torch.cat(\n",
    "                    (mel_outputs, mel_output), dim=0\n",
    "                )\n",
    "                gate_outputs = torch.cat((gate_outputs, gate_output), dim=0)\n",
    "\n",
    "            sigmoid = torch.sigmoid(gate_output).unsqueeze(1)\n",
    "\n",
    "            dec = (\n",
    "                torch.le(sigmoid, torch.tensor(self.gate_threshold))\n",
    "                .to(torch.int32)\n",
    "                .squeeze(1)\n",
    "            )\n",
    "\n",
    "            not_finished = not_finished * dec\n",
    "            mel_lengths += not_finished\n",
    "            if self.early_stopping and torch.sum(not_finished) == 0:\n",
    "                break\n",
    "            if len(mel_outputs) == self.max_decoder_steps:\n",
    "                break\n",
    "\n",
    "            decoder_input = mel_output\n",
    "\n",
    "        mel_outputs, gate_outputs = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs\n",
    "        )\n",
    "\n",
    "        return mel_outputs, gate_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqh3Dm5B83r8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective functions used are Mean Squared Error on the spectrogram and Binary Cross Entropy on the stop-linear gate\n",
    "\n",
    "MSE: $\\sum_{i=1}^{D}(x_i-y_i)^2$\n",
    "\n",
    "BCE: $-{(y\\log(p) + (1 - y)\\log(1 - p))}$\n",
    "\n",
    "The gate loss is multiplied by a positive weight with values 5.0 ~ 8.0 to address the fact that there is only one stop token per token sequence, therefore causing an imbalance that could lead to unstoppable inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhOUxykL9eRY"
   },
   "outputs": [],
   "source": [
    "LossStats = namedtuple(\n",
    "    \"TransformerLoss\", \"loss mel_loss gate_loss\"\n",
    ")\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    \"\"\"The TransformerTTS loss implementation based on Tacotron2\n",
    "\n",
    "    The loss consists of an MSE loss on the spectrogram and a BCE gate loss\n",
    "\n",
    "    The output of the module is a LossStats tuple, which includes both the\n",
    "    total loss\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    gate_loss_weight: float\n",
    "        The constant by which the gate loss will be multiplied. In the paper, it is 5.0 ~ 8.0\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> _ = torch.manual_seed(42)\n",
    "    >>> from speechbrain.lobes.models.Tacotron2 import Loss\n",
    "    >>> loss = Loss(guided_attention_sigma=0.2)\n",
    "    >>> mel_target = torch.randn(2, 80, 861)\n",
    "    >>> gate_target = torch.randn(1722, 1)\n",
    "    >>> mel_out = torch.randn(2, 80, 861)\n",
    "    >>> mel_out_postnet = torch.randn(2, 80, 861)\n",
    "    >>> gate_out = torch.randn(2, 861)\n",
    "    >>> alignments = torch.randn(2, 861, 173)\n",
    "    >>> targets = mel_target, gate_target\n",
    "    >>> model_outputs = mel_out, mel_out_postnet, gate_out, alignments\n",
    "    >>> input_lengths = torch.tensor([173,  91])\n",
    "    >>> target_lengths = torch.tensor([861, 438])\n",
    "    >>> loss(model_outputs, targets, input_lengths, target_lengths, 1)\n",
    "    TacotronLoss(loss=tensor(4.8566), mel_loss=tensor(4.0097), gate_loss=tensor(0.8460), attn_loss=tensor(0.0010), attn_weight=tensor(1.))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            gate_loss_weight=5.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.gate_loss_weight = gate_loss_weight\n",
    "\n",
    "    def forward(\n",
    "            self, model_output, targets\n",
    "    ):\n",
    "        \"\"\"Computes the loss\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        model_output: tuple\n",
    "            the output of the model's forward():\n",
    "            (mel_outputs, mel_outputs_postnet, gate_outputs, alignments)\n",
    "        targets: tuple\n",
    "            the targets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result: LossStats\n",
    "            the total loss - and individual losses (mel and gate)\n",
    "\n",
    "        \"\"\"\n",
    "        mel_target, gate_target = targets[0], targets[1]\n",
    "        mel_target.requires_grad = False\n",
    "        gate_target.requires_grad = False\n",
    "        gate_target = gate_target.view(-1, 1)\n",
    "\n",
    "        mel_out, mel_out_postnet, gate_out, output_lengths = model_output\n",
    "\n",
    "        gate_out = gate_out.view(-1, 1)\n",
    "\n",
    "        mel_loss = self.mse_loss(mel_out, mel_target) + self.mse_loss(\n",
    "            mel_out_postnet, mel_target\n",
    "        )\n",
    "\n",
    "        gate_loss = self.gate_loss_weight * self.bce_loss(gate_out, gate_target) # Applying weight to stop token loss\n",
    "\n",
    "        total_loss = mel_loss + gate_loss\n",
    "        return LossStats(\n",
    "            total_loss, mel_loss, gate_loss\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWz3FFgHwMKa",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Bringing it all in one file to add it to Speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%mkdir /notebooks/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2xlY3EhwOwb",
    "outputId": "17b279ef-3e7e-4a75-cd17-6dc07e394fdb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%file /notebooks/models/TransformerTTS.py\n",
    "\"\"\"\n",
    "Neural network modules for the TransformerTTS end-to-end neural\n",
    "Text-to-Speech (TTS) model\n",
    "\n",
    "Authors\n",
    "* Salman Hussain Ali 2024\n",
    "\"\"\"\n",
    "import math\n",
    "# This code uses a significant portion of the NVidia implementation, even though it\n",
    "# has been modified and enhanced\n",
    "\n",
    "# https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/tacotron2/model.py\n",
    "# *****************************************************************************\n",
    "#  Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "#  Redistribution and use in source and binary forms, with or without\n",
    "#  modification, are permitted provided that the following conditions are met:\n",
    "#      * Redistributions of source code must retain the above copyright\n",
    "#        notice, this list of conditions and the following disclaimer.\n",
    "#      * Redistributions in binary form must reproduce the above copyright\n",
    "#        notice, this list of conditions and the following disclaimer in the\n",
    "#        documentation and/or other materials provided with the distribution.\n",
    "#      * Neither the name of the NVIDIA CORPORATION nor the\n",
    "#        names of its contributors may be used to endorse or promote products\n",
    "#        derived from this software without specific prior written permission.\n",
    "#\n",
    "#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
    "#  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
    "#  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "#  DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n",
    "#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
    "#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
    "#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
    "#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
    "#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "# *****************************************************************************\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from speechbrain.lobes.models.transformer.Transformer import get_mask_from_lengths\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class LinearNorm(torch.nn.Module):\n",
    "    \"\"\"A linear layer with Xavier initialization\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_dim: int\n",
    "        the input dimension\n",
    "    out_dim: int\n",
    "        the output dimension\n",
    "    bias: bool\n",
    "        whether or not to use a bias\n",
    "    w_init_gain: linear\n",
    "        the weight initialization gain type (see torch.nn.init.calculate_gain)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.TransformerTTS import LinearNorm\n",
    "    >>> layer = LinearNorm(in_dim=5, out_dim=3)\n",
    "    >>> x = torch.randn(3, 5)\n",
    "    >>> y = layer(x)\n",
    "    >>> y.shape\n",
    "    torch.Size([3, 3])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain=\"linear\"):\n",
    "        super().__init__()\n",
    "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.linear_layer.weight,\n",
    "            gain=torch.nn.init.calculate_gain(w_init_gain),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            a (batch, features) input tensor\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the linear layer output\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "\n",
    "class ConvNorm(torch.nn.Module):\n",
    "    \"\"\"A 1D convolution layer with Xavier initialization\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_channels: int\n",
    "        the number of input channels\n",
    "    out_channels: int\n",
    "        the number of output channels\n",
    "    kernel_size: int\n",
    "        the kernel size\n",
    "    stride: int\n",
    "        the convolutional stride\n",
    "    padding: int\n",
    "        the amount of padding to include. If not provided, it will be calculated\n",
    "        as dilation * (kernel_size - 1) / 2\n",
    "    dilation: int\n",
    "        the dilation of the convolution\n",
    "    bias: bool\n",
    "        whether or not to use a bias\n",
    "    w_init_gain: linear\n",
    "        the weight initialization gain type (see torch.nn.init.calculate_gain)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.TransformerTTS import ConvNorm\n",
    "    >>> layer = ConvNorm(in_channels=10, out_channels=5, kernel_size=3)\n",
    "    >>> x = torch.randn(3, 10, 5)\n",
    "    >>> y = layer(x)\n",
    "    >>> y.shape\n",
    "    torch.Size([3, 5, 5])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=None,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "            w_init_gain=\"linear\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            assert kernel_size % 2 == 1\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain)\n",
    "        )\n",
    "\n",
    "    def forward(self, signal):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        signal: torch.Tensor\n",
    "            the input to the convolutional layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        return self.conv(signal)\n",
    "\n",
    "\n",
    "class EncoderPreNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        A block for preprocessing text inputs in an encoder network, consisting of a 1D convolutional layer followed by batch normalization, ReLU activation, dropout, and linear transformation.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        in_dim: int\n",
    "            Number of input channels.\n",
    "        kernel_size: int\n",
    "            Size of the convolutional kernel.\n",
    "        stride: int\n",
    "            Stride of the convolution.\n",
    "        padding: int, optional\n",
    "            Amount of padding to include. If not provided, it will be calculated as dilation * (kernel_size - 1) / 2.\n",
    "        dilation: int, optional\n",
    "            Dilation factor of the convolution.\n",
    "        bias: bool, optional\n",
    "            Whether to include bias in the convolutional layer.\n",
    "        dropout: float, optional\n",
    "            Dropout probability.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> import torch\n",
    "        >>> from speechbrain.lobes.models.TransformerTTS import EncoderPreNetBlock\n",
    "        >>> block = EncoderPreNetBlock(in_dim=10, kernel_size=3)\n",
    "        >>> x = torch.randn(3, 10, 5)  # Input tensor shape: (batch_size, in_dim, sequence_length)\n",
    "        >>> y = block(x)\n",
    "        >>> y.shape  # Output shape: (batch_size, in_dim, sequence_length)\n",
    "        torch.Size([3, 10, 5])\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim=512,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=None,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "            dropout=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            assert kernel_size % 2 == 1\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = ConvNorm(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "\n",
    "        self.batch = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        text: torch.Tensor\n",
    "            the input to the convolutional layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        # text = text.transpose(1, 2)\n",
    "        out = self.conv(text)\n",
    "        out = self.batch(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderPrenet(nn.Module):\n",
    "    \"\"\"The TransformerTTS pre-net module consisting of a specified number of\n",
    "    normalized (Xavier-initialized) linear layers\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_dim: int\n",
    "        the input dimensions\n",
    "    sizes: int\n",
    "        the dimension of the hidden layers/output\n",
    "    dropout: float\n",
    "        the dropout probability\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.TransformerTTS import EncoderPrenet\n",
    "    >>> layer = Prenet()\n",
    "    >>> x = torch.randn(862, 2, 80)\n",
    "    >>> output = layer(x)\n",
    "    >>> output.shape\n",
    "    torch.Size([862, 2, 256])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_dim=512,\n",
    "            hidden_dim=256,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=None,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "            num_layers=3,\n",
    "            dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderPreNetBlock(emb_dim, kernel_size, stride, padding, dilation, bias) for i in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(emb_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass for the prenet\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            the prenet inputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        # x = x.transpose(1,2) TODO review\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderPrenet(nn.Module):\n",
    "    \"\"\"The TransformerTTS pre-net module consisting of a specified number of\n",
    "        normalized (Xavier-initialized) linear layers\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        in_dim: int\n",
    "            the input dimensions\n",
    "        sizes: int\n",
    "            the dimension of the hidden layers/output\n",
    "        dropout: float\n",
    "            the dropout probability\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> import torch\n",
    "        >>> from speechbrain.lobes.models.TransformerTTS import DecoderPrenet\n",
    "        >>> layer = DecoderPrenet()\n",
    "        >>> x = torch.randn(862, 2, 80)\n",
    "        >>> output = layer(x)\n",
    "        >>> output.shape\n",
    "        torch.Size([862, 2, 256])\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mel_dims=80,\n",
    "            hidden_dims=256,\n",
    "            d_model=512,\n",
    "            dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(mel_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Linear layer used to project the targets to the same dimensional space as the phoneme's embeddings\n",
    "        self.linear = nn.Linear(hidden_dims, d_model)\n",
    "\n",
    "    def forward(self, mel):\n",
    "        \"\"\"Computes the forward pass for the prenet\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        mel: torch.Tensor\n",
    "            the mel spectrogram inputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        out = self.layers(mel)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Postnet(nn.Module):\n",
    "    \"\"\"The TransformerTTS postnet consists of a number of 1-d convolutional layers\n",
    "    with Xavier initialization and a tanh activation, with batch normalization.\n",
    "    Depending on configuration, the postnet may either refine the MEL spectrogram\n",
    "    or upsample it to a linear spectrogram. It has the same architecture as Tacotron2's Post-Net\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    n_mel_channels: int\n",
    "        the number of MEL spectrogram channels\n",
    "    postnet_embedding_dim: int\n",
    "        the postnet embedding dimension\n",
    "    postnet_kernel_size: int\n",
    "        the kernel size of the convolutions within the decoders\n",
    "    postnet_n_convolutions: int\n",
    "        the number of convolutions in the postnet\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.TransformerTTS import Postnet\n",
    "    >>> layer = Postnet()\n",
    "    >>> x = torch.randn(2, 80, 861)\n",
    "    >>> output = layer(x)\n",
    "    >>> output.shape\n",
    "    torch.Size([2, 80, 861])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_mel_channels=80,\n",
    "            postnet_embedding_dim=512,\n",
    "            postnet_kernel_size=5,\n",
    "            postnet_n_convolutions=5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(\n",
    "                    n_mel_channels,\n",
    "                    postnet_embedding_dim,\n",
    "                    kernel_size=postnet_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=int((postnet_kernel_size - 1) / 2),\n",
    "                    dilation=1,\n",
    "                    w_init_gain=\"tanh\",\n",
    "                ),\n",
    "                nn.BatchNorm1d(postnet_embedding_dim),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for i in range(1, postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    ConvNorm(\n",
    "                        postnet_embedding_dim,\n",
    "                        postnet_embedding_dim,\n",
    "                        kernel_size=postnet_kernel_size,\n",
    "                        stride=1,\n",
    "                        padding=int((postnet_kernel_size - 1) / 2),\n",
    "                        dilation=1,\n",
    "                        w_init_gain=\"tanh\",\n",
    "                    ),\n",
    "                    nn.BatchNorm1d(postnet_embedding_dim),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(\n",
    "                    postnet_embedding_dim,\n",
    "                    n_mel_channels,\n",
    "                    kernel_size=postnet_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=int((postnet_kernel_size - 1) / 2),\n",
    "                    dilation=1,\n",
    "                    w_init_gain=\"linear\",\n",
    "                ),\n",
    "                nn.BatchNorm1d(n_mel_channels),\n",
    "            )\n",
    "        )\n",
    "        self.n_convs = len(self.convolutions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass of the postnet\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            the postnet input (usually a MEL spectrogram)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the postnet output (a refined MEL spectrogram or a\n",
    "            linear spectrogram depending on how the model is\n",
    "            configured)\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        for conv in self.convolutions:\n",
    "            if i < self.n_convs - 1:\n",
    "                x = F.dropout(torch.tanh(conv(x)), 0.5, training=self.training)\n",
    "            else:\n",
    "                x = F.dropout(conv(x), 0.5, training=self.training)\n",
    "            i += 1\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerTTS(nn.Module):\n",
    "    \"\"\"The Transformer text-to-speech model, based on the NVIDIA implementation.\n",
    "\n",
    "    This class is the main entry point for the model, which is responsible\n",
    "    for instantiating all submodules, which, in turn, manage the individual\n",
    "    neural network layers\n",
    "\n",
    "    Simplified STRUCTURE: input->word embedding ->encoder ->attention \\\n",
    "    ->decoder(+prenet) -> postnet ->output\n",
    "\n",
    "    prenet(input is decoder previous time step) output is input to decoder\n",
    "    concatenated with the attention output\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    mask_padding: bool\n",
    "        whether or not to mask pad-outputs of TransformerTTS\n",
    "    n_mel_channels: int\n",
    "        number of mel channels for constructing spectrogram\n",
    "    n_symbols:  int=128\n",
    "        number of accepted char symbols defined in textToSequence\n",
    "    symbols_embedding_dim: int\n",
    "        number of embedding dimension for symbols fed to nn.Embedding\n",
    "    encoder_prenet_kernel_size: int\n",
    "        size of kernel processing the embeddings\n",
    "    encoder_prenet_n_convolutions: int\n",
    "        number of convolution layers in encoder\n",
    "    encoder_prenet_embedding_dim: int\n",
    "        number of kernels in encoder, this is also the dimension\n",
    "        of the bidirectional LSTM in the encoder\n",
    "    n_frames_per_step: int=1\n",
    "        only 1 generated mel-frame per step is supported for the decoder as of now.\n",
    "    decoder_rnn_dim: int\n",
    "        number of 2 unidirectional stacked LSTM units\n",
    "    prenet_dim: int\n",
    "        dimension of linear prenet layers\n",
    "    max_decoder_steps: int\n",
    "        maximum number of steps/frames the decoder generates before stopping\n",
    "    gate_threshold: int\n",
    "        cut off level any output probability above that is considered\n",
    "        complete and stops generation so we have variable length outputs\n",
    "    p_attention_dropout: float\n",
    "        attention drop out probability\n",
    "    p_decoder_dropout: float\n",
    "        decoder drop  out probability\n",
    "    postnet_embedding_dim: int\n",
    "        number os postnet dfilters\n",
    "    postnet_kernel_size: int\n",
    "        1d size of posnet kernel\n",
    "    postnet_n_convolutions: int\n",
    "        number of convolution layers in postnet\n",
    "    decoder_no_early_stopping: bool\n",
    "        determines early stopping of decoder\n",
    "        along with gate_threshold . The logical inverse of this is fed to the decoder\n",
    "    paddig_idx: int\n",
    "        represents the number given to the padding\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> _ = torch.manual_seed(213312)\n",
    "    >>> from speechbrain.lobes.models.TransformerTTS import TransformerTTS\n",
    "    >>> model = TransformerTTS(\n",
    "    ...    mask_padding=True,\n",
    "    ...    n_mel_channels=80,\n",
    "    ...    n_symbols=148,\n",
    "    ...    symbols_embedding_dim=512,\n",
    "    ...    encoder_prenet_kernel_size=5,\n",
    "    ...    encoder_prenet_n_convolutions=3,\n",
    "    ...    encoder_prenet_embedding_dim=512,\n",
    "    ...    attention_rnn_dim=1024,\n",
    "    ...    attention_dim=128,\n",
    "    ...    attention_location_n_filters=32,\n",
    "    ...    attention_location_kernel_size=31,\n",
    "    ...    n_frames_per_step=1,\n",
    "    ...    decoder_rnn_dim=1024,\n",
    "    ...    prenet_dim=256,\n",
    "    ...    max_decoder_steps=32,\n",
    "    ...    gate_threshold=0.5,\n",
    "    ...    p_attention_dropout=0.1,\n",
    "    ...    p_decoder_dropout=0.1,\n",
    "    ...    postnet_embedding_dim=512,\n",
    "    ...    postnet_kernel_size=5,\n",
    "    ...    postnet_n_convolutions=5,\n",
    "    ...    decoder_no_early_stopping=False,\n",
    "    ...    padding_idx=0,\n",
    "    ... )\n",
    "    >>> _ = model.eval()\n",
    "    >>> inputs = torch.tensor([\n",
    "    ...     [13, 12, 31, 14, 19],\n",
    "    ...     [31, 16, 30, 31, 0],\n",
    "    ... ])\n",
    "    >>> input_lengths = torch.tensor([5, 4])\n",
    "    >>> outputs, output_lengths = model.infer(inputs, input_lengths)\n",
    "    >>> outputs.shape, output_lengths.shape\n",
    "    (torch.Size([2, 80, 1]), torch.Size([2]))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mask_padding=True,\n",
    "            # mel generation parameter in data io\n",
    "            n_mel_channels=80,\n",
    "            # symbols\n",
    "            n_symbols=52,\n",
    "            symbols_embedding_dim=512,\n",
    "            # Encoder Pre-Net parameters\n",
    "            encoder_prenet_kernel_size=5,\n",
    "            encoder_prenet_n_convolutions=3,\n",
    "            encoder_prenet_padding=None,\n",
    "            encoder_prenet_dilation=1,\n",
    "            encoder_prenet_bias=True,\n",
    "            encoder_prenet_dropout=0.5,\n",
    "            encoder_prenet_stride=1,\n",
    "            # Decoder Pre-Net parameters\n",
    "            n_frames_per_step=1,\n",
    "            decoder_prenet_hidden_dims=256,\n",
    "            decoder_prenet_dropout=0.15,\n",
    "            # Transformer Parameters\n",
    "            d_model=256,\n",
    "            transformer_nhead=8,\n",
    "            transformer_num_encoder_layers=6,\n",
    "            transformer_num_decoder_layers=6,\n",
    "            transformer_d_ffn=2048,\n",
    "            transformer_dropout=0.1,\n",
    "            transformer_activation=\"relu\",\n",
    "            custom_encoder_module=None,\n",
    "            custom_decoder_module=None,\n",
    "            batch_first=False,\n",
    "            norm_first=False,\n",
    "            layer_norm_eps=1e-5,\n",
    "            # Mel-post processing network parameters\n",
    "            postnet_embedding_dim=512,\n",
    "            postnet_kernel_size=5,\n",
    "            postnet_n_convolutions=5,\n",
    "            gate_threshold=0.5,\n",
    "            max_decoder_steps=32,\n",
    "            early_stopping=False,\n",
    "            padding_idx=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask_padding = mask_padding\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "        self.max_decoder_steps = max_decoder_steps\n",
    "        self.early_stopping = early_stopping\n",
    "\n",
    "        self.gate_threshold = gate_threshold\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(n_symbols, symbols_embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        std = sqrt(2.0 / (n_symbols + d_model))\n",
    "        val = sqrt(3.0) * std  # uniform bounds for std\n",
    "        self.encoder_embedding.weight.data.uniform_(-val, val)\n",
    "\n",
    "        if custom_encoder_module is None:\n",
    "            encoder_block = torch.nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                             nhead=transformer_nhead,\n",
    "                                                             dim_feedforward=transformer_d_ffn,\n",
    "                                                             dropout=transformer_dropout,\n",
    "                                                             activation=transformer_activation,\n",
    "                                                             batch_first=batch_first,\n",
    "                                                             norm_first=norm_first,\n",
    "                                                             layer_norm_eps=layer_norm_eps)\n",
    "        else:\n",
    "            encoder_block = custom_encoder_module\n",
    "\n",
    "        if custom_decoder_module is None:\n",
    "            decoder_block = torch.nn.TransformerDecoderLayer(d_model=d_model,\n",
    "                                                             nhead=transformer_nhead,\n",
    "                                                             dim_feedforward=transformer_d_ffn,\n",
    "                                                             dropout=transformer_dropout,\n",
    "                                                             activation=transformer_activation,\n",
    "                                                             batch_first=batch_first,\n",
    "                                                             norm_first=norm_first,\n",
    "                                                             layer_norm_eps=layer_norm_eps)\n",
    "        else:\n",
    "            decoder_block = custom_decoder_module\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_block, num_layers=transformer_num_encoder_layers)\n",
    "\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_block, num_layers=transformer_num_decoder_layers)\n",
    "\n",
    "        self.encoder_prenet = EncoderPrenet(\n",
    "            emb_dim=symbols_embedding_dim,\n",
    "            hidden_dim=d_model,\n",
    "            kernel_size=encoder_prenet_kernel_size,\n",
    "            num_layers=encoder_prenet_n_convolutions,\n",
    "            stride=encoder_prenet_stride,\n",
    "            padding=encoder_prenet_padding,\n",
    "            dilation=encoder_prenet_dilation,\n",
    "            bias=encoder_prenet_bias,\n",
    "            dropout=encoder_prenet_dropout)\n",
    "\n",
    "        self.decoder_prenet = DecoderPrenet(\n",
    "            mel_dims=n_mel_channels,\n",
    "            hidden_dims=decoder_prenet_hidden_dims,\n",
    "            dropout=decoder_prenet_dropout,\n",
    "            d_model=d_model\n",
    "        )\n",
    "\n",
    "        self.postnet = Postnet(\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            postnet_embedding_dim=postnet_embedding_dim,\n",
    "            postnet_kernel_size=postnet_kernel_size,\n",
    "            postnet_n_convolutions=postnet_n_convolutions,\n",
    "        )\n",
    "\n",
    "        self.encoder_positional_encoding = PositionalEncoding(input_size=d_model)\n",
    "        self.decoder_positional_encoding = PositionalEncoding(input_size=d_model)\n",
    "\n",
    "        self.mel_linear = LinearNorm(d_model, n_mel_channels)\n",
    "        self.stop_linear = LinearNorm(d_model, 1, w_init_gain='sigmoid')\n",
    "\n",
    "    def parse_output(self, outputs, output_lengths):\n",
    "        \"\"\"\n",
    "        Masks the padded part of output\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        outputs: list\n",
    "            a list of tensors - raw outputs\n",
    "        output_lengths: torch.Tensor\n",
    "            a tensor representing the lengths of all outputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs: torch.Tensor\n",
    "        mel_outputs_postnet: torch.Tensor\n",
    "        gate_outputs: torch.Tensor\n",
    "        \"\"\"\n",
    "        mel_outputs, mel_outputs_postnet, gate_outputs = outputs\n",
    "\n",
    "        if self.mask_padding and output_lengths is not None:\n",
    "            mask = get_mask_from_lengths(\n",
    "                output_lengths, max_len=mel_outputs.size(-1)\n",
    "            )\n",
    "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "\n",
    "            mel_outputs.clone().masked_fill_(mask, 0.0)\n",
    "            mel_outputs_postnet.masked_fill_(mask, 0.0)\n",
    "            gate_outputs.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
    "\n",
    "        return mel_outputs, mel_outputs_postnet, gate_outputs\n",
    "\n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs):\n",
    "        \"\"\"Prepares decoder outputs for output\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        mel_outputs: torch.Tensor\n",
    "            MEL-scale spectrogram outputs\n",
    "        gate_outputs: torch.Tensor\n",
    "            gate output energies\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs: torch.Tensor\n",
    "            MEL-scale spectrogram outputs\n",
    "        gate_outputs: torch.Tensor\n",
    "            gate output energies\n",
    "        \"\"\"\n",
    "        # (T_out, B) -> (B, T_out)\n",
    "        if gate_outputs.dim() == 1:\n",
    "            gate_outputs = gate_outputs.unsqueeze(0)\n",
    "        else:\n",
    "            gate_outputs = gate_outputs.transpose(0, 1).contiguous()\n",
    "\n",
    "        # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
    "        mel_outputs = mel_outputs.transpose(0, 1).contiguous()\n",
    "        # decouple frames per step\n",
    "        shape = (mel_outputs.shape[0], -1, self.n_mel_channels)\n",
    "        mel_outputs = mel_outputs.view(*shape)\n",
    "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "        return mel_outputs, gate_outputs\n",
    "\n",
    "    def get_go_frame(self, memory):\n",
    "        \"\"\"Gets all zeros frames to use as first decoder input\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        memory: torch.Tensor\n",
    "            decoder outputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        decoder_input: torch.Tensor\n",
    "            all zeros frames\n",
    "        \"\"\"\n",
    "        B = memory.size(0)\n",
    "        dtype = memory.dtype\n",
    "        device = memory.device\n",
    "        decoder_input = torch.zeros(\n",
    "            B,\n",
    "            self.n_mel_channels * self.n_frames_per_step,\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "        )\n",
    "        return decoder_input\n",
    "\n",
    "    def forward(self, inputs, masks):\n",
    "        \"\"\"Decoder forward pass for training\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        inputs: tuple\n",
    "            batch object\n",
    "        masks: tuple\n",
    "            contains the lookahead target mask, and the src and target key padding mask\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs: torch.Tensor\n",
    "            mel outputs from the decoder\n",
    "        mel_outputs_postnet: torch.Tensor\n",
    "            mel outputs from postnet\n",
    "        gate_outputs: torch.Tensor\n",
    "            gate outputs from the decoder\n",
    "        output_lengths: torch.Tensor\n",
    "            length of the output without padding\n",
    "        \"\"\"\n",
    "        inputs, input_lengths, mel_padded, max_len, mel_len = inputs  # (text_padded, input_lengths, mel_padded, max_len, output_lengths)\n",
    "        tgt_mask, src_mask, src_key_padding_mask, tgt_key_padding_mask = masks\n",
    "\n",
    "        # Generate Embeddings\n",
    "        embedded_inputs = self.encoder_embedding(inputs).transpose(1, 2)\n",
    "\n",
    "        # Pass through encoder pre-net\n",
    "        encoder_prenet_outputs = self.encoder_prenet(embedded_inputs)\n",
    "\n",
    "        # Pass through decoder pre-net\n",
    "        decoder_prenet_outputs = self.decoder_prenet(mel_padded.transpose(1, 2))\n",
    "\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs.transpose(0, 1)\n",
    "        decoder_prenet_outputs = decoder_prenet_outputs.transpose(0, 1)\n",
    "\n",
    "        # Add Scaled Positional Embeddings to encoder and decoder pre-nets\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs + self.encoder_positional_encoding(encoder_prenet_outputs)\n",
    "        decoder_prenet_outputs = decoder_prenet_outputs + self.decoder_positional_encoding(decoder_prenet_outputs)\n",
    "\n",
    "        # Input embedded phonemes into transformer's encoder\n",
    "        memory = self.transformer_encoder(\n",
    "            src=encoder_prenet_outputs,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        # Input memory and mel spectograms into transformer's decoder\n",
    "        mel_outputs = self.transformer_decoder(tgt=decoder_prenet_outputs,\n",
    "                                               memory=memory,\n",
    "                                               tgt_mask=tgt_mask,\n",
    "                                               tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                               memory_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # Calculate mel linear, mel stop, and post-net\n",
    "        # Stop Linear\n",
    "        stop_token = self.stop_linear(mel_outputs).squeeze()\n",
    "\n",
    "        # Mel Linear and Post-Net\n",
    "        mel_outputs = self.mel_linear(mel_outputs)\n",
    "\n",
    "        mel_outputs, stop_token = self.parse_decoder_outputs(mel_outputs, stop_token)\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, stop_token], mel_len)\n",
    "\n",
    "    def infer(self, inputs, input_lengths):\n",
    "        \"\"\"Produces outputs\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        inputs: torch.tensor\n",
    "            text or phonemes converted\n",
    "\n",
    "        input_lengths: torch.tensor\n",
    "            the lengths of input parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs_postnet: torch.Tensor\n",
    "            final mel output of tacotron 2\n",
    "        mel_lengths: torch.Tensor\n",
    "            length of mels\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate Embeddings\n",
    "        embedded_inputs = self.encoder_embedding(inputs).transpose(1, 2)\n",
    "\n",
    "        # Pass through encoder pre-net\n",
    "        encoder_prenet_outputs = self.encoder_prenet(embedded_inputs)\n",
    "\n",
    "        # Add Scaled Positional Embeddings to encoder and decoder pre-nets\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs + self.encoder_positional_encoding(encoder_prenet_outputs)\n",
    "\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs.transpose(0, 1)\n",
    "\n",
    "        # Input embedded phonemes into transformer's encoder\n",
    "        src_key_padding_mask = get_mask_from_lengths(input_lengths).to(inputs.device, non_blocking=True)\n",
    "        memory = self.transformer_encoder(\n",
    "            src=encoder_prenet_outputs,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        decoder_input = self.get_go_frame(memory)\n",
    "\n",
    "        mel_lengths = torch.zeros(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "        not_finished = torch.ones(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "\n",
    "        mel_outputs, gate_outputs = (\n",
    "            torch.zeros(1),\n",
    "            torch.zeros(1),\n",
    "        )\n",
    "\n",
    "        first_iter = True\n",
    "        while True:\n",
    "            decoder_input = self.decoder_prenet(decoder_input)\n",
    "            if len(decoder_input.shape) != 3:\n",
    "                decoder_input = decoder_input.unsqueeze(1)\n",
    "\n",
    "            decoder_input = decoder_input + self.decoder_positional_encoding(decoder_input)\n",
    "\n",
    "            mask_size = decoder_input.shape[0]\n",
    "            tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(mask_size, device=inputs.device)\n",
    "\n",
    "            decoder_output = self.transformer_decoder(memory=memory,\n",
    "                                                      tgt=decoder_input,\n",
    "                                                      memory_key_padding_mask=src_key_padding_mask,\n",
    "                                                      tgt_mask=tgt_mask)\n",
    "\n",
    "            # Calculate mel linear and stop token\n",
    "            # Stop Linear\n",
    "            gate_output = self.stop_linear(decoder_output).squeeze()\n",
    "\n",
    "            # Mel Linear and Post-Net\n",
    "            mel_output = self.mel_linear(decoder_output)\n",
    "\n",
    "            if first_iter:\n",
    "                mel_outputs = mel_output\n",
    "                gate_outputs = gate_output\n",
    "                first_iter = False\n",
    "            else:\n",
    "                mel_outputs = torch.cat(\n",
    "                    (mel_outputs, mel_output), dim=0\n",
    "                )\n",
    "                gate_outputs = torch.cat((gate_outputs, gate_output), dim=0)\n",
    "\n",
    "            sigmoid = torch.sigmoid(gate_output).unsqueeze(1)\n",
    "\n",
    "            dec = (\n",
    "                torch.le(sigmoid, torch.tensor(self.gate_threshold))\n",
    "                .to(torch.int32)\n",
    "                .squeeze(1)\n",
    "            )\n",
    "\n",
    "            not_finished = not_finished * dec\n",
    "            mel_lengths += not_finished\n",
    "            if self.early_stopping and torch.sum(not_finished) == 0:\n",
    "                break\n",
    "            if len(mel_outputs) == self.max_decoder_steps:\n",
    "                break\n",
    "\n",
    "            decoder_input = mel_output\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return mel_outputs_postnet, mel_lengths\n",
    "\n",
    "\n",
    "def infer(model, text_sequences, input_lengths):\n",
    "    \"\"\"\n",
    "    An inference hook for pretrained synthesizers\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    model: TransformerTTS\n",
    "        the TransformerTTS model\n",
    "    text_sequences: torch.Tensor\n",
    "        encoded text sequences\n",
    "    input_lengths: torch.Tensor\n",
    "        input lengths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: tuple\n",
    "        (mel_outputs_postnet, mel_lengths) - the exact\n",
    "        model output\n",
    "    \"\"\"\n",
    "    return model.infer(text_sequences, input_lengths)\n",
    "\n",
    "\n",
    "LossStats = namedtuple(\n",
    "    \"TransformerLoss\", \"loss mel_loss gate_loss\"\n",
    ")\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    \"\"\"TransformerTTS loss function\n",
    "\n",
    "    The loss consists of an MSE loss on the spectrogram and a weighted BCE gate loss\n",
    "\n",
    "    The output of the module is a LossStats tuple, which includes both the\n",
    "    total loss\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    gate_loss_weight: float\n",
    "        The constant by which the gate loss will be multiplied. In the paper, it is 5.0 ~ 8.0\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> _ = torch.manual_seed(42)\n",
    "    >>> from speechbrain.lobes.models.TransformerTTS import Loss\n",
    "    >>> loss = Loss(gate_loss_weight=5.0)\n",
    "    >>> mel_target = torch.randn(2, 80, 861)\n",
    "    >>> gate_target = torch.randn(1722, 1)\n",
    "    >>> mel_out = torch.randn(2, 80, 861)\n",
    "    >>> mel_out_postnet = torch.randn(2, 80, 861)\n",
    "    >>> gate_out = torch.randn(2, 861)\n",
    "    >>> targets = mel_target, gate_target\n",
    "    >>> model_outputs = mel_out, mel_out_postnet, gate_out\n",
    "    >>> input_lengths = torch.tensor([173,  91])\n",
    "    >>> target_lengths = torch.tensor([861, 438])\n",
    "    >>> loss(model_outputs, targets, input_lengths, target_lengths, 1)\n",
    "    TransformerTTSLoss(loss=tensor(4.8566), mel_loss=tensor(4.0097), gate_loss=tensor(0.8460))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            gate_loss_weight=5.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.gate_loss_weight = gate_loss_weight\n",
    "\n",
    "    def forward(\n",
    "            self, model_output, targets\n",
    "    ):\n",
    "        \"\"\"Computes the loss\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        model_output: tuple\n",
    "            the output of the model's forward():\n",
    "            (mel_outputs, mel_outputs_postnet, gate_outputs)\n",
    "        targets: tuple\n",
    "            the targets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result: LossStats\n",
    "            the total loss - and individual losses (mel and gate)\n",
    "\n",
    "        \"\"\"\n",
    "        mel_target, gate_target = targets[0], targets[1]\n",
    "        mel_target.requires_grad = False\n",
    "        gate_target.requires_grad = False\n",
    "        gate_target = gate_target.view(-1, 1)\n",
    "\n",
    "        mel_out, mel_out_postnet, gate_out = model_output\n",
    "\n",
    "        gate_out = gate_out.view(-1, 1)\n",
    "\n",
    "        mel_loss = self.mse_loss(mel_out, mel_target) + self.mse_loss(\n",
    "            mel_out_postnet, mel_target\n",
    "        )\n",
    "\n",
    "        gate_loss = self.gate_loss_weight * self.bce_loss(gate_out, gate_target)  # Applying weight to stop token loss\n",
    "\n",
    "        total_loss = mel_loss + gate_loss\n",
    "        return LossStats(\n",
    "            total_loss, mel_loss, gate_loss\n",
    "        )\n",
    "\n",
    "\n",
    "class TextMelCollate:\n",
    "    \"\"\"Zero-pads model inputs and targets based on number of frames per step\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    n_frames_per_step: int\n",
    "        the number of output frames per step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_frames_per_step=1):\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "\n",
    "    # TODO: Make this more intuitive, use the pipeline\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from normalized text and mel-spectrogram\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch: list\n",
    "            [text_normalized, mel_normalized]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text_padded: torch.Tensor\n",
    "        input_lengths: torch.Tensor\n",
    "        mel_padded: torch.Tensor\n",
    "        gate_padded: torch.Tensor\n",
    "        output_lengths: torch.Tensor\n",
    "        len_x: torch.Tensor\n",
    "        labels: torch.Tensor\n",
    "        wavs: torch.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Remove for loops and this dirty hack\n",
    "        raw_batch = list(batch)\n",
    "        for i in range(\n",
    "                len(batch)\n",
    "        ):  # the pipeline return a dictionary with one element\n",
    "            batch[i] = batch[i][\"mel_text_pair\"]\n",
    "\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x[0]) for x in batch]), dim=0, descending=True\n",
    "        )\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            text = batch[ids_sorted_decreasing[i]][0]\n",
    "            text_padded[i, : text.size(0)] = text\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0][1].size(0)\n",
    "        max_target_len = max([x[1].size(1) for x in batch])\n",
    "        if max_target_len % self.n_frames_per_step != 0:\n",
    "            max_target_len += (\n",
    "                    self.n_frames_per_step - max_target_len % self.n_frames_per_step\n",
    "            )\n",
    "            assert max_target_len % self.n_frames_per_step == 0\n",
    "\n",
    "        # include mel padded and gate padded\n",
    "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_target_len)\n",
    "        mel_padded.zero_()\n",
    "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
    "        gate_padded.zero_()\n",
    "        output_lengths = torch.LongTensor(len(batch))\n",
    "        labels, wavs = [], []\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            idx = ids_sorted_decreasing[i]\n",
    "            mel = batch[idx][1]\n",
    "            mel_padded[i, :, : mel.size(1)] = mel\n",
    "            gate_padded[i, mel.size(1) - 1:] = 1\n",
    "            output_lengths[i] = mel.size(1)\n",
    "            labels.append(raw_batch[idx][\"label\"])\n",
    "            wavs.append(raw_batch[idx][\"wav\"])\n",
    "\n",
    "        # count number of items - characters in text\n",
    "        len_x = [x[2] for x in batch]\n",
    "        len_x = torch.Tensor(len_x)\n",
    "\n",
    "        return (\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            gate_padded,\n",
    "            output_lengths,\n",
    "            len_x,\n",
    "            labels,\n",
    "            wavs,\n",
    "        )\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"This class implements the absolute sinusoidal positional encoding function.\n",
    "    PE(pos, 2i)   = sin(pos/(10000^(2i/dmodel)))\n",
    "    PE(pos, 2i+1) = cos(pos/(10000^(2i/dmodel)))\n",
    "\n",
    "    Based on Cornell & Zhong's implementation in Transformer.py\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    input_size: int\n",
    "        Embedding dimension.\n",
    "    max_len : int, optional\n",
    "        Max length of the input sequences (default 2500).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> a = torch.rand((8, 120, 512))\n",
    "    >>> enc = PositionalEncoding(input_size=a.shape[-1])\n",
    "    >>> b = enc(a)\n",
    "    >>> b.shape\n",
    "    torch.Size([1, 120, 512])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, max_len=2500):\n",
    "        super().__init__()\n",
    "        if input_size % 2 != 0:\n",
    "            raise ValueError(\n",
    "                f\"Cannot use sin/cos positional encoding with odd channels (got channels={input_size})\"\n",
    "            )\n",
    "        self.max_len = max_len\n",
    "        pe = torch.zeros(self.max_len, input_size, requires_grad=False)\n",
    "        positions = torch.arange(0, self.max_len).unsqueeze(1).float()\n",
    "        denominator = torch.exp(\n",
    "            torch.arange(0, input_size, 2).float()\n",
    "            * -(math.log(10000.0) / input_size)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(positions * denominator)\n",
    "        pe[:, 1::2] = torch.cos(positions * denominator)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "        # Define learnable scaling parameter\n",
    "        self.alpha = nn.Parameter(torch.Tensor(1))  # 1-dimensional tensor\n",
    "\n",
    "        # Initialize alpha parameter\n",
    "        nn.init.normal_(self.alpha)  # Initialize with random values\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Input feature shape (batch, time, fea)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The positional encoding.\n",
    "        \"\"\"\n",
    "        scaled_pos_embedding = self.alpha * self.pe[:, : x.size(1)].clone().detach()\n",
    "        return scaled_pos_embedding\n",
    "\n",
    "\n",
    "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
    "    \"\"\"Dynamic range compression for audio signals\"\"\"\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "\n",
    "def mel_spectogram(\n",
    "        sample_rate,\n",
    "        hop_length,\n",
    "        win_length,\n",
    "        n_fft,\n",
    "        n_mels,\n",
    "        f_min,\n",
    "        f_max,\n",
    "        power,\n",
    "        normalized,\n",
    "        norm,\n",
    "        mel_scale,\n",
    "        compression,\n",
    "        audio,\n",
    "):\n",
    "    \"\"\"calculates MelSpectrogram for a raw audio signal\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    sample_rate : int\n",
    "        Sample rate of audio signal.\n",
    "    hop_length : int\n",
    "        Length of hop between STFT windows.\n",
    "    win_length : int\n",
    "        Window size.\n",
    "    n_fft : int\n",
    "        Size of FFT.\n",
    "    n_mels : int\n",
    "        Number of mel filterbanks.\n",
    "    f_min : float\n",
    "        Minimum frequency.\n",
    "    f_max : float\n",
    "        Maximum frequency.\n",
    "    power : float\n",
    "        Exponent for the magnitude spectrogram.\n",
    "    normalized : bool\n",
    "        Whether to normalize by magnitude after stft.\n",
    "    norm : str or None\n",
    "        If \"slaney\", divide the triangular mel weights by the width of the mel band\n",
    "    mel_scale : str\n",
    "        Scale to use: \"htk\" or \"slaney\".\n",
    "    compression : bool\n",
    "        whether to do dynamic range compression\n",
    "    audio : torch.Tensor\n",
    "        input audio signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mel : torch.Tensor\n",
    "        The computed mel spectrogram features.\n",
    "    \"\"\"\n",
    "    from torchaudio import transforms\n",
    "\n",
    "    audio_to_mel = transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        n_fft=n_fft,\n",
    "        n_mels=n_mels,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        power=power,\n",
    "        normalized=normalized,\n",
    "        norm=norm,\n",
    "        mel_scale=mel_scale,\n",
    "    ).to(audio.device)\n",
    "\n",
    "    mel = audio_to_mel(audio)\n",
    "\n",
    "    if compression:\n",
    "        mel = dynamic_range_compression(mel)\n",
    "\n",
    "    return mel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS64EWD_1TYF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Brain class, dataset preparation, and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVn0Inc26gYX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import speechbrain as sb\n",
    "import sys\n",
    "import logging\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "from speechbrain.utils.text_to_sequence import text_to_sequence, _g2p_keep_punctuations, _clean_text\n",
    "from speechbrain.utils.data_utils import scalarize\n",
    "from speechbrain.inference.text import GraphemeToPhoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nPcoy-C6i5L",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCwu8kIf5_kG",
    "tags": []
   },
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvNwSxvt6Jtm"
   },
   "source": [
    "Transforms audio files into their respective mel spectogram, and packages together the mel spectrogram with their respective encoded phonemes in mel_text_pair, which will be consumed by TextMelCollate to pad them before passing them into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSU4s47G6Ja9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataio_prepare(hparams):\n",
    "    lexicon = hparams[\"lexicon\"]\n",
    "    input_encoder = hparams.get(\"input_encoder\")\n",
    "\n",
    "    # add a dummy symbol for idx 0 - used for padding.\n",
    "    lexicon = [\"@@\"] + lexicon\n",
    "    input_encoder.update_from_iterable(lexicon, sequence_input=False)\n",
    "    input_encoder.add_unk()\n",
    "\n",
    "    # Define audio pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wav\", \"phonemes\")\n",
    "    @sb.utils.data_pipeline.provides(\"mel_text_pair\")\n",
    "    def audio_pipeline(wav, phonemes):\n",
    "        # Calculate the mel spectrogram for the audio files\n",
    "        audio = sb.dataio.dataio.read_audio(wav)\n",
    "        mel = hparams[\"mel_spectogram\"](audio=audio)\n",
    "\n",
    "        # Encode phonemes to get the sequence of IDs corresponding to the symbols in the text.\n",
    "        encoded_phonemes = input_encoder.encode_sequence_torch(phonemes).int()\n",
    "\n",
    "        len_phonemes = len(encoded_phonemes)\n",
    "\n",
    "        return encoded_phonemes, mel, len_phonemes\n",
    "\n",
    "    datasets = {}\n",
    "    data_info = {\n",
    "        \"train\": hparams[\"train_json\"],\n",
    "        \"valid\": hparams[\"valid_json\"],\n",
    "        \"test\": hparams[\"test_json\"],\n",
    "    }\n",
    "    for dataset in hparams[\"splits\"]:\n",
    "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "            json_path=data_info[dataset],\n",
    "            replacements={\"data_root\": hparams[\"data_folder\"]},\n",
    "            dynamic_items=[audio_pipeline],\n",
    "            output_keys=[\"mel_text_pair\", \"wav\", \"label\"],\n",
    "        )\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifying LJSpeech prepare to add GrapheneToPhoneme inference in ljspeech_prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file /notebooks/speechbrain/recipes/LJSpeech/TTS/TransformerTTS/ljspeech_prepare.py\n",
    "\"\"\"\n",
    "LJspeech data preparation.\n",
    "Download: https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "\n",
    "Authors\n",
    " * Yingzhi WANG 2022\n",
    " * Sathvik Udupa 2022\n",
    " * Pradnya Kandarkar 2023\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from speechbrain.utils.data_utils import download_file\n",
    "from speechbrain.dataio.dataio import load_pkl, save_pkl\n",
    "import tgt\n",
    "from speechbrain.inference.text import GraphemeToPhoneme\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from speechbrain.utils.text_to_sequence import _g2p_keep_punctuations\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "OPT_FILE = \"opt_ljspeech_prepare.pkl\"\n",
    "METADATA_CSV = \"metadata.csv\"\n",
    "TRAIN_JSON = \"train.json\"\n",
    "VALID_JSON = \"valid.json\"\n",
    "TEST_JSON = \"test.json\"\n",
    "WAVS = \"wavs\"\n",
    "DURATIONS = \"durations\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "OPT_FILE = \"opt_ljspeech_prepare.pkl\"\n",
    "\n",
    "\n",
    "def prepare_ljspeech(\n",
    "    data_folder,\n",
    "    save_folder,\n",
    "    splits=[\"train\", \"valid\"],\n",
    "    split_ratio=[90, 10],\n",
    "    model_name=None,\n",
    "    seed=1234,\n",
    "    pitch_n_fft=1024,\n",
    "    pitch_hop_length=256,\n",
    "    pitch_min_f0=65,\n",
    "    pitch_max_f0=400,\n",
    "    skip_prep=False,\n",
    "    use_custom_cleaner=False,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the csv files for the LJspeech datasets.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    data_folder : str\n",
    "        Path to the folder where the original LJspeech dataset is stored\n",
    "    save_folder : str\n",
    "        The directory where to store the csv/json files\n",
    "    splits : list\n",
    "        List of dataset splits to prepare\n",
    "    split_ratio : list\n",
    "        Proportion for dataset splits\n",
    "    model_name : str\n",
    "        Model name (used to prepare additional model specific data)\n",
    "    seed : int\n",
    "        Random seed\n",
    "    pitch_n_fft : int\n",
    "        Number of fft points for pitch computation\n",
    "    pitch_hop_length : int\n",
    "        Hop length for pitch computation\n",
    "    pitch_min_f0 : int\n",
    "        Minimum f0 for pitch computation\n",
    "    pitch_max_f0 : int\n",
    "        Max f0 for pitch computation\n",
    "    skip_prep : bool\n",
    "        If True, skip preparation\n",
    "    use_custom_cleaner : bool\n",
    "        If True, uses custom cleaner defined for this recipe\n",
    "    device : str\n",
    "        Device for to be used for computation (used as required)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> from recipes.LJSpeech.TTS.ljspeech_prepare import prepare_ljspeech\n",
    "    >>> data_folder = 'data/LJspeech/'\n",
    "    >>> save_folder = 'save/'\n",
    "    >>> splits = ['train', 'valid']\n",
    "    >>> split_ratio = [90, 10]\n",
    "    >>> seed = 1234\n",
    "    >>> prepare_ljspeech(data_folder, save_folder, splits, split_ratio, seed)\n",
    "    \"\"\"\n",
    "    # Sets seeds for reproducible code\n",
    "    random.seed(seed)\n",
    "\n",
    "    if skip_prep:\n",
    "        return\n",
    "\n",
    "    # Creating configuration for easily skipping data_preparation stage\n",
    "    conf = {\n",
    "        \"data_folder\": data_folder,\n",
    "        \"splits\": splits,\n",
    "        \"split_ratio\": split_ratio,\n",
    "        \"save_folder\": save_folder,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    # Setting output files\n",
    "    meta_csv = os.path.join(data_folder, METADATA_CSV)\n",
    "    wavs_folder = os.path.join(data_folder, WAVS)\n",
    "\n",
    "    save_opt = os.path.join(save_folder, OPT_FILE)\n",
    "    save_json_train = os.path.join(save_folder, TRAIN_JSON)\n",
    "    save_json_valid = os.path.join(save_folder, VALID_JSON)\n",
    "    save_json_test = os.path.join(save_folder, TEST_JSON)\n",
    "\n",
    "    phoneme_alignments_folder = None\n",
    "    duration_folder = None\n",
    "    pitch_folder = None\n",
    "    # Setting up additional folders required for FastSpeech2\n",
    "    if model_name is not None and \"FastSpeech2\" in model_name:\n",
    "        # This step requires phoneme alignments to be present in the data_folder\n",
    "        # We automatically download the alignments from https://www.dropbox.com/s/v28x5ldqqa288pu/LJSpeech.zip\n",
    "        # Download and unzip LJSpeech phoneme alignments from here: https://drive.google.com/drive/folders/1DBRkALpPd6FL9gjHMmMEdHODmkgNIIK4\n",
    "        alignment_URL = (\n",
    "            \"https://www.dropbox.com/s/v28x5ldqqa288pu/LJSpeech.zip?dl=1\"\n",
    "        )\n",
    "        phoneme_alignments_folder = os.path.join(\n",
    "            data_folder, \"TextGrid\", \"LJSpeech\"\n",
    "        )\n",
    "        download_file(\n",
    "            alignment_URL, data_folder + \"/alignments.zip\", unpack=True\n",
    "        )\n",
    "\n",
    "        duration_folder = os.path.join(data_folder, \"durations\")\n",
    "        if not os.path.exists(duration_folder):\n",
    "            os.makedirs(duration_folder)\n",
    "\n",
    "        # extract pitch for both Fastspeech2 and FastSpeech2WithAligner models\n",
    "        pitch_folder = os.path.join(data_folder, \"pitch\")\n",
    "        if not os.path.exists(pitch_folder):\n",
    "            os.makedirs(pitch_folder)\n",
    "\n",
    "    # Check if this phase is already done (if so, skip it)\n",
    "    if skip(splits, save_folder, conf):\n",
    "        logger.info(\"Skipping preparation, completed in previous run.\")\n",
    "        return\n",
    "\n",
    "    # Additional check to make sure metadata.csv and wavs folder exists\n",
    "    assert os.path.exists(meta_csv), \"metadata.csv does not exist\"\n",
    "    assert os.path.exists(wavs_folder), \"wavs/ folder does not exist\"\n",
    "\n",
    "    # Prepare data splits\n",
    "    msg = \"Creating json file for ljspeech Dataset..\"\n",
    "    logger.info(msg)\n",
    "    data_split, meta_csv = split_sets(data_folder, splits, split_ratio)\n",
    "\n",
    "    if \"train\" in splits:\n",
    "        prepare_json(\n",
    "            model_name,\n",
    "            data_split[\"train\"],\n",
    "            save_json_train,\n",
    "            wavs_folder,\n",
    "            meta_csv,\n",
    "            phoneme_alignments_folder,\n",
    "            duration_folder,\n",
    "            pitch_folder,\n",
    "            pitch_n_fft,\n",
    "            pitch_hop_length,\n",
    "            pitch_min_f0,\n",
    "            pitch_max_f0,\n",
    "            use_custom_cleaner,\n",
    "            device,\n",
    "        )\n",
    "    if \"valid\" in splits:\n",
    "        prepare_json(\n",
    "            model_name,\n",
    "            data_split[\"valid\"],\n",
    "            save_json_valid,\n",
    "            wavs_folder,\n",
    "            meta_csv,\n",
    "            phoneme_alignments_folder,\n",
    "            duration_folder,\n",
    "            pitch_folder,\n",
    "            pitch_n_fft,\n",
    "            pitch_hop_length,\n",
    "            pitch_min_f0,\n",
    "            pitch_max_f0,\n",
    "            use_custom_cleaner,\n",
    "            device,\n",
    "        )\n",
    "    if \"test\" in splits:\n",
    "        prepare_json(\n",
    "            model_name,\n",
    "            data_split[\"test\"],\n",
    "            save_json_test,\n",
    "            wavs_folder,\n",
    "            meta_csv,\n",
    "            phoneme_alignments_folder,\n",
    "            duration_folder,\n",
    "            pitch_folder,\n",
    "            pitch_n_fft,\n",
    "            pitch_hop_length,\n",
    "            pitch_min_f0,\n",
    "            pitch_max_f0,\n",
    "            use_custom_cleaner,\n",
    "            device,\n",
    "        )\n",
    "    save_pkl(conf, save_opt)\n",
    "\n",
    "\n",
    "def skip(splits, save_folder, conf):\n",
    "    \"\"\"\n",
    "    Detects if the ljspeech data_preparation has been already done.\n",
    "    If the preparation has been done, we can skip it.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    splits : list\n",
    "        The portions of data to review.\n",
    "    save_folder : str\n",
    "        The path to the directory containing prepared files.\n",
    "    conf : dict\n",
    "        Configuration to match against saved config.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        if True, the preparation phase can be skipped.\n",
    "        if False, it must be done.\n",
    "    \"\"\"\n",
    "    # Checking json files\n",
    "    skip = True\n",
    "\n",
    "    split_files = {\n",
    "        \"train\": TRAIN_JSON,\n",
    "        \"valid\": VALID_JSON,\n",
    "        \"test\": TEST_JSON,\n",
    "    }\n",
    "\n",
    "    for split in splits:\n",
    "        if not os.path.isfile(os.path.join(save_folder, split_files[split])):\n",
    "            skip = False\n",
    "\n",
    "    #  Checking saved options\n",
    "    save_opt = os.path.join(save_folder, OPT_FILE)\n",
    "    if skip is True:\n",
    "        if os.path.isfile(save_opt):\n",
    "            opts_old = load_pkl(save_opt)\n",
    "            if opts_old == conf:\n",
    "                skip = True\n",
    "            else:\n",
    "                skip = False\n",
    "        else:\n",
    "            skip = False\n",
    "    return skip\n",
    "\n",
    "\n",
    "def split_sets(data_folder, splits, split_ratio):\n",
    "    \"\"\"Randomly splits the wav list into training, validation, and test lists.\n",
    "    Note that a better approach is to make sure that all the classes have the\n",
    "    same proportion of samples for each session.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    data_folder : str\n",
    "        The path to the directory containing the data.\n",
    "    splits : list\n",
    "        The list of the selected splits.\n",
    "    split_ratio : list\n",
    "        List composed of three integers that sets split ratios for train,\n",
    "        valid, and test sets, respectively.\n",
    "        For instance split_ratio=[80, 10, 10] will assign 80% of the sentences\n",
    "        to training, 10% for validation, and 10% for test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary containing train, valid, and test splits.\n",
    "    \"\"\"\n",
    "    meta_csv = os.path.join(data_folder, METADATA_CSV)\n",
    "    csv_reader = csv.reader(\n",
    "        open(meta_csv), delimiter=\"|\", quoting=csv.QUOTE_NONE\n",
    "    )\n",
    "\n",
    "    meta_csv = list(csv_reader)\n",
    "\n",
    "    index_for_sessions = []\n",
    "    session_id_start = \"LJ001\"\n",
    "    index_this_session = []\n",
    "    for i in range(len(meta_csv)):\n",
    "        session_id = meta_csv[i][0].split(\"-\")[0]\n",
    "        if session_id == session_id_start:\n",
    "            index_this_session.append(i)\n",
    "            if i == len(meta_csv) - 1:\n",
    "                index_for_sessions.append(index_this_session)\n",
    "        else:\n",
    "            index_for_sessions.append(index_this_session)\n",
    "            session_id_start = session_id\n",
    "            index_this_session = [i]\n",
    "\n",
    "    session_len = [len(session) for session in index_for_sessions]\n",
    "\n",
    "    data_split = {}\n",
    "    for i, split in enumerate(splits):\n",
    "        data_split[split] = []\n",
    "        for j in range(len(index_for_sessions)):\n",
    "            if split == \"train\":\n",
    "                random.shuffle(index_for_sessions[j])\n",
    "                n_snts = int(session_len[j] * split_ratio[i] / sum(split_ratio))\n",
    "                data_split[split].extend(index_for_sessions[j][0:n_snts])\n",
    "                del index_for_sessions[j][0:n_snts]\n",
    "            if split == \"valid\":\n",
    "                if \"test\" in splits:\n",
    "                    random.shuffle(index_for_sessions[j])\n",
    "                    n_snts = int(\n",
    "                        session_len[j] * split_ratio[i] / sum(split_ratio)\n",
    "                    )\n",
    "                    data_split[split].extend(index_for_sessions[j][0:n_snts])\n",
    "                    del index_for_sessions[j][0:n_snts]\n",
    "                else:\n",
    "                    data_split[split].extend(index_for_sessions[j])\n",
    "            if split == \"test\":\n",
    "                data_split[split].extend(index_for_sessions[j])\n",
    "\n",
    "    return data_split, meta_csv\n",
    "\n",
    "\n",
    "def prepare_json(\n",
    "    model_name,\n",
    "    seg_lst,\n",
    "    json_file,\n",
    "    wavs_folder,\n",
    "    csv_reader,\n",
    "    phoneme_alignments_folder,\n",
    "    durations_folder,\n",
    "    pitch_folder,\n",
    "    pitch_n_fft,\n",
    "    pitch_hop_length,\n",
    "    pitch_min_f0,\n",
    "    pitch_max_f0,\n",
    "    use_custom_cleaner=False,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates json file given a list of indexes.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    model_name : str\n",
    "        Model name (used to prepare additional model specific data)\n",
    "    seg_lst : list\n",
    "        The list of json indexes of a given data split\n",
    "    json_file : str\n",
    "        Output json path\n",
    "    wavs_folder : str\n",
    "        LJspeech wavs folder\n",
    "    csv_reader : _csv.reader\n",
    "        LJspeech metadata\n",
    "    phoneme_alignments_folder : path\n",
    "        Path where the phoneme alignments are stored\n",
    "    durations_folder : path\n",
    "        Folder where to store the duration values of each audio\n",
    "    pitch_folder : path\n",
    "        Folder where to store the pitch of each audio\n",
    "    pitch_n_fft : int\n",
    "        Number of fft points for pitch computation\n",
    "    pitch_hop_length : int\n",
    "        Hop length for pitch computation\n",
    "    pitch_min_f0 : int\n",
    "        Minimum f0 for pitch computation\n",
    "    pitch_max_f0 : int\n",
    "        Max f0 for pitch computation\n",
    "    use_custom_cleaner : bool\n",
    "        If True, uses custom cleaner defined for this recipe\n",
    "    device : str\n",
    "        Device for to be used for computation (used as required)\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"preparing {json_file}.\")\n",
    "    if model_name in [\"Tacotron2\", \"FastSpeech2WithAlignment\", \"TransformerTTS\"]:\n",
    "        logger.info(\n",
    "            \"Computing phonemes for LJSpeech labels using SpeechBrain G2P. This may take a while.\"\n",
    "        )\n",
    "        g2p = GraphemeToPhoneme.from_hparams(\n",
    "            \"speechbrain/soundchoice-g2p\", run_opts={\"device\": device}\n",
    "        )\n",
    "    if model_name is not None and \"FastSpeech2\" in model_name:\n",
    "        logger.info(\n",
    "            \"Computing pitch as required for FastSpeech2. This may take a while.\"\n",
    "        )\n",
    "\n",
    "    json_dict = {}\n",
    "    for index in tqdm(seg_lst):\n",
    "        # Common data preparation\n",
    "        id = list(csv_reader)[index][0]\n",
    "        wav = os.path.join(wavs_folder, f\"{id}.wav\")\n",
    "        label = list(csv_reader)[index][2]\n",
    "        if use_custom_cleaner:\n",
    "            label = custom_clean(label, model_name)\n",
    "\n",
    "        json_dict[id] = {\n",
    "            \"uttid\": id,\n",
    "            \"wav\": wav,\n",
    "            \"label\": label,\n",
    "            \"segment\": True if \"train\" in json_file else False,\n",
    "        }\n",
    "\n",
    "        # FastSpeech2 specific data preparation\n",
    "        if model_name == \"FastSpeech2\":\n",
    "            audio, fs = torchaudio.load(wav)\n",
    "\n",
    "            # Parses phoneme alignments\n",
    "            textgrid_path = os.path.join(\n",
    "                phoneme_alignments_folder, f\"{id}.TextGrid\"\n",
    "            )\n",
    "            textgrid = tgt.io.read_textgrid(\n",
    "                textgrid_path, include_empty_intervals=True\n",
    "            )\n",
    "\n",
    "            last_phoneme_flags = get_last_phoneme_info(\n",
    "                textgrid.get_tier_by_name(\"words\"),\n",
    "                textgrid.get_tier_by_name(\"phones\"),\n",
    "            )\n",
    "            (\n",
    "                phonemes,\n",
    "                duration,\n",
    "                start,\n",
    "                end,\n",
    "                trimmed_last_phoneme_flags,\n",
    "            ) = get_alignment(\n",
    "                textgrid.get_tier_by_name(\"phones\"),\n",
    "                fs,\n",
    "                pitch_hop_length,\n",
    "                last_phoneme_flags,\n",
    "            )\n",
    "\n",
    "            # Gets label phonemes\n",
    "            label_phoneme = \" \".join(phonemes)\n",
    "            spn_labels = [0] * len(phonemes)\n",
    "            for i in range(1, len(phonemes)):\n",
    "                if phonemes[i] == \"spn\":\n",
    "                    spn_labels[i - 1] = 1\n",
    "            if start >= end:\n",
    "                print(f\"Skipping {id}\")\n",
    "                continue\n",
    "\n",
    "            # Saves durations\n",
    "            duration_file_path = os.path.join(durations_folder, f\"{id}.npy\")\n",
    "            np.save(duration_file_path, duration)\n",
    "\n",
    "            # Computes pitch\n",
    "            audio = audio[:, int(fs * start) : int(fs * end)]\n",
    "            pitch_file = wav.replace(\".wav\", \".npy\").replace(\n",
    "                wavs_folder, pitch_folder\n",
    "            )\n",
    "            if not os.path.isfile(pitch_file):\n",
    "                pitch = torchaudio.functional.detect_pitch_frequency(\n",
    "                    waveform=audio,\n",
    "                    sample_rate=fs,\n",
    "                    frame_time=(pitch_hop_length / fs),\n",
    "                    win_length=3,\n",
    "                    freq_low=pitch_min_f0,\n",
    "                    freq_high=pitch_max_f0,\n",
    "                ).squeeze(0)\n",
    "\n",
    "                # Concatenate last element to match duration.\n",
    "                pitch = torch.cat([pitch, pitch[-1].unsqueeze(0)])\n",
    "\n",
    "                # Mean and Variance Normalization\n",
    "                mean = 256.1732939688805\n",
    "                std = 328.319759158607\n",
    "\n",
    "                pitch = (pitch - mean) / std\n",
    "\n",
    "                pitch = pitch[: sum(duration)]\n",
    "                np.save(pitch_file, pitch)\n",
    "\n",
    "            # Updates data for the utterance\n",
    "            json_dict[id].update({\"label_phoneme\": label_phoneme})\n",
    "            json_dict[id].update({\"spn_labels\": spn_labels})\n",
    "            json_dict[id].update({\"start\": start})\n",
    "            json_dict[id].update({\"end\": end})\n",
    "            json_dict[id].update({\"durations\": duration_file_path})\n",
    "            json_dict[id].update({\"pitch\": pitch_file})\n",
    "            json_dict[id].update(\n",
    "                {\"last_phoneme_flags\": trimmed_last_phoneme_flags}\n",
    "            )\n",
    "\n",
    "        # FastSpeech2WithAlignment specific data preparation\n",
    "        if model_name == \"FastSpeech2WithAlignment\":\n",
    "            audio, fs = torchaudio.load(wav)\n",
    "            # Computes pitch\n",
    "            pitch_file = wav.replace(\".wav\", \".npy\").replace(\n",
    "                wavs_folder, pitch_folder\n",
    "            )\n",
    "            if not os.path.isfile(pitch_file):\n",
    "                if torchaudio.__version__ < \"2.1\":\n",
    "                    pitch = torchaudio.functional.compute_kaldi_pitch(\n",
    "                        waveform=audio,\n",
    "                        sample_rate=fs,\n",
    "                        frame_length=(pitch_n_fft / fs * 1000),\n",
    "                        frame_shift=(pitch_hop_length / fs * 1000),\n",
    "                        min_f0=pitch_min_f0,\n",
    "                        max_f0=pitch_max_f0,\n",
    "                    )[0, :, 0]\n",
    "                else:\n",
    "                    pitch = torchaudio.functional.detect_pitch_frequency(\n",
    "                        waveform=audio,\n",
    "                        sample_rate=fs,\n",
    "                        frame_time=(pitch_hop_length / fs),\n",
    "                        win_length=3,\n",
    "                        freq_low=pitch_min_f0,\n",
    "                        freq_high=pitch_max_f0,\n",
    "                    ).squeeze(0)\n",
    "\n",
    "                    # Concatenate last element to match duration.\n",
    "                    pitch = torch.cat([pitch, pitch[-1].unsqueeze(0)])\n",
    "\n",
    "                    # Mean and Variance Normalization\n",
    "                    mean = 256.1732939688805\n",
    "                    std = 328.319759158607\n",
    "\n",
    "                    pitch = (pitch - mean) / std\n",
    "\n",
    "                np.save(pitch_file, pitch)\n",
    "\n",
    "            phonemes = _g2p_keep_punctuations(g2p, label)\n",
    "            # Updates data for the utterance\n",
    "            json_dict[id].update({\"phonemes\": phonemes})\n",
    "            json_dict[id].update({\"pitch\": pitch_file})\n",
    "\n",
    "        if model_name == \"TransformerTTS\":\n",
    "            print(f\"Id: {id}/// Label: {label}\")\n",
    "            phonemes = _g2p_keep_punctuations(g2p, label)\n",
    "            # Updates data for the utterance\n",
    "            json_dict[id].update({\"phonemes\": phonemes})\n",
    "\n",
    "    # Writing the dictionary to the json file\n",
    "    with open(json_file, mode=\"w\") as json_f:\n",
    "        json.dump(json_dict, json_f, indent=2)\n",
    "\n",
    "    logger.info(f\"{json_file} successfully created!\")\n",
    "\n",
    "\n",
    "def get_alignment(tier, sampling_rate, hop_length, last_phoneme_flags):\n",
    "    \"\"\"\n",
    "    Returns phonemes, phoneme durations (in frames), start time (in seconds), end time (in seconds).\n",
    "    This function is adopted from https://github.com/ming024/FastSpeech2/blob/master/preprocessor/preprocessor.py\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    tier : tgt.core.IntervalTier\n",
    "        For an utterance, contains Interval objects for phonemes and their start time and end time in seconds\n",
    "    sampling_rate : int\n",
    "        Sample rate if audio signal\n",
    "    hop_length : int\n",
    "        Hop length for duration computation\n",
    "    last_phoneme_flags : list\n",
    "        List of (phoneme, flag) tuples with flag=1 if the phoneme is the last phoneme else flag=0\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (phones, durations, start_time, end_time) : tuple\n",
    "        The phonemes, durations, start time, and end time for an utterance\n",
    "    \"\"\"\n",
    "\n",
    "    sil_phones = [\"sil\", \"sp\", \"spn\", \"\"]\n",
    "\n",
    "    phonemes = []\n",
    "    durations = []\n",
    "    start_time = 0\n",
    "    end_time = 0\n",
    "    end_idx = 0\n",
    "    trimmed_last_phoneme_flags = []\n",
    "\n",
    "    flag_iter = iter(last_phoneme_flags)\n",
    "\n",
    "    for t in tier._objects:\n",
    "        s, e, p = t.start_time, t.end_time, t.text\n",
    "        current_flag = next(flag_iter)\n",
    "\n",
    "        # Trims leading silences\n",
    "        if phonemes == []:\n",
    "            if p in sil_phones:\n",
    "                continue\n",
    "            else:\n",
    "                start_time = s\n",
    "\n",
    "        if p not in sil_phones:\n",
    "            # For ordinary phones\n",
    "            # Removes stress indicators\n",
    "            if p[-1].isdigit():\n",
    "                phonemes.append(p[:-1])\n",
    "            else:\n",
    "                phonemes.append(p)\n",
    "            trimmed_last_phoneme_flags.append(current_flag[1])\n",
    "            end_time = e\n",
    "            end_idx = len(phonemes)\n",
    "        else:\n",
    "            # Uses a unique token for all silent phones\n",
    "            phonemes.append(\"spn\")\n",
    "            trimmed_last_phoneme_flags.append(current_flag[1])\n",
    "\n",
    "        durations.append(\n",
    "            int(\n",
    "                np.round(e * sampling_rate / hop_length)\n",
    "                - np.round(s * sampling_rate / hop_length)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Trims tailing silences\n",
    "    phonemes = phonemes[:end_idx]\n",
    "    durations = durations[:end_idx]\n",
    "\n",
    "    return phonemes, durations, start_time, end_time, trimmed_last_phoneme_flags\n",
    "\n",
    "\n",
    "def get_last_phoneme_info(words_seq, phones_seq):\n",
    "    \"\"\"This function takes word and phoneme tiers from a TextGrid file as input\n",
    "    and provides a list of tuples for the phoneme sequence indicating whether\n",
    "    each of the phonemes is the last phoneme of a word or not.\n",
    "\n",
    "    Each tuple of the returned list has this format: (phoneme, flag)\n",
    "\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    words_seq : tier\n",
    "        word tier from a TextGrid file\n",
    "    phones_seq : tier\n",
    "        phoneme tier from a TextGrid file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    last_phoneme_flags : list\n",
    "        each tuple of the returned list has this format: (phoneme, flag)\n",
    "    \"\"\"\n",
    "\n",
    "    # Gets all phoneme objects for the entire sequence\n",
    "    phoneme_objects = phones_seq._objects\n",
    "    phoneme_iter = iter(phoneme_objects)\n",
    "\n",
    "    # Stores flags to show if an element (phoneme) is a the last phoneme of a word\n",
    "    last_phoneme_flags = list()\n",
    "\n",
    "    # Matches the end times of the phoneme and word objects to get the last phoneme information\n",
    "    for word_obj in words_seq._objects:\n",
    "        word_end_time = word_obj.end_time\n",
    "\n",
    "        current_phoneme = next(phoneme_iter, None)\n",
    "        while current_phoneme:\n",
    "            phoneme_end_time = current_phoneme.end_time\n",
    "            if phoneme_end_time == word_end_time:\n",
    "                last_phoneme_flags.append((current_phoneme.text, 1))\n",
    "                break\n",
    "            else:\n",
    "                last_phoneme_flags.append((current_phoneme.text, 0))\n",
    "            current_phoneme = next(phoneme_iter, None)\n",
    "\n",
    "    return last_phoneme_flags\n",
    "\n",
    "\n",
    "def custom_clean(text, model_name):\n",
    "    \"\"\"\n",
    "    Uses custom criteria to clean text.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    text : str\n",
    "        Input text to be cleaned\n",
    "    model_name : str\n",
    "        whether to treat punctuations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    _abbreviations = [\n",
    "        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n",
    "        for x in [\n",
    "            (\"mrs\", \"missus\"),\n",
    "            (\"mr\", \"mister\"),\n",
    "            (\"dr\", \"doctor\"),\n",
    "            (\"st\", \"saint\"),\n",
    "            (\"co\", \"company\"),\n",
    "            (\"jr\", \"junior\"),\n",
    "            (\"maj\", \"major\"),\n",
    "            (\"gen\", \"general\"),\n",
    "            (\"drs\", \"doctors\"),\n",
    "            (\"rev\", \"reverend\"),\n",
    "            (\"lt\", \"lieutenant\"),\n",
    "            (\"hon\", \"honorable\"),\n",
    "            (\"sgt\", \"sergeant\"),\n",
    "            (\"capt\", \"captain\"),\n",
    "            (\"esq\", \"esquire\"),\n",
    "            (\"ltd\", \"limited\"),\n",
    "            (\"col\", \"colonel\"),\n",
    "            (\"ft\", \"fort\"),\n",
    "        ]\n",
    "    ]\n",
    "    text = unidecode(text.lower())\n",
    "    if model_name != \"FastSpeech2WithAlignment\":\n",
    "        text = re.sub(\"[:;]\", \" - \", text)\n",
    "        text = re.sub(r'[)(\\[\\]\"]', \" \", text)\n",
    "        text = text.strip().strip().strip(\"-\")\n",
    "\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    for regex, replacement in _abbreviations:\n",
    "        text = re.sub(regex, replacement, text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dryclaz8E0T",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Relevant classes and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-pads the model's inputs(phonemes) and targets(mel spectogram) based on number of frames per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ga0XDqT-8JuE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextMelCollate:\n",
    "    \"\"\"Zero-pads model inputs and targets based on number of frames per step\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    n_frames_per_step: int\n",
    "        the number of output frames per step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_frames_per_step=1):\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "\n",
    "    # TODO: Make this more intuitive, use the pipeline\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from normalized text and mel-spectrogram\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch: list\n",
    "            [text_normalized, mel_normalized]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text_padded: torch.Tensor\n",
    "        input_lengths: torch.Tensor\n",
    "        mel_padded: torch.Tensor\n",
    "        gate_padded: torch.Tensor\n",
    "        output_lengths: torch.Tensor\n",
    "        len_x: torch.Tensor\n",
    "        labels: torch.Tensor\n",
    "        wavs: torch.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Remove for loops and this dirty hack\n",
    "        raw_batch = list(batch)\n",
    "        for i in range(\n",
    "                len(batch)\n",
    "        ):  # the pipeline return a dictionary with one element\n",
    "            batch[i] = batch[i][\"mel_text_pair\"]\n",
    "\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x[0]) for x in batch]), dim=0, descending=True\n",
    "        )\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            text = batch[ids_sorted_decreasing[i]][0]\n",
    "            text_padded[i, : text.size(0)] = text\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0][1].size(0)\n",
    "        max_target_len = max([x[1].size(1) for x in batch])\n",
    "        if max_target_len % self.n_frames_per_step != 0:\n",
    "            max_target_len += (\n",
    "                    self.n_frames_per_step - max_target_len % self.n_frames_per_step\n",
    "            )\n",
    "            assert max_target_len % self.n_frames_per_step == 0\n",
    "\n",
    "        # include mel padded and gate padded\n",
    "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_target_len)\n",
    "        mel_padded.zero_()\n",
    "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
    "        gate_padded.zero_()\n",
    "        output_lengths = torch.LongTensor(len(batch))\n",
    "        labels, wavs = [], []\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            idx = ids_sorted_decreasing[i]\n",
    "            mel = batch[idx][1]\n",
    "            mel_padded[i, :, : mel.size(1)] = mel\n",
    "            gate_padded[i, mel.size(1) - 1:] = 1\n",
    "            output_lengths[i] = mel.size(1)\n",
    "            labels.append(raw_batch[idx][\"label\"])\n",
    "            wavs.append(raw_batch[idx][\"wav\"])\n",
    "\n",
    "        # count number of items - characters in text\n",
    "        len_x = [x[2] for x in batch]\n",
    "        len_x = torch.Tensor(len_x)\n",
    "        return (\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            gate_padded,\n",
    "            output_lengths,\n",
    "            len_x,\n",
    "            labels,\n",
    "            wavs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmiG5th28L5S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
    "    \"\"\"Dynamic range compression for audio signals\"\"\"\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for the calculation of the audio signal's mel spectogram in the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOfhcV8l8Y6x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mel_spectogram(\n",
    "        sample_rate,\n",
    "        hop_length,\n",
    "        win_length,\n",
    "        n_fft,\n",
    "        n_mels,\n",
    "        f_min,\n",
    "        f_max,\n",
    "        power,\n",
    "        normalized,\n",
    "        norm,\n",
    "        mel_scale,\n",
    "        compression,\n",
    "        audio,\n",
    "):\n",
    "    \"\"\"calculates MelSpectrogram for a raw audio signal\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    sample_rate : int\n",
    "        Sample rate of audio signal.\n",
    "    hop_length : int\n",
    "        Length of hop between STFT windows.\n",
    "    win_length : int\n",
    "        Window size.\n",
    "    n_fft : int\n",
    "        Size of FFT.\n",
    "    n_mels : int\n",
    "        Number of mel filterbanks.\n",
    "    f_min : float\n",
    "        Minimum frequency.\n",
    "    f_max : float\n",
    "        Maximum frequency.\n",
    "    power : float\n",
    "        Exponent for the magnitude spectrogram.\n",
    "    normalized : bool\n",
    "        Whether to normalize by magnitude after stft.\n",
    "    norm : str or None\n",
    "        If \"slaney\", divide the triangular mel weights by the width of the mel band\n",
    "    mel_scale : str\n",
    "        Scale to use: \"htk\" or \"slaney\".\n",
    "    compression : bool\n",
    "        whether to do dynamic range compression\n",
    "    audio : torch.Tensor\n",
    "        input audio signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mel : torch.Tensor\n",
    "        The computed mel spectrogram features.\n",
    "    \"\"\"\n",
    "    from torchaudio import transforms\n",
    "\n",
    "    audio_to_mel = transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        n_fft=n_fft,\n",
    "        n_mels=n_mels,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        power=power,\n",
    "        normalized=normalized,\n",
    "        norm=norm,\n",
    "        mel_scale=mel_scale,\n",
    "    ).to(audio.device)\n",
    "\n",
    "    mel = audio_to_mel(audio)\n",
    "\n",
    "    if compression:\n",
    "        mel = dynamic_range_compression(mel)\n",
    "\n",
    "    return mel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWJRM8tL8epE",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Qha3G9l8gSF"
   },
   "outputs": [],
   "source": [
    "def infer(model, text_sequences, input_lengths):\n",
    "    \"\"\"\n",
    "    An inference hook for pretrained synthesizers\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    model: TransformerTTS\n",
    "        the TransformerTTS model\n",
    "    text_sequences: torch.Tensor\n",
    "        encoded text sequences\n",
    "    input_lengths: torch.Tensor\n",
    "        input lengths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: tuple\n",
    "        (mel_outputs_postnet, mel_lengths) - the exact\n",
    "        model output\n",
    "    \"\"\"\n",
    "    return model.infer(text_sequences, input_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJn0PxwdszL4",
    "tags": []
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PF9Xl2Nxs3-w",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "35738f3c-a6fb-49e6-d3aa-b3e51bd50825",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%file /notebooks/train.yaml\n",
    "############################################################################\n",
    "# Model: TransformerTTS\n",
    "# Tokens: Phonemes (ARPABET)\n",
    "# Training: LJSpeech\n",
    "# Authors: Salman Hussain Ali, 2024\n",
    "# ############################################################################\n",
    "\n",
    "\n",
    "###################################\n",
    "# Experiment Parameters and setup #\n",
    "###################################\n",
    "seed: 1234\n",
    "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
    "output_folder: !ref ./results/transformertts/<seed>\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "epochs: 750\n",
    "keep_checkpoint_interval: 50\n",
    "model_name: \"TransformerTTS\"\n",
    "\n",
    "###################################\n",
    "# Progress Samples                #\n",
    "###################################\n",
    "# Progress samples are used to monitor the progress\n",
    "# of an ongoing training session by outputting samples\n",
    "# of spectrograms, alignments, etc at regular intervals\n",
    "\n",
    "# Whether to enable progress samples\n",
    "progress_samples: True\n",
    "\n",
    "# The path where the samples will be stored\n",
    "progress_sample_path: !ref <output_folder>/samples\n",
    "# The interval, in epochs. For instance, if it is set to 5,\n",
    "# progress samples will be output every 5 epochs\n",
    "progress_samples_interval: 1\n",
    "# The sample size for raw batch samples saved in batch.pth\n",
    "# (useful mostly for model debugging)\n",
    "progress_batch_sample_size: 3\n",
    "\n",
    "#################################\n",
    "# Data files and pre-processing #\n",
    "#################################\n",
    "data_folder: !PLACEHOLDER # e.g, /localscratch/ljspeech\n",
    "\n",
    "train_json: !ref <save_folder>/train.json\n",
    "valid_json: !ref <save_folder>/valid.json\n",
    "test_json: !ref <save_folder>/test.json\n",
    "\n",
    "splits: [\"train\",\"valid\"]\n",
    "split_ratio: [90,10]\n",
    "\n",
    "skip_prep: False\n",
    "\n",
    "# Use the original preprocessing from nvidia\n",
    "# The cleaners to be used (applicable to nvidia only)\n",
    "text_cleaners: ['english_cleaners']\n",
    "\n",
    "################################\n",
    "# Audio Parameters             #\n",
    "################################\n",
    "sample_rate: 16000\n",
    "hop_length: 200\n",
    "win_length: 1024\n",
    "n_mel_channels: 80\n",
    "n_fft: 1024\n",
    "mel_fmin: 0.0\n",
    "mel_fmax: 8000.0\n",
    "mel_normalized: False\n",
    "power: 1\n",
    "norm: \"slaney\"\n",
    "mel_scale: \"slaney\"\n",
    "dynamic_range_compression: True\n",
    "\n",
    "################################\n",
    "# Optimization Hyperparameters #\n",
    "################################\n",
    "learning_rate: 0.001\n",
    "weight_decay: 0.000006\n",
    "batch_size: 32 #minimum 2\n",
    "num_workers: 8\n",
    "mask_padding: True\n",
    "gate_loss_weight: 7.0\n",
    "\n",
    "train_dataloader_opts:\n",
    "  batch_size: !ref <batch_size>\n",
    "  drop_last: True  #True #False\n",
    "  num_workers: !ref <num_workers>\n",
    "  collate_fn: !new:models.TransformerTTS.TextMelCollate\n",
    "\n",
    "valid_dataloader_opts:\n",
    "  batch_size: !ref <batch_size>\n",
    "  num_workers: !ref <num_workers>\n",
    "  collate_fn: !new:models.TransformerTTS.TextMelCollate\n",
    "\n",
    "test_dataloader_opts:\n",
    "  batch_size: !ref <batch_size>\n",
    "  num_workers: !ref <num_workers>\n",
    "  collate_fn: !new:models.TransformerTTS.TextMelCollate\n",
    "\n",
    "################################\n",
    "# Model Parameters and model   #\n",
    "################################\n",
    "\n",
    "# Input parameters\n",
    "lexicon:\n",
    "    - \"AA\"\n",
    "    - \"AE\"\n",
    "    - \"AH\"\n",
    "    - \"AO\"\n",
    "    - \"AW\"\n",
    "    - \"AY\"\n",
    "    - \"B\"\n",
    "    - \"CH\"\n",
    "    - \"D\"\n",
    "    - \"DH\"\n",
    "    - \"EH\"\n",
    "    - \"ER\"\n",
    "    - \"EY\"\n",
    "    - \"F\"\n",
    "    - \"G\"\n",
    "    - \"HH\"\n",
    "    - \"IH\"\n",
    "    - \"IY\"\n",
    "    - \"JH\"\n",
    "    - \"K\"\n",
    "    - \"L\"\n",
    "    - \"M\"\n",
    "    - \"N\"\n",
    "    - \"NG\"\n",
    "    - \"OW\"\n",
    "    - \"OY\"\n",
    "    - \"P\"\n",
    "    - \"R\"\n",
    "    - \"S\"\n",
    "    - \"SH\"\n",
    "    - \"T\"\n",
    "    - \"TH\"\n",
    "    - \"UH\"\n",
    "    - \"UW\"\n",
    "    - \"V\"\n",
    "    - \"W\"\n",
    "    - \"Y\"\n",
    "    - \"Z\"\n",
    "    - \"ZH\"\n",
    "    - \"-\"\n",
    "    - \"!\"\n",
    "    - \"'\"\n",
    "    - \"(\"\n",
    "    - \")\"\n",
    "    - \",\"\n",
    "    - \".\"\n",
    "    - \":\"\n",
    "    - \";\"\n",
    "    - \"?\"\n",
    "    - \" \"\n",
    "\n",
    "n_symbols: 52 #fixed depending on symbols in the lexicon (+1 for a dummy symbol used for padding, +1 for unknown)\n",
    "padding_idx: 0\n",
    "symbols_embedding_dim: 512\n",
    "\n",
    "# Encoder Pre-Net parameters\n",
    "encoder_kernel_size: 5\n",
    "encoder_n_convolutions: 3\n",
    "encoder_embedding_dim: 512\n",
    "encoder_stride: 1\n",
    "encoder_prenet_dropout: 0.1\n",
    "encoder_dilation: 1\n",
    "encoder_bias: True\n",
    "encoder_padding: \"same\"\n",
    "\n",
    "# Decoder Pre-Net parameters\n",
    "# The number of frames in the target per encoder step\n",
    "n_frames_per_step: 1\n",
    "decoder_prenet_dim: 256\n",
    "max_decoder_steps: 1000\n",
    "p_decoder_dropout: 0.5\n",
    "\n",
    "# Transformer parameters\n",
    "d_model: 512\n",
    "transformer_nhead: 8\n",
    "transformer_num_encoder_layers: 6\n",
    "transformer_num_decoder_layers: 6\n",
    "transformer_d_ffn: 2048\n",
    "transformer_dropout: 0.1\n",
    "transformer_activation: \"relu\"\n",
    "\n",
    "# Mel-post processing network parameters\n",
    "postnet_embedding_dim: 512\n",
    "postnet_kernel_size: 5\n",
    "postnet_n_convolutions: 5\n",
    "gate_threshold: 0.5 # TODO - maybe remove\n",
    "\n",
    "\n",
    "mel_spectogram: !name:models.TransformerTTS.mel_spectogram\n",
    "  sample_rate: !ref <sample_rate>\n",
    "  hop_length: !ref <hop_length>\n",
    "  win_length: !ref <win_length>\n",
    "  n_fft: !ref <n_fft>\n",
    "  n_mels: !ref <n_mel_channels>\n",
    "  f_min: !ref <mel_fmin>\n",
    "  f_max: !ref <mel_fmax>\n",
    "  power: !ref <power>\n",
    "  normalized: !ref <mel_normalized>\n",
    "  norm: !ref <norm>\n",
    "  mel_scale: !ref <mel_scale>\n",
    "  compression: !ref <dynamic_range_compression>\n",
    "\n",
    "#model\n",
    "model: !new:models.TransformerTTS.TransformerTTS\n",
    "  mask_padding: !ref <mask_padding>\n",
    "  n_mel_channels: !ref <n_mel_channels>\n",
    "  # symbols\n",
    "  n_symbols: !ref <n_symbols>\n",
    "  symbols_embedding_dim: !ref <symbols_embedding_dim>\n",
    "  # encoder pre-net\n",
    "  encoder_prenet_kernel_size: !ref <encoder_kernel_size>\n",
    "  encoder_prenet_n_convolutions: !ref <encoder_n_convolutions>\n",
    "  encoder_prenet_dilation: !ref <encoder_dilation>\n",
    "  encoder_prenet_bias: !ref <encoder_bias>\n",
    "  encoder_prenet_dropout: !ref <encoder_prenet_dropout>\n",
    "  encoder_prenet_stride: !ref <encoder_stride>\n",
    "  # decoder pre-net\n",
    "  n_frames_per_step: !ref <n_frames_per_step>\n",
    "  decoder_prenet_hidden_dims: !ref <decoder_prenet_dim>\n",
    "  decoder_prenet_dropout: !ref <p_decoder_dropout>\n",
    "  # transformer\n",
    "  d_model: !ref <d_model>\n",
    "  transformer_nhead: !ref <transformer_nhead>\n",
    "  transformer_num_encoder_layers: !ref <transformer_num_encoder_layers>\n",
    "  transformer_num_decoder_layers: !ref <transformer_num_decoder_layers>\n",
    "  transformer_d_ffn: !ref <transformer_d_ffn>\n",
    "  transformer_dropout: !ref <transformer_dropout>\n",
    "  transformer_activation: !ref <transformer_activation>\n",
    "  # postnet\n",
    "  postnet_embedding_dim: !ref <postnet_embedding_dim>\n",
    "  postnet_kernel_size: !ref <postnet_kernel_size>\n",
    "  postnet_n_convolutions: !ref <postnet_n_convolutions>\n",
    "  #decoder_no_early_stopping: !ref <decoder_no_early_stopping>\n",
    "  gate_threshold: !ref <gate_threshold>\n",
    "  padding_idx: !ref <padding_idx>\n",
    "\n",
    "criterion: !new:models.TransformerTTS.Loss\n",
    "  gate_loss_weight: !ref <gate_loss_weight>\n",
    "\n",
    "# Masks\n",
    "lookahead_mask: !name:speechbrain.lobes.models.transformer.Transformer.get_lookahead_mask\n",
    "padding_mask: !name:speechbrain.lobes.models.transformer.Transformer.get_mask_from_lengths\n",
    "\n",
    "modules:\n",
    "  model: !ref <model>\n",
    "\n",
    "#optimizer\n",
    "opt_class: !name:torch.optim.Adam\n",
    "  lr: !ref <learning_rate>\n",
    "  weight_decay: !ref <weight_decay>\n",
    "\n",
    "#epoch object\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "  limit: !ref <epochs>\n",
    "\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "  save_file: !ref <train_log>\n",
    "\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.NoamScheduler\n",
    "    lr_initial: !ref <learning_rate>\n",
    "    n_warmup_steps: 25000\n",
    "\n",
    "#checkpointer\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "  checkpoints_dir: !ref <save_folder>\n",
    "  recoverables:\n",
    "    model: !ref <model>\n",
    "    counter: !ref <epoch_counter>\n",
    "    scheduler: !ref <lr_annealing>\n",
    "\n",
    "infer: !name:models.TransformerTTS.infer\n",
    "\n",
    "progress_sample_logger: !new:speechbrain.utils.train_logger.ProgressSampleLogger\n",
    "  output_path: !ref <progress_sample_path>\n",
    "  batch_sample_size: !ref <progress_batch_sample_size>\n",
    "  formats:\n",
    "    raw_batch: raw\n",
    "\n",
    "input_encoder: !new:speechbrain.dataio.encoder.TextEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqFH4xs-s2E0",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Brain class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ogR27mg6qUQ",
    "outputId": "590105de-eb91-48dd-c286-fe0012f639ef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%file /notebooks/train.py\n",
    "\"\"\"\n",
    " Recipe for training the TransformerTTS Text-To-Speech model, an end-to-end\n",
    " neural text-to-speech (TTS) system\n",
    "\n",
    " To run this recipe, do the following:\n",
    " # python train.py --device=cuda:0 --max_grad_norm=1.0 --data_folder=/your_folder/LJSpeech-1.1 hparams/train.yaml\n",
    "\n",
    " to infer simply load saved model and do\n",
    " savemodel.infer(text_Sequence,len(textsequence))\n",
    "\n",
    " where text_Sequence is the output of the text_to_sequence function from\n",
    " textToSequence.py (from textToSequence import text_to_sequence)\n",
    "\n",
    " Authors\n",
    " * Salman Hussain Ali 2024\n",
    "\"\"\"\n",
    "import torch\n",
    "import speechbrain as sb\n",
    "import sys\n",
    "import logging\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "from speechbrain.inference import GraphemeToPhoneme\n",
    "from speechbrain.utils.data_utils import scalarize\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TransformerTTSBrain(sb.Brain):\n",
    "    \"\"\"The Brain implementation for TransformerTTS\"\"\"\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"Gets called at the beginning of ``fit()``, on multiple processes\n",
    "        if ``distributed_count > 0`` and backend is ddp and initializes statistics\n",
    "        \"\"\"\n",
    "        self.hparams.progress_sample_logger.reset()\n",
    "        self.last_epoch = 0\n",
    "        self.last_batch = None\n",
    "        self.last_loss_stats = {}\n",
    "        self.g2p = GraphemeToPhoneme.from_hparams(\"speechbrain/soundchoice-g2p\")\n",
    "        return super().on_fit_start()\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch: str\n",
    "            a single batch\n",
    "        stage: speechbrain.Stage\n",
    "            the training stage\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the model output\n",
    "        \"\"\"\n",
    "        # Batch is the results of TextMelCollate\n",
    "        effective_batch = self.batch_to_device(batch)\n",
    "\n",
    "        inputs, y, num_items, _, _ = effective_batch\n",
    "\n",
    "        _, input_lengths, mel, _, output_lengths = inputs\n",
    "\n",
    "        # Getting target mask (to avoid looking ahead)\n",
    "        mask_size = mel.shape[2]\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(mask_size, device=self.device)\n",
    "\n",
    "        # Padding masks for source and targets\n",
    "        src_key_padding_mask = self.hparams.padding_mask(input_lengths).to(self.device, non_blocking=True)\n",
    "        tgt_key_padding_mask = self.hparams.padding_mask(output_lengths).to(self.device, non_blocking=True)\n",
    "\n",
    "        masks = (tgt_mask, None, src_key_padding_mask, tgt_key_padding_mask)\n",
    "\n",
    "        return self.modules.model(inputs, masks)\n",
    "\n",
    "    def on_fit_batch_end(self, batch, outputs, loss, should_step):\n",
    "        \"\"\"At the end of the optimizer step, apply noam annealing.\"\"\"\n",
    "        if should_step:\n",
    "            self.hparams.lr_annealing(self.optimizer)\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions : torch.Tensor\n",
    "            The model generated spectrograms and other metrics from `compute_forward`.\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            A one-element tensor used for backpropagating the gradient.\n",
    "        \"\"\"\n",
    "        effective_batch = self.batch_to_device(batch)\n",
    "        # Hold on to the batch for the inference sample. This is needed because\n",
    "        # the inference sample is run from on_stage_end only, where\n",
    "        # batch information is not available\n",
    "        self.last_batch = effective_batch\n",
    "        # Hold on to a sample (for logging)\n",
    "        self._remember_sample(effective_batch, predictions)\n",
    "        # Compute the loss\n",
    "        loss = self._compute_loss(predictions, effective_batch, stage)\n",
    "        return loss\n",
    "\n",
    "    def _compute_loss(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the value of the loss function and updates stats\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions: tuple\n",
    "            model predictions\n",
    "        batch: PaddedBatch\n",
    "            Inputs for this training iteration.\n",
    "        stage: sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: torch.Tensor\n",
    "            the loss value\n",
    "        \"\"\"\n",
    "        inputs, targets, num_items, labels, wavs = batch\n",
    "        text_padded, input_lengths, _, max_len, output_lengths = inputs\n",
    "\n",
    "        loss_stats = self.hparams.criterion(\n",
    "            predictions, targets\n",
    "        )\n",
    "        self.last_loss_stats[stage] = scalarize(loss_stats)\n",
    "        return loss_stats.loss\n",
    "\n",
    "    def _remember_sample(self, batch, predictions):\n",
    "        \"\"\"Remembers samples of spectrograms and the batch for logging purposes\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch: tuple\n",
    "            a training batch\n",
    "        predictions: tuple\n",
    "            predictions (raw output of the TransformerTTS model)\n",
    "        \"\"\"\n",
    "        inputs, targets, num_items, labels, wavs = batch\n",
    "        text_padded, input_lengths, _, max_len, output_lengths = inputs\n",
    "        mel_target, _ = targets\n",
    "        mel_out, mel_out_postnet, gate_out = predictions\n",
    "\n",
    "        self.hparams.progress_sample_logger.remember(\n",
    "            target=self._get_spectrogram_sample(mel_target),\n",
    "            output=self._get_spectrogram_sample(mel_out),\n",
    "            output_postnet=self._get_spectrogram_sample(mel_out_postnet),\n",
    "            raw_batch=self.hparams.progress_sample_logger.get_batch_sample(\n",
    "                {\n",
    "                    \"text_padded\": text_padded,\n",
    "                    \"input_lengths\": input_lengths,\n",
    "                    \"mel_target\": mel_target,\n",
    "                    \"mel_out\": mel_out,\n",
    "                    \"mel_out_postnet\": mel_out_postnet,\n",
    "                    \"gate_out\": gate_out,\n",
    "                    \"labels\": labels,\n",
    "                    \"wavs\": wavs,\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def batch_to_device(self, batch):\n",
    "        \"\"\"Transfers the batch to the target device\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch: tuple\n",
    "            the batch to use\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        batch: tuple\n",
    "            the batch on the correct device\n",
    "        \"\"\"\n",
    "        (\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            gate_padded,\n",
    "            output_lengths,\n",
    "            len_x,\n",
    "            labels,\n",
    "            wavs,\n",
    "        ) = batch\n",
    "        text_padded = text_padded.to(self.device, non_blocking=True).long()\n",
    "        input_lengths = input_lengths.to(self.device, non_blocking=True).long()\n",
    "        max_len = torch.max(input_lengths.data).item()\n",
    "        mel_padded = mel_padded.to(self.device, non_blocking=True).float()\n",
    "        gate_padded = gate_padded.to(self.device, non_blocking=True).float()\n",
    "\n",
    "        output_lengths = output_lengths.to(\n",
    "            self.device, non_blocking=True\n",
    "        ).long()\n",
    "        x = (text_padded, input_lengths, mel_padded, max_len, output_lengths)\n",
    "        y = (mel_padded, gate_padded)\n",
    "        len_x = torch.sum(output_lengths)\n",
    "        return (x, y, len_x, labels, wavs)\n",
    "\n",
    "    def _get_spectrogram_sample(self, raw):\n",
    "        \"\"\"Converts a raw spectrogram to one that can be saved as an image\n",
    "        sample  = sqrt(exp(raw))\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        raw: torch.Tensor\n",
    "            the raw spectrogram (as used in the model)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample: torch.Tensor\n",
    "            the spectrogram, for image saving purposes\n",
    "        \"\"\"\n",
    "        sample = raw[0]\n",
    "        return torch.sqrt(torch.exp(sample))\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of an epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
    "        stage_loss : float\n",
    "            The average loss for all of the data processed in this stage.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the train loss until the validation stage\n",
    "\n",
    "        # At the end of validation, we can write\n",
    "        if stage == sb.Stage.VALID:\n",
    "            # Update learning rate\n",
    "            print(\"Stage End\")\n",
    "            lr = self.optimizer.param_groups[-1][\"lr\"]\n",
    "            self.last_epoch = epoch\n",
    "\n",
    "            # The train_logger writes a summary to stdout and to the logfile.\n",
    "            self.hparams.train_logger.log_stats(  # 1#2#\n",
    "                stats_meta={\"Epoch\": epoch, \"lr\": lr},\n",
    "                train_stats=self.last_loss_stats[sb.Stage.TRAIN],\n",
    "                valid_stats=self.last_loss_stats[sb.Stage.VALID],\n",
    "            )\n",
    "\n",
    "            # Save the current checkpoint and delete previous checkpoints.\n",
    "            epoch_metadata = {\n",
    "                **{\"epoch\": epoch},\n",
    "                **self.last_loss_stats[sb.Stage.VALID],\n",
    "            }\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta=epoch_metadata,\n",
    "                min_keys=[\"loss\"],\n",
    "                ckpt_predicate=(\n",
    "                    (\n",
    "                        lambda ckpt: (\n",
    "                                ckpt.meta[\"epoch\"]\n",
    "                                % self.hparams.keep_checkpoint_interval\n",
    "                                != 0\n",
    "                        )\n",
    "                    )\n",
    "                    if self.hparams.keep_checkpoint_interval is not None\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "            output_progress_sample = (\n",
    "                    self.hparams.progress_samples\n",
    "                    and epoch % self.hparams.progress_samples_interval == 0\n",
    "            )\n",
    "            if output_progress_sample:\n",
    "                self.run_inference_sample()\n",
    "                self.hparams.progress_sample_logger.save(epoch)\n",
    "\n",
    "        # We also write statistics about test data to stdout and to the logfile.\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=self.last_loss_stats[sb.Stage.TEST],\n",
    "            )\n",
    "            if self.hparams.progress_samples:\n",
    "                self.run_inference_sample()\n",
    "                self.hparams.progress_sample_logger.save(\"test\")\n",
    "\n",
    "    def run_inference_sample(self):\n",
    "        \"\"\"Produces a sample in inference mode. This is called when producing\n",
    "        samples and can be useful because\"\"\"\n",
    "        if self.last_batch is None:\n",
    "            return\n",
    "        inputs, _, _, _, _ = self.last_batch\n",
    "        text_padded, input_lengths, _, _, _ = inputs\n",
    "        mel_out, _, _ = self.hparams.model.infer(\n",
    "            text_padded[:1], input_lengths[:1]\n",
    "        )\n",
    "        self.hparams.progress_sample_logger.remember(\n",
    "            inference_mel_out=self._get_spectrogram_sample(mel_out)\n",
    "        )\n",
    "\n",
    "\n",
    "def dataio_prepare(hparams):\n",
    "    lexicon = hparams[\"lexicon\"]\n",
    "    input_encoder = hparams.get(\"input_encoder\")\n",
    "\n",
    "    # add a dummy symbol for idx 0 - used for padding.\n",
    "    lexicon = [\"@@\"] + lexicon\n",
    "    input_encoder.update_from_iterable(lexicon, sequence_input=False)\n",
    "    input_encoder.add_unk()\n",
    "\n",
    "    # Define audio pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wav\", \"phonemes\")\n",
    "    @sb.utils.data_pipeline.provides(\"mel_text_pair\")\n",
    "    def audio_pipeline(wav, phonemes):\n",
    "        # Calculate the mel spectrogram for the audio files\n",
    "        audio = sb.dataio.dataio.read_audio(wav)\n",
    "        mel = hparams[\"mel_spectogram\"](audio=audio)\n",
    "\n",
    "        # Encode phonemes to get the sequence of IDs corresponding to the symbols in the text.\n",
    "        encoded_phonemes = input_encoder.encode_sequence_torch(phonemes).int()\n",
    "\n",
    "        len_phonemes = len(encoded_phonemes)\n",
    "\n",
    "        return encoded_phonemes, mel, len_phonemes\n",
    "\n",
    "    datasets = {}\n",
    "    data_info = {\n",
    "        \"train\": hparams[\"train_json\"],\n",
    "        \"valid\": hparams[\"valid_json\"],\n",
    "        \"test\": hparams[\"test_json\"],\n",
    "    }\n",
    "    for dataset in hparams[\"splits\"]:\n",
    "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "            json_path=data_info[dataset],\n",
    "            replacements={\"data_root\": hparams[\"data_folder\"]},\n",
    "            dynamic_items=[audio_pipeline],\n",
    "            output_keys=[\"mel_text_pair\", \"wav\", \"label\"],\n",
    "        )\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load hyperparameters file with command-line overrides\n",
    "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
    "\n",
    "    with open(hparams_file) as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    # create ddp_group with the right communication protocol\n",
    "    sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "    # Create experiment directory\n",
    "    sb.create_experiment_directory(\n",
    "        experiment_directory=hparams[\"output_folder\"],\n",
    "        hyperparams_to_save=hparams_file,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "    from ljspeech_prepare import prepare_ljspeech\n",
    "\n",
    "    sb.utils.distributed.run_on_main(\n",
    "        prepare_ljspeech,\n",
    "        kwargs={\n",
    "            \"model_name\": hparams[\"model_name\"],\n",
    "            \"data_folder\": hparams[\"data_folder\"],\n",
    "            \"save_folder\": hparams[\"save_folder\"],\n",
    "            \"splits\": hparams[\"splits\"],\n",
    "            \"split_ratio\": hparams[\"split_ratio\"],\n",
    "            \"seed\": hparams[\"seed\"],\n",
    "            \"skip_prep\": hparams[\"skip_prep\"],\n",
    "            \"device\": run_opts[\"device\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    datasets = dataio_prepare(hparams)\n",
    "\n",
    "    # Brain class initialization\n",
    "    TransformerTTS_brain = TransformerTTSBrain(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"opt_class\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    TransformerTTS_brain.fit(\n",
    "        TransformerTTS_brain.hparams.epoch_counter,\n",
    "        train_set=datasets[\"train\"],\n",
    "        valid_set=datasets[\"valid\"],\n",
    "        train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "        valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    "    )\n",
    "\n",
    "    # Test\n",
    "    if \"test\" in datasets:\n",
    "        TransformerTTS_brain.evaluate(\n",
    "            datasets[\"test\"],\n",
    "            test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-rt2NWglKl7",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYW5vmWolMSb",
    "outputId": "4565df4c-0902-4dea-924d-c7ae41196e99",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python train.py --device=cuda:0 --max_grad_norm=1.0 --data_folder=/notebooks/LJSpeech-1.1 train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTTS(Pretrained):\n",
    "    \"\"\"\n",
    "    A ready-to-use wrapper for TransformerTTS (text -> mel_spec).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    *args : tuple\n",
    "    **kwargs : dict\n",
    "        Arguments are forwarded to ``Pretrained`` parent class.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> tmpdir_tts = getfixture('tmpdir') / \"tts\"\n",
    "    >>> transformerTTS = TransformerTTS.from_hparams(source=\"speechbrain/tts-fastspeech2-internal-alignment-ljspeech\", savedir=tmpdir_tts) # doctest: +SKIP\n",
    "    >>> mel_outputs, durations = transformerTTS.encode_text([\"Mary had a little lamb.\"]) # doctest: +SKIP\n",
    "    >>> items = [\n",
    "    ...   \"A quick brown fox jumped over the lazy dog\",\n",
    "    ...   \"How much wood would a woodchuck chuck?\",\n",
    "    ...   \"Never odd or even\"\n",
    "    ... ]\n",
    "    >>> mel_outputs = transformerTTS.encode_text(items) # doctest: +SKIP\n",
    "    >>> # One can combine the TTS model with a vocoder (that generates the final waveform)\n",
    "    >>> # Initialize the Vocoder (HiFIGAN)\n",
    "    >>> tmpdir_vocoder = getfixture('tmpdir') / \"vocoder\"\n",
    "    >>> from speechbrain.inference.vocoders import HIFIGAN\n",
    "    >>> hifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-ljspeech\", savedir=tmpdir_vocoder) # doctest: +SKIP\n",
    "    >>> # Running the TTS\n",
    "    >>> mel_outputs = transformerTTS.encode_text([\"Mary had a little lamb.\"]) # doctest: +SKIP\n",
    "    >>> # Running Vocoder (spectrogram-to-waveform)\n",
    "    >>> waveforms = hifi_gan.decode_batch(mel_outputs) # doctest: +SKIP\n",
    "    \"\"\"\n",
    "\n",
    "    HPARAMS_NEEDED = [\"model\", \"input_encoder\"]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        lexicon = self.hparams.lexicon\n",
    "        lexicon = [\"@@\"] + lexicon\n",
    "        self.input_encoder = self.hparams.input_encoder\n",
    "        self.input_encoder.update_from_iterable(lexicon, sequence_input=False)\n",
    "        self.input_encoder.add_unk()\n",
    "\n",
    "        self.g2p = GraphemeToPhoneme.from_hparams(\"speechbrain/soundchoice-g2p\")\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        \"\"\"Computes mel-spectrogram for a list of texts\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        texts: List[str]\n",
    "            texts to be converted to spectrogram\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensors of output spectrograms and output lengths\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocessing required at the inference time for the input text\n",
    "        # \"label\" below contains input text\n",
    "        # \"phoneme_labels\" contain the phoneme sequences corresponding to input text labels\n",
    "\n",
    "        phoneme_labels = list()\n",
    "        max_seq_len = -1\n",
    "\n",
    "        for label in texts:\n",
    "            phonemes_with_punc = self._g2p_keep_punctuations(self.g2p, label)\n",
    "            if max_seq_len < len(phonemes_with_punc):\n",
    "                max_seq_len = len(phonemes_with_punc)\n",
    "            token_seq = (\n",
    "                self.input_encoder.encode_sequence_torch(phonemes_with_punc)\n",
    "                .int()\n",
    "                .to(self.device)\n",
    "            )\n",
    "            phoneme_labels.append(token_seq)\n",
    "\n",
    "        input_lens = len(texts)\n",
    "\n",
    "        tokens_padded = torch.LongTensor(len(texts), max_seq_len).to(\n",
    "            self.device\n",
    "        )\n",
    "        tokens_padded.zero_()\n",
    "\n",
    "        for seq_idx, seq in enumerate(phoneme_labels):\n",
    "            tokens_padded[seq_idx, : len(seq)] = seq\n",
    "\n",
    "        return self.encode_batch(\n",
    "            tokens_padded, input_lens\n",
    "        )\n",
    "\n",
    "    def _g2p_keep_punctuations(self, g2p_model, text):\n",
    "        \"\"\"do grapheme to phoneme and keep the punctuations between the words\"\"\"\n",
    "        # find the words where a \"-\" or \"'\" or \".\" or \":\" appears in the middle\n",
    "        special_words = re.findall(r\"\\w+[-':\\.][-':\\.\\w]*\\w+\", text)\n",
    "\n",
    "        # remove intra-word punctuations (\"-':.\"), this does not change the output of speechbrain g2p\n",
    "        for special_word in special_words:\n",
    "            rmp = special_word.replace(\"-\", \"\")\n",
    "            rmp = rmp.replace(\"'\", \"\")\n",
    "            rmp = rmp.replace(\":\", \"\")\n",
    "            rmp = rmp.replace(\".\", \"\")\n",
    "            text = text.replace(special_word, rmp)\n",
    "\n",
    "        # keep inter-word punctuations\n",
    "        all_ = re.findall(r\"[\\w]+|[-!'(),.:;? ]\", text)\n",
    "        try:\n",
    "            phonemes = g2p_model(text)\n",
    "        except RuntimeError:\n",
    "            logger.info(f\"error with text: {text}\")\n",
    "            quit()\n",
    "        word_phonemes = \"-\".join(phonemes).split(\" \")\n",
    "\n",
    "        phonemes_with_punc = []\n",
    "        count = 0\n",
    "        try:\n",
    "            # if the g2p model splits the words correctly\n",
    "            for i in all_:\n",
    "                if i not in \"-!'(),.:;? \":\n",
    "                    phonemes_with_punc.extend(word_phonemes[count].split(\"-\"))\n",
    "                    count += 1\n",
    "                else:\n",
    "                    phonemes_with_punc.append(i)\n",
    "        except IndexError:\n",
    "            # sometimes the g2p model cannot split the words correctly\n",
    "            logger.warning(\n",
    "                f\"Do g2p word by word because of unexpected outputs from g2p for text: {text}\"\n",
    "            )\n",
    "\n",
    "            for i in all_:\n",
    "                if i not in \"-!'(),.:;? \":\n",
    "                    p = g2p_model.g2p(i)\n",
    "                    p_without_space = [i for i in p if i != \" \"]\n",
    "                    phonemes_with_punc.extend(p_without_space)\n",
    "                else:\n",
    "                    phonemes_with_punc.append(i)\n",
    "\n",
    "        while \"\" in phonemes_with_punc:\n",
    "            phonemes_with_punc.remove(\"\")\n",
    "        return phonemes_with_punc\n",
    "\n",
    "    def encode_phoneme(self, phonemes):\n",
    "        \"\"\"Computes mel-spectrogram for a list of phoneme sequences\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        phonemes: List[List[str]]\n",
    "            phonemes to be converted to spectrogram\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensors of output spectrograms and output lengths\n",
    "        \"\"\"\n",
    "\n",
    "        all_tokens = []\n",
    "        max_seq_len = -1\n",
    "        input_lens = []\n",
    "        for phoneme in phonemes:\n",
    "            token_seq = (\n",
    "                self.input_encoder.encode_sequence_torch(phoneme)\n",
    "                .int()\n",
    "                .to(self.device)\n",
    "            )\n",
    "            if max_seq_len < token_seq.shape[-1]:\n",
    "                max_seq_len = token_seq.shape[-1]\n",
    "            all_tokens.append(token_seq)\n",
    "            input_lens.append(len(phoneme))\n",
    "\n",
    "        tokens_padded = torch.LongTensor(len(phonemes), max_seq_len).to(\n",
    "            self.device\n",
    "        )\n",
    "        tokens_padded.zero_()\n",
    "\n",
    "        for seq_idx, seq in enumerate(all_tokens):\n",
    "            tokens_padded[seq_idx, : len(seq)] = seq\n",
    "\n",
    "        return self.encode_batch(tokens_padded, input_lens)\n",
    "\n",
    "    def encode_batch(self, tokens_padded, input_lens):\n",
    "        \"\"\"Batch inference for a tensor of phoneme sequences\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        tokens_padded : torch.Tensor\n",
    "            A sequence of encoded phonemes to be converted to spectrogram\n",
    "        input_lens:\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        post_mel_outputs : torch.Tensor\n",
    "        durations : torch.Tensor\n",
    "        pitch : torch.Tensor\n",
    "        energy : torch.Tensor\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                post_mel_outputs,\n",
    "                durations,\n",
    "            ) = self.hparams.model(tokens_padded, input_lens)\n",
    "\n",
    "            # Transposes to make in compliant with HiFI GAN expected format\n",
    "            post_mel_outputs = post_mel_outputs.transpose(-1, 1)\n",
    "\n",
    "        return post_mel_outputs, durations\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"Batch inference for a tensor of phoneme sequences\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        text : str\n",
    "            A text to be converted to spectrogram\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Encoded text\n",
    "        \"\"\"\n",
    "        return self.encode_text(\n",
    "            [text]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges Faced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While building the model, I faced several challenges. The main challenge that inhibited the success of my project was the selection of learning rate annealing scheduling method. Initially, I chose to use the Interval Scheduler, the same as that used in the Tacotron2 recipe, and this caused my model to not converge whatsoever. Upon doing some research online, I discovered that I should have used a scheduling algorithm that includes warmup for my transformer model. So, I decided to try that and it solved my issues with training the model.<br>\n",
    "Also, I had some trouble figuring out how to perform inference on the model, and unfortunately could not figure it out in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I have provided a link to the google drive containing the logs and checkpoints. I managed to train for around 65 epochs using one A100 and achieved the following:\n",
    "|            |Train(T): Loss    |(T) mel_loss |(T) gate_loss |Valid(V): Loss    |(V) mel_loss |(V) gate_loss |\n",
    "|------------|---------|----------|-----------|---------|----------|-----------|\n",
    "| TransformerTTS      | 0.284    | 0.273 | 0.0111 |  0.371 | 0.316 | 0.0548 |\n",
    "| Tacotron2      | 0.293    | 0.292 | 6.54e-04 | 0.292 | 0.291 | 1.34e-03 |\n",
    "\n",
    "I took Tacotron2's stats from the speechbrain repo's dropbox link\n",
    "<br>\n",
    "<br>Drive: https://drive.google.com/drive/folders/1FIqVPfJl4n4jIt0DOPV89_hg8wYLUNLn?usp=sharing\n",
    "I am certain that the loss would get even lower as the model trained for longer, but I trained for as long as I could.\n",
    "\n",
    "<br> Average epoch took ~ 3 minutes and 33 seconds, and had ~1.76 it/sec on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Li, N. et al. (2019) Neural speech synthesis with Transformer network, arXiv.org. Available at: https://arxiv.org/abs/1809.08895 (Accessed: 10 April 2024). \n",
    "2. Vaswani, A. et al. (2017) Attention is all you need, arXiv.org. Available at: https://arxiv.org/abs/1706.03762 (Accessed: 16 April 2024). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
