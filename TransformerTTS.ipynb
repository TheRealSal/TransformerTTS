{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff431a75-1f60-405d-b0ae-23fc822ca931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (24.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7acd1-5726-42bf-9f13-70dfa51c5d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install speechbrain\n",
    "!pip install transformers\n",
    "!pip install tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae000d3-b107-4f76-8ae1-6edd1505c957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645684f1-364c-4b42-86ef-5e8597f271f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PyTorch Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ead865f2-e75f-45ab-ad70-bbe5e5d19216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /notebooks/models/TransformerTTS.py\n"
     ]
    }
   ],
   "source": [
    "%%file /notebooks/models/TransformerTTS.py\n",
    "# @title Bringing it all in one file in speechbrain\n",
    "\n",
    "\"\"\"\n",
    "Neural network modules for the Tacotron2 end-to-end neural\n",
    "Text-to-Speech (TTS) model\n",
    "\n",
    "Authors\n",
    "* Salman Hussain Ali 2024\n",
    "\"\"\"\n",
    "import math\n",
    "# This code uses a significant portion of the NVidia implementation, even though it\n",
    "# has been modified and enhanced\n",
    "\n",
    "# https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/tacotron2/model.py\n",
    "# *****************************************************************************\n",
    "#  Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "#  Redistribution and use in source and binary forms, with or without\n",
    "#  modification, are permitted provided that the following conditions are met:\n",
    "#      * Redistributions of source code must retain the above copyright\n",
    "#        notice, this list of conditions and the following disclaimer.\n",
    "#      * Redistributions in binary form must reproduce the above copyright\n",
    "#        notice, this list of conditions and the following disclaimer in the\n",
    "#        documentation and/or other materials provided with the distribution.\n",
    "#      * Neither the name of the NVIDIA CORPORATION nor the\n",
    "#        names of its contributors may be used to endorse or promote products\n",
    "#        derived from this software without specific prior written permission.\n",
    "#\n",
    "#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
    "#  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
    "#  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "#  DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n",
    "#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
    "#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
    "#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
    "#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
    "#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "# *****************************************************************************\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from speechbrain.lobes.models.transformer.Transformer import get_mask_from_lengths\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class LinearNorm(torch.nn.Module):\n",
    "    \"\"\"A linear layer with Xavier initialization\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_dim: int\n",
    "        the input dimension\n",
    "    out_dim: int\n",
    "        the output dimension\n",
    "    bias: bool\n",
    "        whether or not to use a bias\n",
    "    w_init_gain: linear\n",
    "        the weight initialization gain type (see torch.nn.init.calculate_gain)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.Tacotron2 import LinearNorm\n",
    "    >>> layer = LinearNorm(in_dim=5, out_dim=3)\n",
    "    >>> x = torch.randn(3, 5)\n",
    "    >>> y = layer(x)\n",
    "    >>> y.shape\n",
    "    torch.Size([3, 3])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain=\"linear\"):\n",
    "        super().__init__()\n",
    "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.linear_layer.weight,\n",
    "            gain=torch.nn.init.calculate_gain(w_init_gain),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            a (batch, features) input tensor\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the linear layer output\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "\n",
    "class ConvNorm(torch.nn.Module):\n",
    "    \"\"\"A 1D convolution layer with Xavier initialization\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_channels: int\n",
    "        the number of input channels\n",
    "    out_channels: int\n",
    "        the number of output channels\n",
    "    kernel_size: int\n",
    "        the kernel size\n",
    "    stride: int\n",
    "        the convolutional stride\n",
    "    padding: int\n",
    "        the amount of padding to include. If not provided, it will be calculated\n",
    "        as dilation * (kernel_size - 1) / 2\n",
    "    dilation: int\n",
    "        the dilation of the convolution\n",
    "    bias: bool\n",
    "        whether or not to use a bias\n",
    "    w_init_gain: linear\n",
    "        the weight initialization gain type (see torch.nn.init.calculate_gain)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.Tacotron2 import ConvNorm\n",
    "    >>> layer = ConvNorm(in_channels=10, out_channels=5, kernel_size=3)\n",
    "    >>> x = torch.randn(3, 10, 5)\n",
    "    >>> y = layer(x)\n",
    "    >>> y.shape\n",
    "    torch.Size([3, 5, 5])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=None,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "            w_init_gain=\"linear\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            assert kernel_size % 2 == 1\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain)\n",
    "        )\n",
    "\n",
    "    def forward(self, signal):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        signal: torch.Tensor\n",
    "            the input to the convolutional layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        return self.conv(signal)\n",
    "\n",
    "\n",
    "class EncoderPreNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        A block for preprocessing text inputs in an encoder network, consisting of a 1D convolutional layer followed by batch normalization, ReLU activation, dropout, and linear transformation.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        in_dim: int\n",
    "            Number of input channels.\n",
    "        kernel_size: int\n",
    "            Size of the convolutional kernel.\n",
    "        stride: int\n",
    "            Stride of the convolution.\n",
    "        padding: int, optional\n",
    "            Amount of padding to include. If not provided, it will be calculated as dilation * (kernel_size - 1) / 2.\n",
    "        dilation: int, optional\n",
    "            Dilation factor of the convolution.\n",
    "        bias: bool, optional\n",
    "            Whether to include bias in the convolutional layer.\n",
    "        dropout: float, optional\n",
    "            Dropout probability.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> import torch\n",
    "        >>> from speechbrain.lobes.models.TransformerTTS import EncoderPreNetBlock\n",
    "        >>> block = EncoderPreNetBlock(in_dim=10, kernel_size=3)\n",
    "        >>> x = torch.randn(3, 10, 5)  # Input tensor shape: (batch_size, in_dim, sequence_length)\n",
    "        >>> y = block(x)\n",
    "        >>> y.shape  # Output shape: (batch_size, in_dim, sequence_length)\n",
    "        torch.Size([3, 10, 5])\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim=512,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=None,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "            dropout=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            assert kernel_size % 2 == 1\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = ConvNorm(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "\n",
    "        self.batch = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        text: torch.Tensor\n",
    "            the input to the convolutional layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        # text = text.transpose(1, 2)\n",
    "        out = self.conv(text)\n",
    "        out = self.batch(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderPrenet(nn.Module):\n",
    "    \"\"\"The Tacotron pre-net module consisting of a specified number of\n",
    "    normalized (Xavier-initialized) linear layers\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_dim: int\n",
    "        the input dimensions\n",
    "    sizes: int\n",
    "        the dimension of the hidden layers/output\n",
    "    dropout: float\n",
    "        the dropout probability\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.Tacotron2 import Prenet\n",
    "    >>> layer = Prenet()\n",
    "    >>> x = torch.randn(862, 2, 80)\n",
    "    >>> output = layer(x)\n",
    "    >>> output.shape\n",
    "    torch.Size([862, 2, 256])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_dim=512,\n",
    "            hidden_dim=256,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=None,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "            num_layers=3,\n",
    "            dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderPreNetBlock(emb_dim, kernel_size, stride, padding, dilation, bias) for i in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(emb_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass for the prenet\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            the prenet inputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        # x = x.transpose(1,2) TODO review\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderPrenet(nn.Module):\n",
    "    \"\"\"The Tacotron pre-net module consisting of a specified number of\n",
    "        normalized (Xavier-initialized) linear layers\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        in_dim: int\n",
    "            the input dimensions\n",
    "        sizes: int\n",
    "            the dimension of the hidden layers/output\n",
    "        dropout: float\n",
    "            the dropout probability\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> import torch\n",
    "        >>> from speechbrain.lobes.models.TransformerTTS import DecoderPrenet\n",
    "        >>> layer = DecoderPrenet()\n",
    "        >>> x = torch.randn(862, 2, 80)\n",
    "        >>> output = layer(x)\n",
    "        >>> output.shape\n",
    "        torch.Size([862, 2, 256])\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mel_dims=80,\n",
    "            hidden_dims=256,\n",
    "            d_model=512,\n",
    "            dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(mel_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dims, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dims, d_model)\n",
    "\n",
    "    def forward(self, mel):\n",
    "        \"\"\"Computes the forward pass for the prenet\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            the prenet inputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the output\n",
    "        \"\"\"\n",
    "        out = self.layers(mel)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Postnet(nn.Module):\n",
    "    \"\"\"The TransformerTTS postnet consists of a number of 1-d convolutional layers\n",
    "    with Xavier initialization and a tanh activation, with batch normalization.\n",
    "    Depending on configuration, the postnet may either refine the MEL spectrogram\n",
    "    or upsample it to a linear spectrogram. It has the same architecture as Tacotron2's Post-Net\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    n_mel_channels: int\n",
    "        the number of MEL spectrogram channels\n",
    "    postnet_embedding_dim: int\n",
    "        the postnet embedding dimension\n",
    "    postnet_kernel_size: int\n",
    "        the kernel size of the convolutions within the decoders\n",
    "    postnet_n_convolutions: int\n",
    "        the number of convolutions in the postnet\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from speechbrain.lobes.models.Tacotron2 import Postnet\n",
    "    >>> layer = Postnet()\n",
    "    >>> x = torch.randn(2, 80, 861)\n",
    "    >>> output = layer(x)\n",
    "    >>> output.shape\n",
    "    torch.Size([2, 80, 861])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_mel_channels=80,\n",
    "            postnet_embedding_dim=512,\n",
    "            postnet_kernel_size=5,\n",
    "            postnet_n_convolutions=5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(\n",
    "                    n_mel_channels,\n",
    "                    postnet_embedding_dim,\n",
    "                    kernel_size=postnet_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=int((postnet_kernel_size - 1) / 2),\n",
    "                    dilation=1,\n",
    "                    w_init_gain=\"tanh\",\n",
    "                ),\n",
    "                nn.BatchNorm1d(postnet_embedding_dim),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for i in range(1, postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    ConvNorm(\n",
    "                        postnet_embedding_dim,\n",
    "                        postnet_embedding_dim,\n",
    "                        kernel_size=postnet_kernel_size,\n",
    "                        stride=1,\n",
    "                        padding=int((postnet_kernel_size - 1) / 2),\n",
    "                        dilation=1,\n",
    "                        w_init_gain=\"tanh\",\n",
    "                    ),\n",
    "                    nn.BatchNorm1d(postnet_embedding_dim),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(\n",
    "                    postnet_embedding_dim,\n",
    "                    n_mel_channels,\n",
    "                    kernel_size=postnet_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=int((postnet_kernel_size - 1) / 2),\n",
    "                    dilation=1,\n",
    "                    w_init_gain=\"linear\",\n",
    "                ),\n",
    "                nn.BatchNorm1d(n_mel_channels),\n",
    "            )\n",
    "        )\n",
    "        self.n_convs = len(self.convolutions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass of the postnet\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            the postnet input (usually a MEL spectrogram)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.Tensor\n",
    "            the postnet output (a refined MEL spectrogram or a\n",
    "            linear spectrogram depending on how the model is\n",
    "            configured)\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        for conv in self.convolutions:\n",
    "            if i < self.n_convs - 1:\n",
    "                x = F.dropout(torch.tanh(conv(x)), 0.5, training=self.training)\n",
    "            else:\n",
    "                x = F.dropout(conv(x), 0.5, training=self.training)\n",
    "            i += 1\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerTTS(nn.Module):\n",
    "    \"\"\"The Transformer text-to-speech model, based on the NVIDIA implementation.\n",
    "\n",
    "    This class is the main entry point for the model, which is responsible\n",
    "    for instantiating all submodules, which, in turn, manage the individual\n",
    "    neural network layers\n",
    "\n",
    "    Simplified STRUCTURE: input->word embedding ->encoder ->attention \\\n",
    "    ->decoder(+prenet) -> postnet ->output\n",
    "\n",
    "    prenet(input is decoder previous time step) output is input to decoder\n",
    "    concatenated with the attention output\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    mask_padding: bool\n",
    "        whether or not to mask pad-outputs of tacotron\n",
    "    n_mel_channels: int\n",
    "        number of mel channels for constructing spectrogram\n",
    "    n_symbols:  int=128\n",
    "        number of accepted char symbols defined in textToSequence\n",
    "    symbols_embedding_dim: int\n",
    "        number of embedding dimension for symbols fed to nn.Embedding\n",
    "    encoder_prenet_kernel_size: int\n",
    "        size of kernel processing the embeddings\n",
    "    encoder_prenet_n_convolutions: int\n",
    "        number of convolution layers in encoder\n",
    "    encoder_prenet_embedding_dim: int\n",
    "        number of kernels in encoder, this is also the dimension\n",
    "        of the bidirectional LSTM in the encoder\n",
    "    n_frames_per_step: int=1\n",
    "        only 1 generated mel-frame per step is supported for the decoder as of now.\n",
    "    decoder_rnn_dim: int\n",
    "        number of 2 unidirectional stacked LSTM units\n",
    "    prenet_dim: int\n",
    "        dimension of linear prenet layers\n",
    "    max_decoder_steps: int\n",
    "        maximum number of steps/frames the decoder generates before stopping\n",
    "    gate_threshold: int\n",
    "        cut off level any output probability above that is considered\n",
    "        complete and stops generation so we have variable length outputs\n",
    "    p_attention_dropout: float\n",
    "        attention drop out probability\n",
    "    p_decoder_dropout: float\n",
    "        decoder drop  out probability\n",
    "    postnet_embedding_dim: int\n",
    "        number os postnet dfilters\n",
    "    postnet_kernel_size: int\n",
    "        1d size of posnet kernel\n",
    "    postnet_n_convolutions: int\n",
    "        number of convolution layers in postnet\n",
    "    decoder_no_early_stopping: bool\n",
    "        determines early stopping of decoder\n",
    "        along with gate_threshold . The logical inverse of this is fed to the decoder\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> _ = torch.manual_seed(213312)\n",
    "    >>> from speechbrain.lobes.models.Tacotron2 import Tacotron2\n",
    "    >>> model = Tacotron2(\n",
    "    ...    mask_padding=True,\n",
    "    ...    n_mel_channels=80,\n",
    "    ...    n_symbols=148,\n",
    "    ...    symbols_embedding_dim=512,\n",
    "    ...    encoder_prenet_kernel_size=5,\n",
    "    ...    encoder_prenet_n_convolutions=3,\n",
    "    ...    encoder_prenet_embedding_dim=512,\n",
    "    ...    attention_rnn_dim=1024,\n",
    "    ...    attention_dim=128,\n",
    "    ...    attention_location_n_filters=32,\n",
    "    ...    attention_location_kernel_size=31,\n",
    "    ...    n_frames_per_step=1,\n",
    "    ...    decoder_rnn_dim=1024,\n",
    "    ...    prenet_dim=256,\n",
    "    ...    max_decoder_steps=32,\n",
    "    ...    gate_threshold=0.5,\n",
    "    ...    p_attention_dropout=0.1,\n",
    "    ...    p_decoder_dropout=0.1,\n",
    "    ...    postnet_embedding_dim=512,\n",
    "    ...    postnet_kernel_size=5,\n",
    "    ...    postnet_n_convolutions=5,\n",
    "    ...    decoder_no_early_stopping=False\n",
    "    ... )\n",
    "    >>> _ = model.eval()\n",
    "    >>> inputs = torch.tensor([\n",
    "    ...     [13, 12, 31, 14, 19],\n",
    "    ...     [31, 16, 30, 31, 0],\n",
    "    ... ])\n",
    "    >>> input_lengths = torch.tensor([5, 4])\n",
    "    >>> outputs, output_lengths, alignments = model.infer(inputs, input_lengths)\n",
    "    >>> outputs.shape, output_lengths.shape, alignments.shape\n",
    "    (torch.Size([2, 80, 1]), torch.Size([2]), torch.Size([2, 1, 5]))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mask_padding=True,\n",
    "            # mel generation parameter in data io\n",
    "            n_mel_channels=80,\n",
    "            # symbols\n",
    "            n_symbols=52,\n",
    "            symbols_embedding_dim=512,\n",
    "            # Encoder Pre-Net parameters\n",
    "            encoder_prenet_kernel_size=5,\n",
    "            encoder_prenet_n_convolutions=3,\n",
    "            encoder_prenet_padding=None,\n",
    "            encoder_prenet_dilation=1,\n",
    "            encoder_prenet_bias=True,\n",
    "            encoder_prenet_dropout=0.5,\n",
    "            encoder_prenet_stride=1,\n",
    "            # Decoder Pre-Net parameters\n",
    "            n_frames_per_step=1,\n",
    "            decoder_prenet_hidden_dims=256,\n",
    "            decoder_prenet_dropout=0.15,\n",
    "            # Transformer Parameters\n",
    "            d_model=256,\n",
    "            transformer_nhead=8,\n",
    "            transformer_num_encoder_layers=6,\n",
    "            transformer_num_decoder_layers=6,\n",
    "            transformer_d_ffn=2048,\n",
    "            transformer_dropout=0.1,\n",
    "            transformer_activation=\"relu\",\n",
    "            custom_encoder_module=None,\n",
    "            custom_decoder_module=None,\n",
    "            batch_first=False,\n",
    "            norm_first=False,\n",
    "            layer_norm_eps=1e-5,\n",
    "            # Mel-post processing network parameters\n",
    "            postnet_embedding_dim=512,\n",
    "            postnet_kernel_size=5,\n",
    "            postnet_n_convolutions=5,\n",
    "            gate_threshold=0.5,\n",
    "            max_decoder_steps=32,\n",
    "            early_stopping=False,\n",
    "            padding_idx=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask_padding = mask_padding\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "        self.max_decoder_steps = max_decoder_steps\n",
    "        self.early_stopping = early_stopping\n",
    "\n",
    "        self.gate_threshold = gate_threshold\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(n_symbols, symbols_embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        std = sqrt(2.0 / (n_symbols + d_model))\n",
    "        val = sqrt(3.0) * std  # uniform bounds for std\n",
    "        self.encoder_embedding.weight.data.uniform_(-val, val)\n",
    "\n",
    "        if custom_encoder_module is None:\n",
    "            encoder_block = torch.nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                         nhead=transformer_nhead,\n",
    "                                                         dim_feedforward=transformer_d_ffn,\n",
    "                                                         dropout=transformer_dropout,\n",
    "                                                         activation=transformer_activation,\n",
    "                                                         batch_first=batch_first,\n",
    "                                                         norm_first=norm_first,\n",
    "                                                         layer_norm_eps=layer_norm_eps)\n",
    "        else:\n",
    "            encoder_block = custom_encoder_module\n",
    "\n",
    "        if custom_decoder_module is None:\n",
    "            decoder_block = torch.nn.TransformerDecoderLayer(d_model=d_model,\n",
    "                                                         nhead=transformer_nhead,\n",
    "                                                         dim_feedforward=transformer_d_ffn,\n",
    "                                                         dropout=transformer_dropout,\n",
    "                                                         activation=transformer_activation,\n",
    "                                                         batch_first=batch_first,\n",
    "                                                         norm_first=norm_first,\n",
    "                                                         layer_norm_eps=layer_norm_eps)\n",
    "        else:\n",
    "            decoder_block = custom_decoder_module\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_block, num_layers=transformer_num_encoder_layers)\n",
    "\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_block, num_layers=transformer_num_decoder_layers)\n",
    "\n",
    "        self.encoder_prenet = EncoderPrenet(\n",
    "            emb_dim=symbols_embedding_dim,\n",
    "            hidden_dim=d_model,\n",
    "            kernel_size=encoder_prenet_kernel_size,\n",
    "            num_layers=encoder_prenet_n_convolutions,\n",
    "            stride=encoder_prenet_stride,\n",
    "            padding=encoder_prenet_padding,\n",
    "            dilation=encoder_prenet_dilation,\n",
    "            bias=encoder_prenet_bias,\n",
    "            dropout=encoder_prenet_dropout)\n",
    "\n",
    "        self.decoder_prenet = DecoderPrenet(\n",
    "            mel_dims=n_mel_channels,\n",
    "            hidden_dims=decoder_prenet_hidden_dims,\n",
    "            dropout=decoder_prenet_dropout,\n",
    "            d_model=d_model\n",
    "        )\n",
    "\n",
    "        self.postnet = Postnet(\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            postnet_embedding_dim=postnet_embedding_dim,\n",
    "            postnet_kernel_size=postnet_kernel_size,\n",
    "            postnet_n_convolutions=postnet_n_convolutions,\n",
    "        )\n",
    "\n",
    "        self.encoder_positional_encoding = PositionalEncoding(input_size=d_model)\n",
    "        self.decoder_positional_encoding = PositionalEncoding(input_size=d_model)\n",
    "\n",
    "        self.mel_linear = LinearNorm(d_model, n_mel_channels)\n",
    "        self.stop_linear = LinearNorm(d_model, 1, w_init_gain='sigmoid')\n",
    "\n",
    "    def parse_output(self, outputs, output_lengths):\n",
    "        \"\"\"\n",
    "        Masks the padded part of output\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        outputs: list\n",
    "            a list of tensors - raw outputs\n",
    "        output_lengths: torch.Tensor\n",
    "            a tensor representing the lengths of all outputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs: torch.Tensor\n",
    "        mel_outputs_postnet: torch.Tensor\n",
    "        gate_outputs: torch.Tensor\n",
    "        \"\"\"\n",
    "        mel_outputs, mel_outputs_postnet, gate_outputs = outputs\n",
    "\n",
    "        if self.mask_padding and output_lengths is not None:\n",
    "            mask = get_mask_from_lengths(\n",
    "                output_lengths, max_len=mel_outputs.size(-1)\n",
    "            )\n",
    "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "\n",
    "            mel_outputs.clone().masked_fill_(mask, 0.0)\n",
    "            mel_outputs_postnet.masked_fill_(mask, 0.0)\n",
    "            gate_outputs.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
    "\n",
    "        return mel_outputs, mel_outputs_postnet, gate_outputs\n",
    "\n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs):\n",
    "        \"\"\"Prepares decoder outputs for output\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        mel_outputs: torch.Tensor\n",
    "            MEL-scale spectrogram outputs\n",
    "        gate_outputs: torch.Tensor\n",
    "            gate output energies\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs: torch.Tensor\n",
    "            MEL-scale spectrogram outputs\n",
    "        gate_outputs: torch.Tensor\n",
    "            gate output energies\n",
    "        \"\"\"\n",
    "        # (T_out, B) -> (B, T_out)\n",
    "        if gate_outputs.dim() == 1:\n",
    "            gate_outputs = gate_outputs.unsqueeze(0)\n",
    "        else:\n",
    "            gate_outputs = gate_outputs.transpose(0, 1).contiguous()\n",
    "\n",
    "        # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
    "        mel_outputs = mel_outputs.transpose(0, 1).contiguous()\n",
    "        # decouple frames per step\n",
    "        shape = (mel_outputs.shape[0], -1, self.n_mel_channels)\n",
    "        mel_outputs = mel_outputs.view(*shape)\n",
    "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "        return mel_outputs, gate_outputs\n",
    "\n",
    "    def get_go_frame(self, memory):\n",
    "        \"\"\"Gets all zeros frames to use as first decoder input\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        memory: torch.Tensor\n",
    "            decoder outputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        decoder_input: torch.Tensor\n",
    "            all zeros frames\n",
    "        \"\"\"\n",
    "        B = memory.size(0)\n",
    "        dtype = memory.dtype\n",
    "        device = memory.device\n",
    "        decoder_input = torch.zeros(\n",
    "            B,\n",
    "            self.n_mel_channels * self.n_frames_per_step,\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "        )\n",
    "        return decoder_input\n",
    "\n",
    "    def forward(self, inputs, masks):\n",
    "        \"\"\"Decoder forward pass for training\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        inputs: tuple\n",
    "            batch object\n",
    "        alignments_dim: int\n",
    "            the desired dimension of the alignments along the last axis\n",
    "            Optional but needed for data-parallel training\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs: torch.Tensor\n",
    "            mel outputs from the decoder\n",
    "        mel_outputs_postnet: torch.Tensor\n",
    "            mel outputs from postnet\n",
    "        gate_outputs: torch.Tensor\n",
    "            gate outputs from the decoder\n",
    "        output_lengths: torch.Tensor\n",
    "            length of the output without padding\n",
    "        \"\"\"\n",
    "        inputs, input_lengths, mel_padded, max_len, mel_len = inputs  # (text_padded, input_lengths, mel_padded, max_len, output_lengths)\n",
    "        tgt_mask, src_mask, src_key_padding_mask, tgt_key_padding_mask = masks\n",
    "\n",
    "        # Generate Embeddings\n",
    "        embedded_inputs = self.encoder_embedding(inputs).transpose(1, 2)\n",
    "\n",
    "        # Pass through encoder pre-net\n",
    "        encoder_prenet_outputs = self.encoder_prenet(embedded_inputs)\n",
    "\n",
    "        # Pass through decoder pre-net\n",
    "        decoder_prenet_outputs = self.decoder_prenet(mel_padded.transpose(1, 2))\n",
    "        \n",
    "        encoder_prenet_outputs = encoder_prenet_outputs.transpose(0, 1)\n",
    "        decoder_prenet_outputs = decoder_prenet_outputs.transpose(0, 1)\n",
    "\n",
    "        # Add Scaled Positional Embeddings to encoder and decoder pre-nets\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs + self.encoder_positional_encoding(encoder_prenet_outputs)\n",
    "        decoder_prenet_outputs = decoder_prenet_outputs + self.decoder_positional_encoding(decoder_prenet_outputs)\n",
    "\n",
    "        # Input embedded phonemes into transformer's encoder\n",
    "        memory = self.transformer_encoder(\n",
    "            src=encoder_prenet_outputs,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        # Input memory and mel spectograms into transformer's decoder\n",
    "        mel_outputs = self.transformer_decoder(tgt=decoder_prenet_outputs,\n",
    "                                               memory=memory,\n",
    "                                               tgt_mask=tgt_mask,\n",
    "                                               tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                                memory_key_padding_mask = src_key_padding_mask)\n",
    "\n",
    "        # Calculate mel linear, mel stop, and post-net\n",
    "        # Stop Linear\n",
    "        stop_token = self.stop_linear(mel_outputs).squeeze()\n",
    "\n",
    "        # Mel Linear and Post-Net\n",
    "        mel_outputs = self.mel_linear(mel_outputs)\n",
    "\n",
    "        mel_outputs, stop_token = self.parse_decoder_outputs(mel_outputs, stop_token)\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, stop_token], mel_len)\n",
    "\n",
    "    def infer(self, inputs, input_lengths):\n",
    "        \"\"\"Produces outputs\n",
    "\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        inputs: torch.tensor\n",
    "            text or phonemes converted\n",
    "\n",
    "        input_lengths: torch.tensor\n",
    "            the lengths of input parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mel_outputs_postnet: torch.Tensor\n",
    "            final mel output of tacotron 2\n",
    "        mel_lengths: torch.Tensor\n",
    "            length of mels\n",
    "        alignments: torch.Tensor\n",
    "            sequence of attention weights\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate Embeddings\n",
    "        embedded_inputs = self.encoder_embedding(inputs).transpose(1, 2)\n",
    "\n",
    "        # Pass through encoder pre-net\n",
    "        encoder_prenet_outputs = self.encoder_prenet(embedded_inputs)\n",
    "\n",
    "        # Add Scaled Positional Embeddings to encoder and decoder pre-nets\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs + self.encoder_positional_encoding(encoder_prenet_outputs)\n",
    "\n",
    "        encoder_prenet_outputs = encoder_prenet_outputs.transpose(0, 1)\n",
    "\n",
    "        # Input embedded phonemes into transformer's encoder\n",
    "        src_key_padding_mask = get_mask_from_lengths(input_lengths).to(inputs.device, non_blocking=True)\n",
    "        memory = self.transformer_encoder(\n",
    "            src=encoder_prenet_outputs,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        decoder_input = self.get_go_frame(memory)\n",
    "        print(f\"Decoder Input Shape:{decoder_input.shape}\")\n",
    "\n",
    "        mask = get_mask_from_lengths(input_lengths)\n",
    "\n",
    "        mel_lengths = torch.zeros(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "        not_finished = torch.ones(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "\n",
    "        mel_outputs, gate_outputs = (\n",
    "            torch.zeros(1),\n",
    "            torch.zeros(1),\n",
    "        )\n",
    "        #print(f\"Memory shape: {memory.shape}\")\n",
    "\n",
    "        first_iter = True\n",
    "        while True:\n",
    "            decoder_input = self.decoder_prenet(decoder_input)\n",
    "            if len(decoder_input.shape) != 3:\n",
    "                decoder_input = decoder_input.unsqueeze(1)\n",
    "            \n",
    "            #print(f\"decoder_input shape: {decoder_input.shape}\")\n",
    "            decoder_output = self.transformer_decoder(memory=memory, tgt=decoder_input)\n",
    "            #print(f\"Decoder Output Shape: {decoder_output.shape}\")\n",
    "\n",
    "            # Calculate mel linear and stop token\n",
    "            # Stop Linear\n",
    "            gate_output = self.stop_linear(decoder_output).squeeze()\n",
    "            #print(f\"Stop Linear Output Shape: {gate_output.shape}\")\n",
    "\n",
    "            # Mel Linear and Post-Net\n",
    "            mel_output = self.mel_linear(decoder_output)\n",
    "            #print(f\"Mel Linear Output Shape: {mel_output.shape}\")\n",
    "\n",
    "            if first_iter:\n",
    "                mel_outputs = mel_output\n",
    "                gate_outputs = gate_output\n",
    "                first_iter = False\n",
    "            else:\n",
    "                mel_outputs = torch.cat(\n",
    "                    (mel_outputs, mel_output), dim=0\n",
    "                )\n",
    "                gate_outputs = torch.cat((gate_outputs, gate_output), dim=0)\n",
    "            #print(f\"Mel Linear Output Shape: {mel_output.shape}\")\n",
    "            #print(f\"Mel_Outputs Shape: {mel_outputs.shape}\")\n",
    "            \n",
    "            sigmoid = torch.sigmoid(gate_output).unsqueeze(1)\n",
    "\n",
    "            dec = (\n",
    "                torch.le(sigmoid, torch.tensor(self.gate_threshold))\n",
    "                .to(torch.int32)\n",
    "                .squeeze(1)\n",
    "            )\n",
    "\n",
    "            not_finished = not_finished * dec\n",
    "            mel_lengths += not_finished\n",
    "            if self.early_stopping and torch.sum(not_finished) == 0:\n",
    "                break\n",
    "            if len(mel_outputs) == self.max_decoder_steps:\n",
    "                break\n",
    "            \n",
    "            #print(f\"Pre_Assignment Output Shape: {mel_output.shape}\")\n",
    "            decoder_input = mel_output\n",
    "\n",
    "        mel_outputs, gate_outputs = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs\n",
    "        )\n",
    "\n",
    "        return mel_outputs, gate_outputs\n",
    "\n",
    "def infer(model, text_sequences, input_lengths):\n",
    "    \"\"\"\n",
    "    An inference hook for pretrained synthesizers\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    model: TransformerTTS\n",
    "        the tacotron model\n",
    "    text_sequences: torch.Tensor\n",
    "        encoded text sequences\n",
    "    input_lengths: torch.Tensor\n",
    "        input lengths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: tuple\n",
    "        (mel_outputs_postnet, mel_lengths) - the exact\n",
    "        model output\n",
    "    \"\"\"\n",
    "    return model.infer(text_sequences, input_lengths)\n",
    "\n",
    "\n",
    "LossStats = namedtuple(\n",
    "    \"TransformerLoss\", \"loss mel_loss gate_loss\"\n",
    ")\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    \"\"\"The TransformerTTS loss implementation based on Tacotron2\n",
    "\n",
    "    The loss consists of an MSE loss on the spectrogram and a BCE gate loss\n",
    "\n",
    "    The output of the module is a LossStats tuple, which includes both the\n",
    "    total loss\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    gate_loss_weight: float\n",
    "        The constant by which the gate loss will be multiplied. In the paper, it is 5.0 ~ 8.0\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> _ = torch.manual_seed(42)\n",
    "    >>> from speechbrain.lobes.models.Tacotron2 import Loss\n",
    "    >>> loss = Loss(guided_attention_sigma=0.2)\n",
    "    >>> mel_target = torch.randn(2, 80, 861)\n",
    "    >>> gate_target = torch.randn(1722, 1)\n",
    "    >>> mel_out = torch.randn(2, 80, 861)\n",
    "    >>> mel_out_postnet = torch.randn(2, 80, 861)\n",
    "    >>> gate_out = torch.randn(2, 861)\n",
    "    >>> alignments = torch.randn(2, 861, 173)\n",
    "    >>> targets = mel_target, gate_target\n",
    "    >>> model_outputs = mel_out, mel_out_postnet, gate_out, alignments\n",
    "    >>> input_lengths = torch.tensor([173,  91])\n",
    "    >>> target_lengths = torch.tensor([861, 438])\n",
    "    >>> loss(model_outputs, targets, input_lengths, target_lengths, 1)\n",
    "    TacotronLoss(loss=tensor(4.8566), mel_loss=tensor(4.0097), gate_loss=tensor(0.8460), attn_loss=tensor(0.0010), attn_weight=tensor(1.))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            gate_loss_weight=5.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.gate_loss_weight = gate_loss_weight\n",
    "\n",
    "    def forward(\n",
    "            self, model_output, targets\n",
    "    ):\n",
    "        \"\"\"Computes the loss\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        model_output: tuple\n",
    "            the output of the model's forward():\n",
    "            (mel_outputs, mel_outputs_postnet, gate_outputs, alignments)\n",
    "        targets: tuple\n",
    "            the targets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result: LossStats\n",
    "            the total loss - and individual losses (mel and gate)\n",
    "\n",
    "        \"\"\"\n",
    "        mel_target, gate_target = targets[0], targets[1]\n",
    "        mel_target.requires_grad = False\n",
    "        gate_target.requires_grad = False\n",
    "        gate_target = gate_target.view(-1, 1)\n",
    "\n",
    "        mel_out, mel_out_postnet, gate_out = model_output\n",
    "\n",
    "        gate_out = gate_out.view(-1, 1)\n",
    "\n",
    "        mel_loss = self.mse_loss(mel_out, mel_target) + self.mse_loss(\n",
    "            mel_out_postnet, mel_target\n",
    "        )\n",
    "\n",
    "        gate_loss = self.gate_loss_weight * self.bce_loss(gate_out, gate_target)  # Applying weight to stop token loss\n",
    "        \n",
    "        total_loss = mel_loss + gate_loss\n",
    "        return LossStats(\n",
    "            total_loss, mel_loss, gate_loss\n",
    "        )\n",
    "\n",
    "\n",
    "class TextMelCollate:\n",
    "    \"\"\"Zero-pads model inputs and targets based on number of frames per step\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    n_frames_per_step: int\n",
    "        the number of output frames per step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_frames_per_step=1):\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "\n",
    "    # TODO: Make this more intuitive, use the pipeline\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from normalized text and mel-spectrogram\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch: list\n",
    "            [text_normalized, mel_normalized]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text_padded: torch.Tensor\n",
    "        input_lengths: torch.Tensor\n",
    "        mel_padded: torch.Tensor\n",
    "        gate_padded: torch.Tensor\n",
    "        output_lengths: torch.Tensor\n",
    "        len_x: torch.Tensor\n",
    "        labels: torch.Tensor\n",
    "        wavs: torch.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Remove for loops and this dirty hack\n",
    "        raw_batch = list(batch)\n",
    "        for i in range(\n",
    "                len(batch)\n",
    "        ):  # the pipeline return a dictionary with one element\n",
    "            batch[i] = batch[i][\"mel_text_pair\"]\n",
    "\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x[0]) for x in batch]), dim=0, descending=True\n",
    "        )\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            text = batch[ids_sorted_decreasing[i]][0]\n",
    "            text_padded[i, : text.size(0)] = text\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0][1].size(0)\n",
    "        max_target_len = max([x[1].size(1) for x in batch])\n",
    "        if max_target_len % self.n_frames_per_step != 0:\n",
    "            max_target_len += (\n",
    "                    self.n_frames_per_step - max_target_len % self.n_frames_per_step\n",
    "            )\n",
    "            assert max_target_len % self.n_frames_per_step == 0\n",
    "\n",
    "        # include mel padded and gate padded\n",
    "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_target_len)\n",
    "        mel_padded.zero_()\n",
    "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
    "        gate_padded.zero_()\n",
    "        output_lengths = torch.LongTensor(len(batch))\n",
    "        labels, wavs = [], []\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            idx = ids_sorted_decreasing[i]\n",
    "            mel = batch[idx][1]\n",
    "            mel_padded[i, :, : mel.size(1)] = mel\n",
    "            gate_padded[i, mel.size(1) - 1:] = 1\n",
    "            output_lengths[i] = mel.size(1)\n",
    "            labels.append(raw_batch[idx][\"label\"])\n",
    "            wavs.append(raw_batch[idx][\"wav\"])\n",
    "\n",
    "        # count number of items - characters in text\n",
    "        len_x = [x[2] for x in batch]\n",
    "        len_x = torch.Tensor(len_x)\n",
    "\n",
    "        return (\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            gate_padded,\n",
    "            output_lengths,\n",
    "            len_x,\n",
    "            labels,\n",
    "            wavs,\n",
    "        )\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"This class implements the absolute sinusoidal positional encoding function.\n",
    "    PE(pos, 2i)   = sin(pos/(10000^(2i/dmodel)))\n",
    "    PE(pos, 2i+1) = cos(pos/(10000^(2i/dmodel)))\n",
    "\n",
    "    Based on Cornell & Zhong's implementation in Transformer.py\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    input_size: int\n",
    "        Embedding dimension.\n",
    "    max_len : int, optional\n",
    "        Max length of the input sequences (default 2500).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> a = torch.rand((8, 120, 512))\n",
    "    >>> enc = PositionalEncoding(input_size=a.shape[-1])\n",
    "    >>> b = enc(a)\n",
    "    >>> b.shape\n",
    "    torch.Size([1, 120, 512])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, max_len=2500):\n",
    "        super().__init__()\n",
    "        if input_size % 2 != 0:\n",
    "            raise ValueError(\n",
    "                f\"Cannot use sin/cos positional encoding with odd channels (got channels={input_size})\"\n",
    "            )\n",
    "        self.max_len = max_len\n",
    "        pe = torch.zeros(self.max_len, input_size, requires_grad=False)\n",
    "        positions = torch.arange(0, self.max_len).unsqueeze(1).float()\n",
    "        denominator = torch.exp(\n",
    "            torch.arange(0, input_size, 2).float()\n",
    "            * -(math.log(10000.0) / input_size)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(positions * denominator)\n",
    "        pe[:, 1::2] = torch.cos(positions * denominator)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "        # Define learnable scaling parameter\n",
    "        self.alpha = nn.Parameter(torch.Tensor(1))  # 1-dimensional tensor\n",
    "\n",
    "        # Initialize alpha parameter\n",
    "        nn.init.normal_(self.alpha)  # Initialize with random values\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Input feature shape (batch, time, fea)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The positional encoding.\n",
    "        \"\"\"\n",
    "        scaled_pos_embedding = self.alpha * self.pe[:, : x.size(1)].clone().detach()\n",
    "        return scaled_pos_embedding\n",
    "\n",
    "\n",
    "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
    "    \"\"\"Dynamic range compression for audio signals\"\"\"\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "\n",
    "def mel_spectogram(\n",
    "        sample_rate,\n",
    "        hop_length,\n",
    "        win_length,\n",
    "        n_fft,\n",
    "        n_mels,\n",
    "        f_min,\n",
    "        f_max,\n",
    "        power,\n",
    "        normalized,\n",
    "        norm,\n",
    "        mel_scale,\n",
    "        compression,\n",
    "        audio,\n",
    "):\n",
    "    \"\"\"calculates MelSpectrogram for a raw audio signal\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    sample_rate : int\n",
    "        Sample rate of audio signal.\n",
    "    hop_length : int\n",
    "        Length of hop between STFT windows.\n",
    "    win_length : int\n",
    "        Window size.\n",
    "    n_fft : int\n",
    "        Size of FFT.\n",
    "    n_mels : int\n",
    "        Number of mel filterbanks.\n",
    "    f_min : float\n",
    "        Minimum frequency.\n",
    "    f_max : float\n",
    "        Maximum frequency.\n",
    "    power : float\n",
    "        Exponent for the magnitude spectrogram.\n",
    "    normalized : bool\n",
    "        Whether to normalize by magnitude after stft.\n",
    "    norm : str or None\n",
    "        If \"slaney\", divide the triangular mel weights by the width of the mel band\n",
    "    mel_scale : str\n",
    "        Scale to use: \"htk\" or \"slaney\".\n",
    "    compression : bool\n",
    "        whether to do dynamic range compression\n",
    "    audio : torch.Tensor\n",
    "        input audio signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mel : torch.Tensor\n",
    "        The computed mel spectrogram features.\n",
    "    \"\"\"\n",
    "    from torchaudio import transforms\n",
    "\n",
    "    audio_to_mel = transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        n_fft=n_fft,\n",
    "        n_mels=n_mels,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        power=power,\n",
    "        normalized=normalized,\n",
    "        norm=norm,\n",
    "        mel_scale=mel_scale,\n",
    "    ).to(audio.device)\n",
    "\n",
    "    mel = audio_to_mel(audio)\n",
    "\n",
    "    if compression:\n",
    "        mel = dynamic_range_compression(mel)\n",
    "\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1c9f2-e012-4d35-96d9-faadbb59bef0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### LJSpeech Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2720942-05a6-493d-b0fe-f13e4fbeb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file /notebooks/ljspeech_prepare.py\n",
    "\"\"\"\n",
    "LJspeech data preparation.\n",
    "Download: https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "\n",
    "Authors\n",
    " * Yingzhi WANG 2022\n",
    " * Sathvik Udupa 2022\n",
    " * Pradnya Kandarkar 2023\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from speechbrain.utils.data_utils import download_file\n",
    "from speechbrain.dataio.dataio import load_pkl, save_pkl\n",
    "import tgt\n",
    "from speechbrain.inference.text import GraphemeToPhoneme\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from speechbrain.utils.text_to_sequence import _g2p_keep_punctuations\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "OPT_FILE = \"opt_ljspeech_prepare.pkl\"\n",
    "METADATA_CSV = \"metadata.csv\"\n",
    "TRAIN_JSON = \"train.json\"\n",
    "VALID_JSON = \"valid.json\"\n",
    "TEST_JSON = \"test.json\"\n",
    "WAVS = \"wavs\"\n",
    "DURATIONS = \"durations\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "OPT_FILE = \"opt_ljspeech_prepare.pkl\"\n",
    "\n",
    "\n",
    "def prepare_ljspeech(\n",
    "    data_folder,\n",
    "    save_folder,\n",
    "    splits=[\"train\", \"valid\"],\n",
    "    split_ratio=[90, 10],\n",
    "    model_name=None,\n",
    "    seed=1234,\n",
    "    pitch_n_fft=1024,\n",
    "    pitch_hop_length=256,\n",
    "    pitch_min_f0=65,\n",
    "    pitch_max_f0=400,\n",
    "    skip_prep=False,\n",
    "    use_custom_cleaner=False,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the csv files for the LJspeech datasets.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    data_folder : str\n",
    "        Path to the folder where the original LJspeech dataset is stored\n",
    "    save_folder : str\n",
    "        The directory where to store the csv/json files\n",
    "    splits : list\n",
    "        List of dataset splits to prepare\n",
    "    split_ratio : list\n",
    "        Proportion for dataset splits\n",
    "    model_name : str\n",
    "        Model name (used to prepare additional model specific data)\n",
    "    seed : int\n",
    "        Random seed\n",
    "    pitch_n_fft : int\n",
    "        Number of fft points for pitch computation\n",
    "    pitch_hop_length : int\n",
    "        Hop length for pitch computation\n",
    "    pitch_min_f0 : int\n",
    "        Minimum f0 for pitch computation\n",
    "    pitch_max_f0 : int\n",
    "        Max f0 for pitch computation\n",
    "    skip_prep : bool\n",
    "        If True, skip preparation\n",
    "    use_custom_cleaner : bool\n",
    "        If True, uses custom cleaner defined for this recipe\n",
    "    device : str\n",
    "        Device for to be used for computation (used as required)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> from recipes.LJSpeech.TTS.ljspeech_prepare import prepare_ljspeech\n",
    "    >>> data_folder = 'data/LJspeech/'\n",
    "    >>> save_folder = 'save/'\n",
    "    >>> splits = ['train', 'valid']\n",
    "    >>> split_ratio = [90, 10]\n",
    "    >>> seed = 1234\n",
    "    >>> prepare_ljspeech(data_folder, save_folder, splits, split_ratio, seed)\n",
    "    \"\"\"\n",
    "    # Sets seeds for reproducible code\n",
    "    random.seed(seed)\n",
    "\n",
    "    if skip_prep:\n",
    "        return\n",
    "\n",
    "    # Creating configuration for easily skipping data_preparation stage\n",
    "    conf = {\n",
    "        \"data_folder\": data_folder,\n",
    "        \"splits\": splits,\n",
    "        \"split_ratio\": split_ratio,\n",
    "        \"save_folder\": save_folder,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    # Setting output files\n",
    "    meta_csv = os.path.join(data_folder, METADATA_CSV)\n",
    "    wavs_folder = os.path.join(data_folder, WAVS)\n",
    "\n",
    "    save_opt = os.path.join(save_folder, OPT_FILE)\n",
    "    save_json_train = os.path.join(save_folder, TRAIN_JSON)\n",
    "    save_json_valid = os.path.join(save_folder, VALID_JSON)\n",
    "    save_json_test = os.path.join(save_folder, TEST_JSON)\n",
    "\n",
    "    phoneme_alignments_folder = None\n",
    "    duration_folder = None\n",
    "    pitch_folder = None\n",
    "    # Setting up additional folders required for FastSpeech2\n",
    "    if model_name is not None and \"FastSpeech2\" in model_name:\n",
    "        # This step requires phoneme alignments to be present in the data_folder\n",
    "        # We automatically download the alignments from https://www.dropbox.com/s/v28x5ldqqa288pu/LJSpeech.zip\n",
    "        # Download and unzip LJSpeech phoneme alignments from here: https://drive.google.com/drive/folders/1DBRkALpPd6FL9gjHMmMEdHODmkgNIIK4\n",
    "        alignment_URL = (\n",
    "            \"https://www.dropbox.com/s/v28x5ldqqa288pu/LJSpeech.zip?dl=1\"\n",
    "        )\n",
    "        phoneme_alignments_folder = os.path.join(\n",
    "            data_folder, \"TextGrid\", \"LJSpeech\"\n",
    "        )\n",
    "        download_file(\n",
    "            alignment_URL, data_folder + \"/alignments.zip\", unpack=True\n",
    "        )\n",
    "\n",
    "        duration_folder = os.path.join(data_folder, \"durations\")\n",
    "        if not os.path.exists(duration_folder):\n",
    "            os.makedirs(duration_folder)\n",
    "\n",
    "        # extract pitch for both Fastspeech2 and FastSpeech2WithAligner models\n",
    "        pitch_folder = os.path.join(data_folder, \"pitch\")\n",
    "        if not os.path.exists(pitch_folder):\n",
    "            os.makedirs(pitch_folder)\n",
    "\n",
    "    # Check if this phase is already done (if so, skip it)\n",
    "    if skip(splits, save_folder, conf):\n",
    "        logger.info(\"Skipping preparation, completed in previous run.\")\n",
    "        return\n",
    "\n",
    "    # Additional check to make sure metadata.csv and wavs folder exists\n",
    "    assert os.path.exists(meta_csv), \"metadata.csv does not exist\"\n",
    "    assert os.path.exists(wavs_folder), \"wavs/ folder does not exist\"\n",
    "\n",
    "    # Prepare data splits\n",
    "    msg = \"Creating json file for ljspeech Dataset..\"\n",
    "    logger.info(msg)\n",
    "    data_split, meta_csv = split_sets(data_folder, splits, split_ratio)\n",
    "\n",
    "    if \"train\" in splits:\n",
    "        prepare_json(\n",
    "            model_name,\n",
    "            data_split[\"train\"],\n",
    "            save_json_train,\n",
    "            wavs_folder,\n",
    "            meta_csv,\n",
    "            phoneme_alignments_folder,\n",
    "            duration_folder,\n",
    "            pitch_folder,\n",
    "            pitch_n_fft,\n",
    "            pitch_hop_length,\n",
    "            pitch_min_f0,\n",
    "            pitch_max_f0,\n",
    "            use_custom_cleaner,\n",
    "            device,\n",
    "        )\n",
    "    if \"valid\" in splits:\n",
    "        prepare_json(\n",
    "            model_name,\n",
    "            data_split[\"valid\"],\n",
    "            save_json_valid,\n",
    "            wavs_folder,\n",
    "            meta_csv,\n",
    "            phoneme_alignments_folder,\n",
    "            duration_folder,\n",
    "            pitch_folder,\n",
    "            pitch_n_fft,\n",
    "            pitch_hop_length,\n",
    "            pitch_min_f0,\n",
    "            pitch_max_f0,\n",
    "            use_custom_cleaner,\n",
    "            device,\n",
    "        )\n",
    "    if \"test\" in splits:\n",
    "        prepare_json(\n",
    "            model_name,\n",
    "            data_split[\"test\"],\n",
    "            save_json_test,\n",
    "            wavs_folder,\n",
    "            meta_csv,\n",
    "            phoneme_alignments_folder,\n",
    "            duration_folder,\n",
    "            pitch_folder,\n",
    "            pitch_n_fft,\n",
    "            pitch_hop_length,\n",
    "            pitch_min_f0,\n",
    "            pitch_max_f0,\n",
    "            use_custom_cleaner,\n",
    "            device,\n",
    "        )\n",
    "    save_pkl(conf, save_opt)\n",
    "\n",
    "\n",
    "def skip(splits, save_folder, conf):\n",
    "    \"\"\"\n",
    "    Detects if the ljspeech data_preparation has been already done.\n",
    "    If the preparation has been done, we can skip it.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    splits : list\n",
    "        The portions of data to review.\n",
    "    save_folder : str\n",
    "        The path to the directory containing prepared files.\n",
    "    conf : dict\n",
    "        Configuration to match against saved config.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        if True, the preparation phase can be skipped.\n",
    "        if False, it must be done.\n",
    "    \"\"\"\n",
    "    # Checking json files\n",
    "    skip = True\n",
    "\n",
    "    split_files = {\n",
    "        \"train\": TRAIN_JSON,\n",
    "        \"valid\": VALID_JSON,\n",
    "        \"test\": TEST_JSON,\n",
    "    }\n",
    "\n",
    "    for split in splits:\n",
    "        if not os.path.isfile(os.path.join(save_folder, split_files[split])):\n",
    "            skip = False\n",
    "\n",
    "    #  Checking saved options\n",
    "    save_opt = os.path.join(save_folder, OPT_FILE)\n",
    "    if skip is True:\n",
    "        if os.path.isfile(save_opt):\n",
    "            opts_old = load_pkl(save_opt)\n",
    "            if opts_old == conf:\n",
    "                skip = True\n",
    "            else:\n",
    "                skip = False\n",
    "        else:\n",
    "            skip = False\n",
    "    return skip\n",
    "\n",
    "\n",
    "def split_sets(data_folder, splits, split_ratio):\n",
    "    \"\"\"Randomly splits the wav list into training, validation, and test lists.\n",
    "    Note that a better approach is to make sure that all the classes have the\n",
    "    same proportion of samples for each session.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    data_folder : str\n",
    "        The path to the directory containing the data.\n",
    "    splits : list\n",
    "        The list of the selected splits.\n",
    "    split_ratio : list\n",
    "        List composed of three integers that sets split ratios for train,\n",
    "        valid, and test sets, respectively.\n",
    "        For instance split_ratio=[80, 10, 10] will assign 80% of the sentences\n",
    "        to training, 10% for validation, and 10% for test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary containing train, valid, and test splits.\n",
    "    \"\"\"\n",
    "    meta_csv = os.path.join(data_folder, METADATA_CSV)\n",
    "    csv_reader = csv.reader(\n",
    "        open(meta_csv), delimiter=\"|\", quoting=csv.QUOTE_NONE\n",
    "    )\n",
    "\n",
    "    meta_csv = list(csv_reader)\n",
    "\n",
    "    index_for_sessions = []\n",
    "    session_id_start = \"LJ001\"\n",
    "    index_this_session = []\n",
    "    for i in range(len(meta_csv)):\n",
    "        session_id = meta_csv[i][0].split(\"-\")[0]\n",
    "        if session_id == session_id_start:\n",
    "            index_this_session.append(i)\n",
    "            if i == len(meta_csv) - 1:\n",
    "                index_for_sessions.append(index_this_session)\n",
    "        else:\n",
    "            index_for_sessions.append(index_this_session)\n",
    "            session_id_start = session_id\n",
    "            index_this_session = [i]\n",
    "\n",
    "    session_len = [len(session) for session in index_for_sessions]\n",
    "\n",
    "    data_split = {}\n",
    "    for i, split in enumerate(splits):\n",
    "        data_split[split] = []\n",
    "        for j in range(len(index_for_sessions)):\n",
    "            if split == \"train\":\n",
    "                random.shuffle(index_for_sessions[j])\n",
    "                n_snts = int(session_len[j] * split_ratio[i] / sum(split_ratio))\n",
    "                data_split[split].extend(index_for_sessions[j][0:n_snts])\n",
    "                del index_for_sessions[j][0:n_snts]\n",
    "            if split == \"valid\":\n",
    "                if \"test\" in splits:\n",
    "                    random.shuffle(index_for_sessions[j])\n",
    "                    n_snts = int(\n",
    "                        session_len[j] * split_ratio[i] / sum(split_ratio)\n",
    "                    )\n",
    "                    data_split[split].extend(index_for_sessions[j][0:n_snts])\n",
    "                    del index_for_sessions[j][0:n_snts]\n",
    "                else:\n",
    "                    data_split[split].extend(index_for_sessions[j])\n",
    "            if split == \"test\":\n",
    "                data_split[split].extend(index_for_sessions[j])\n",
    "\n",
    "    return data_split, meta_csv\n",
    "\n",
    "\n",
    "def prepare_json(\n",
    "    model_name,\n",
    "    seg_lst,\n",
    "    json_file,\n",
    "    wavs_folder,\n",
    "    csv_reader,\n",
    "    phoneme_alignments_folder,\n",
    "    durations_folder,\n",
    "    pitch_folder,\n",
    "    pitch_n_fft,\n",
    "    pitch_hop_length,\n",
    "    pitch_min_f0,\n",
    "    pitch_max_f0,\n",
    "    use_custom_cleaner=False,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates json file given a list of indexes.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    model_name : str\n",
    "        Model name (used to prepare additional model specific data)\n",
    "    seg_lst : list\n",
    "        The list of json indexes of a given data split\n",
    "    json_file : str\n",
    "        Output json path\n",
    "    wavs_folder : str\n",
    "        LJspeech wavs folder\n",
    "    csv_reader : _csv.reader\n",
    "        LJspeech metadata\n",
    "    phoneme_alignments_folder : path\n",
    "        Path where the phoneme alignments are stored\n",
    "    durations_folder : path\n",
    "        Folder where to store the duration values of each audio\n",
    "    pitch_folder : path\n",
    "        Folder where to store the pitch of each audio\n",
    "    pitch_n_fft : int\n",
    "        Number of fft points for pitch computation\n",
    "    pitch_hop_length : int\n",
    "        Hop length for pitch computation\n",
    "    pitch_min_f0 : int\n",
    "        Minimum f0 for pitch computation\n",
    "    pitch_max_f0 : int\n",
    "        Max f0 for pitch computation\n",
    "    use_custom_cleaner : bool\n",
    "        If True, uses custom cleaner defined for this recipe\n",
    "    device : str\n",
    "        Device for to be used for computation (used as required)\n",
    "    \"\"\"\n",
    "    g2p = GraphemeToPhoneme.from_hparams(\n",
    "            \"speechbrain/soundchoice-g2p\", run_opts={\"device\": device}\n",
    "        )\n",
    "    logger.info(f\"preparing {json_file}.\")\n",
    "    if model_name in [\"Tacotron2\", \"FastSpeech2WithAlignment\", \"TransformerTTS\"]:\n",
    "        logger.info(\n",
    "            \"Computing phonemes for LJSpeech labels using SpeechBrain G2P. This may take a while.\"\n",
    "        )\n",
    "    if model_name is not None and \"FastSpeech2\" in model_name:\n",
    "        logger.info(\n",
    "            \"Computing pitch as required for FastSpeech2. This may take a while.\"\n",
    "        )\n",
    "\n",
    "    json_dict = {}\n",
    "    for index in tqdm(seg_lst):\n",
    "        # Common data preparation\n",
    "        id = list(csv_reader)[index][0]\n",
    "        wav = os.path.join(wavs_folder, f\"{id}.wav\")\n",
    "        label = list(csv_reader)[index][2]\n",
    "        if use_custom_cleaner:\n",
    "            label = custom_clean(label, model_name)\n",
    "\n",
    "        json_dict[id] = {\n",
    "            \"uttid\": id,\n",
    "            \"wav\": wav,\n",
    "            \"label\": label,\n",
    "            \"segment\": True if \"train\" in json_file else False,\n",
    "        }\n",
    "\n",
    "        # FastSpeech2 specific data preparation\n",
    "        if model_name == \"FastSpeech2\":\n",
    "            audio, fs = torchaudio.load(wav)\n",
    "\n",
    "            # Parses phoneme alignments\n",
    "            textgrid_path = os.path.join(\n",
    "                phoneme_alignments_folder, f\"{id}.TextGrid\"\n",
    "            )\n",
    "            textgrid = tgt.io.read_textgrid(\n",
    "                textgrid_path, include_empty_intervals=True\n",
    "            )\n",
    "\n",
    "            last_phoneme_flags = get_last_phoneme_info(\n",
    "                textgrid.get_tier_by_name(\"words\"),\n",
    "                textgrid.get_tier_by_name(\"phones\"),\n",
    "            )\n",
    "            (\n",
    "                phonemes,\n",
    "                duration,\n",
    "                start,\n",
    "                end,\n",
    "                trimmed_last_phoneme_flags,\n",
    "            ) = get_alignment(\n",
    "                textgrid.get_tier_by_name(\"phones\"),\n",
    "                fs,\n",
    "                pitch_hop_length,\n",
    "                last_phoneme_flags,\n",
    "            )\n",
    "\n",
    "            # Gets label phonemes\n",
    "            label_phoneme = \" \".join(phonemes)\n",
    "            spn_labels = [0] * len(phonemes)\n",
    "            for i in range(1, len(phonemes)):\n",
    "                if phonemes[i] == \"spn\":\n",
    "                    spn_labels[i - 1] = 1\n",
    "            if start >= end:\n",
    "                print(f\"Skipping {id}\")\n",
    "                continue\n",
    "\n",
    "            # Saves durations\n",
    "            duration_file_path = os.path.join(durations_folder, f\"{id}.npy\")\n",
    "            np.save(duration_file_path, duration)\n",
    "\n",
    "            # Computes pitch\n",
    "            audio = audio[:, int(fs * start) : int(fs * end)]\n",
    "            pitch_file = wav.replace(\".wav\", \".npy\").replace(\n",
    "                wavs_folder, pitch_folder\n",
    "            )\n",
    "            if not os.path.isfile(pitch_file):\n",
    "                pitch = torchaudio.functional.detect_pitch_frequency(\n",
    "                    waveform=audio,\n",
    "                    sample_rate=fs,\n",
    "                    frame_time=(pitch_hop_length / fs),\n",
    "                    win_length=3,\n",
    "                    freq_low=pitch_min_f0,\n",
    "                    freq_high=pitch_max_f0,\n",
    "                ).squeeze(0)\n",
    "\n",
    "                # Concatenate last element to match duration.\n",
    "                pitch = torch.cat([pitch, pitch[-1].unsqueeze(0)])\n",
    "\n",
    "                # Mean and Variance Normalization\n",
    "                mean = 256.1732939688805\n",
    "                std = 328.319759158607\n",
    "\n",
    "                pitch = (pitch - mean) / std\n",
    "\n",
    "                pitch = pitch[: sum(duration)]\n",
    "                np.save(pitch_file, pitch)\n",
    "\n",
    "            # Updates data for the utterance\n",
    "            json_dict[id].update({\"label_phoneme\": label_phoneme})\n",
    "            json_dict[id].update({\"spn_labels\": spn_labels})\n",
    "            json_dict[id].update({\"start\": start})\n",
    "            json_dict[id].update({\"end\": end})\n",
    "            json_dict[id].update({\"durations\": duration_file_path})\n",
    "            json_dict[id].update({\"pitch\": pitch_file})\n",
    "            json_dict[id].update(\n",
    "                {\"last_phoneme_flags\": trimmed_last_phoneme_flags}\n",
    "            )\n",
    "\n",
    "        # FastSpeech2WithAlignment specific data preparation\n",
    "        if model_name == \"FastSpeech2WithAlignment\":\n",
    "            audio, fs = torchaudio.load(wav)\n",
    "            # Computes pitch\n",
    "            pitch_file = wav.replace(\".wav\", \".npy\").replace(\n",
    "                wavs_folder, pitch_folder\n",
    "            )\n",
    "            if not os.path.isfile(pitch_file):\n",
    "                if torchaudio.__version__ < \"2.1\":\n",
    "                    pitch = torchaudio.functional.compute_kaldi_pitch(\n",
    "                        waveform=audio,\n",
    "                        sample_rate=fs,\n",
    "                        frame_length=(pitch_n_fft / fs * 1000),\n",
    "                        frame_shift=(pitch_hop_length / fs * 1000),\n",
    "                        min_f0=pitch_min_f0,\n",
    "                        max_f0=pitch_max_f0,\n",
    "                    )[0, :, 0]\n",
    "                else:\n",
    "                    pitch = torchaudio.functional.detect_pitch_frequency(\n",
    "                        waveform=audio,\n",
    "                        sample_rate=fs,\n",
    "                        frame_time=(pitch_hop_length / fs),\n",
    "                        win_length=3,\n",
    "                        freq_low=pitch_min_f0,\n",
    "                        freq_high=pitch_max_f0,\n",
    "                    ).squeeze(0)\n",
    "\n",
    "                    # Concatenate last element to match duration.\n",
    "                    pitch = torch.cat([pitch, pitch[-1].unsqueeze(0)])\n",
    "\n",
    "                    # Mean and Variance Normalization\n",
    "                    mean = 256.1732939688805\n",
    "                    std = 328.319759158607\n",
    "\n",
    "                    pitch = (pitch - mean) / std\n",
    "\n",
    "                np.save(pitch_file, pitch)\n",
    "\n",
    "            phonemes = _g2p_keep_punctuations(g2p, label)\n",
    "            # Updates data for the utterance\n",
    "            json_dict[id].update({\"phonemes\": phonemes})\n",
    "            json_dict[id].update({\"pitch\": pitch_file})\n",
    "\n",
    "        if model_name == \"TransformerTTS\":\n",
    "            phonemes = _g2p_keep_punctuations(g2p, label)\n",
    "            # Updates data for the utterance\n",
    "            json_dict[id].update({\"phonemes\": phonemes})\n",
    "\n",
    "    # Writing the dictionary to the json file\n",
    "    with open(json_file, mode=\"w\") as json_f:\n",
    "        json.dump(json_dict, json_f, indent=2)\n",
    "\n",
    "    logger.info(f\"{json_file} successfully created!\")\n",
    "\n",
    "\n",
    "def get_alignment(tier, sampling_rate, hop_length, last_phoneme_flags):\n",
    "    \"\"\"\n",
    "    Returns phonemes, phoneme durations (in frames), start time (in seconds), end time (in seconds).\n",
    "    This function is adopted from https://github.com/ming024/FastSpeech2/blob/master/preprocessor/preprocessor.py\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    tier : tgt.core.IntervalTier\n",
    "        For an utterance, contains Interval objects for phonemes and their start time and end time in seconds\n",
    "    sampling_rate : int\n",
    "        Sample rate if audio signal\n",
    "    hop_length : int\n",
    "        Hop length for duration computation\n",
    "    last_phoneme_flags : list\n",
    "        List of (phoneme, flag) tuples with flag=1 if the phoneme is the last phoneme else flag=0\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (phones, durations, start_time, end_time) : tuple\n",
    "        The phonemes, durations, start time, and end time for an utterance\n",
    "    \"\"\"\n",
    "\n",
    "    sil_phones = [\"sil\", \"sp\", \"spn\", \"\"]\n",
    "\n",
    "    phonemes = []\n",
    "    durations = []\n",
    "    start_time = 0\n",
    "    end_time = 0\n",
    "    end_idx = 0\n",
    "    trimmed_last_phoneme_flags = []\n",
    "\n",
    "    flag_iter = iter(last_phoneme_flags)\n",
    "\n",
    "    for t in tier._objects:\n",
    "        s, e, p = t.start_time, t.end_time, t.text\n",
    "        current_flag = next(flag_iter)\n",
    "\n",
    "        # Trims leading silences\n",
    "        if phonemes == []:\n",
    "            if p in sil_phones:\n",
    "                continue\n",
    "            else:\n",
    "                start_time = s\n",
    "\n",
    "        if p not in sil_phones:\n",
    "            # For ordinary phones\n",
    "            # Removes stress indicators\n",
    "            if p[-1].isdigit():\n",
    "                phonemes.append(p[:-1])\n",
    "            else:\n",
    "                phonemes.append(p)\n",
    "            trimmed_last_phoneme_flags.append(current_flag[1])\n",
    "            end_time = e\n",
    "            end_idx = len(phonemes)\n",
    "        else:\n",
    "            # Uses a unique token for all silent phones\n",
    "            phonemes.append(\"spn\")\n",
    "            trimmed_last_phoneme_flags.append(current_flag[1])\n",
    "\n",
    "        durations.append(\n",
    "            int(\n",
    "                np.round(e * sampling_rate / hop_length)\n",
    "                - np.round(s * sampling_rate / hop_length)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Trims tailing silences\n",
    "    phonemes = phonemes[:end_idx]\n",
    "    durations = durations[:end_idx]\n",
    "\n",
    "    return phonemes, durations, start_time, end_time, trimmed_last_phoneme_flags\n",
    "\n",
    "\n",
    "def get_last_phoneme_info(words_seq, phones_seq):\n",
    "    \"\"\"This function takes word and phoneme tiers from a TextGrid file as input\n",
    "    and provides a list of tuples for the phoneme sequence indicating whether\n",
    "    each of the phonemes is the last phoneme of a word or not.\n",
    "\n",
    "    Each tuple of the returned list has this format: (phoneme, flag)\n",
    "\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    words_seq : tier\n",
    "        word tier from a TextGrid file\n",
    "    phones_seq : tier\n",
    "        phoneme tier from a TextGrid file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    last_phoneme_flags : list\n",
    "        each tuple of the returned list has this format: (phoneme, flag)\n",
    "    \"\"\"\n",
    "\n",
    "    # Gets all phoneme objects for the entire sequence\n",
    "    phoneme_objects = phones_seq._objects\n",
    "    phoneme_iter = iter(phoneme_objects)\n",
    "\n",
    "    # Stores flags to show if an element (phoneme) is a the last phoneme of a word\n",
    "    last_phoneme_flags = list()\n",
    "\n",
    "    # Matches the end times of the phoneme and word objects to get the last phoneme information\n",
    "    for word_obj in words_seq._objects:\n",
    "        word_end_time = word_obj.end_time\n",
    "\n",
    "        current_phoneme = next(phoneme_iter, None)\n",
    "        while current_phoneme:\n",
    "            phoneme_end_time = current_phoneme.end_time\n",
    "            if phoneme_end_time == word_end_time:\n",
    "                last_phoneme_flags.append((current_phoneme.text, 1))\n",
    "                break\n",
    "            else:\n",
    "                last_phoneme_flags.append((current_phoneme.text, 0))\n",
    "            current_phoneme = next(phoneme_iter, None)\n",
    "\n",
    "    return last_phoneme_flags\n",
    "\n",
    "\n",
    "def custom_clean(text, model_name):\n",
    "    \"\"\"\n",
    "    Uses custom criteria to clean text.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    text : str\n",
    "        Input text to be cleaned\n",
    "    model_name : str\n",
    "        whether to treat punctuations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    _abbreviations = [\n",
    "        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n",
    "        for x in [\n",
    "            (\"mrs\", \"missus\"),\n",
    "            (\"mr\", \"mister\"),\n",
    "            (\"dr\", \"doctor\"),\n",
    "            (\"st\", \"saint\"),\n",
    "            (\"co\", \"company\"),\n",
    "            (\"jr\", \"junior\"),\n",
    "            (\"maj\", \"major\"),\n",
    "            (\"gen\", \"general\"),\n",
    "            (\"drs\", \"doctors\"),\n",
    "            (\"rev\", \"reverend\"),\n",
    "            (\"lt\", \"lieutenant\"),\n",
    "            (\"hon\", \"honorable\"),\n",
    "            (\"sgt\", \"sergeant\"),\n",
    "            (\"capt\", \"captain\"),\n",
    "            (\"esq\", \"esquire\"),\n",
    "            (\"ltd\", \"limited\"),\n",
    "            (\"col\", \"colonel\"),\n",
    "            (\"ft\", \"fort\"),\n",
    "        ]\n",
    "    ]\n",
    "    text = unidecode(text.lower())\n",
    "    if model_name != \"FastSpeech2WithAlignment\":\n",
    "        text = re.sub(\"[:;]\", \" - \", text)\n",
    "        text = re.sub(r'[)(\\[\\]\"]', \" \", text)\n",
    "        text = text.strip().strip().strip(\"-\")\n",
    "\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    for regex, replacement in _abbreviations:\n",
    "        text = re.sub(regex, replacement, text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b8880-17b5-489d-bfdb-e6d708aa1f5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e58f6cd-146e-4930-8897-f23d36c08319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /notebooks/train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%file /notebooks/train.yaml\n",
    "\n",
    "############################################################################\n",
    "# Model: TransformerTTS\n",
    "# Tokens: Phonemes (ARPABET)\n",
    "# Training: LJSpeech\n",
    "# Authors: Salman Hussain Ali, 2024\n",
    "# ############################################################################\n",
    "\n",
    "\n",
    "###################################\n",
    "# Experiment Parameters and setup #\n",
    "###################################\n",
    "seed: 1234\n",
    "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
    "output_folder: !ref ./results/transformertts/<seed>\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "epochs: 750\n",
    "keep_checkpoint_interval: 50\n",
    "model_name: \"TransformerTTS\"\n",
    "\n",
    "###################################\n",
    "# Progress Samples                #\n",
    "###################################\n",
    "# Progress samples are used to monitor the progress\n",
    "# of an ongoing training session by outputting samples\n",
    "# of spectrograms, alignments, etc at regular intervals\n",
    "\n",
    "# Whether to enable progress samples\n",
    "progress_samples: True\n",
    "\n",
    "# The path where the samples will be stored\n",
    "progress_sample_path: !ref <output_folder>/samples\n",
    "# The interval, in epochs. For instance, if it is set to 5,\n",
    "# progress samples will be output every 5 epochs\n",
    "progress_samples_interval: 1\n",
    "# The sample size for raw batch samples saved in batch.pth\n",
    "# (useful mostly for model debugging)\n",
    "progress_batch_sample_size: 3\n",
    "\n",
    "#################################\n",
    "# Data files and pre-processing #\n",
    "#################################\n",
    "data_folder: !PLACEHOLDER # e.g, /localscratch/ljspeech\n",
    "\n",
    "train_json: !ref <save_folder>/train.json\n",
    "valid_json: !ref <save_folder>/valid.json\n",
    "test_json: !ref <save_folder>/test.json\n",
    "\n",
    "splits: [\"train\",\"valid\"]\n",
    "split_ratio: [90,10]\n",
    "\n",
    "skip_prep: False\n",
    "\n",
    "# Use the original preprocessing from nvidia\n",
    "# The cleaners to be used (applicable to nvidia only)\n",
    "text_cleaners: ['english_cleaners']\n",
    "\n",
    "################################\n",
    "# Audio Parameters             #\n",
    "################################\n",
    "sample_rate: 16000\n",
    "hop_length: 200\n",
    "win_length: 1024\n",
    "n_mel_channels: 80\n",
    "n_fft: 1024\n",
    "mel_fmin: 0.0\n",
    "mel_fmax: 8000.0\n",
    "mel_normalized: False\n",
    "power: 1\n",
    "norm: \"slaney\"\n",
    "mel_scale: \"slaney\"\n",
    "dynamic_range_compression: True\n",
    "\n",
    "################################\n",
    "# Optimization Hyperparameters #\n",
    "################################\n",
    "learning_rate: 0.001\n",
    "weight_decay: 0.000006\n",
    "batch_size: 32 #minimum 2\n",
    "num_workers: 8\n",
    "mask_padding: True\n",
    "gate_loss_weight: 7.0\n",
    "\n",
    "train_dataloader_opts:\n",
    "  batch_size: !ref <batch_size>\n",
    "  drop_last: True  #True #False\n",
    "  num_workers: !ref <num_workers>\n",
    "  collate_fn: !new:models.TransformerTTS.TextMelCollate\n",
    "\n",
    "valid_dataloader_opts:\n",
    "  batch_size: !ref <batch_size>\n",
    "  num_workers: !ref <num_workers>\n",
    "  collate_fn: !new:models.TransformerTTS.TextMelCollate\n",
    "\n",
    "test_dataloader_opts:\n",
    "  batch_size: !ref <batch_size>\n",
    "  num_workers: !ref <num_workers>\n",
    "  collate_fn: !new:models.TransformerTTS.TextMelCollate\n",
    "\n",
    "################################\n",
    "# Model Parameters and model   #\n",
    "################################\n",
    "\n",
    "# Input parameters\n",
    "lexicon:\n",
    "    - \"AA\"\n",
    "    - \"AE\"\n",
    "    - \"AH\"\n",
    "    - \"AO\"\n",
    "    - \"AW\"\n",
    "    - \"AY\"\n",
    "    - \"B\"\n",
    "    - \"CH\"\n",
    "    - \"D\"\n",
    "    - \"DH\"\n",
    "    - \"EH\"\n",
    "    - \"ER\"\n",
    "    - \"EY\"\n",
    "    - \"F\"\n",
    "    - \"G\"\n",
    "    - \"HH\"\n",
    "    - \"IH\"\n",
    "    - \"IY\"\n",
    "    - \"JH\"\n",
    "    - \"K\"\n",
    "    - \"L\"\n",
    "    - \"M\"\n",
    "    - \"N\"\n",
    "    - \"NG\"\n",
    "    - \"OW\"\n",
    "    - \"OY\"\n",
    "    - \"P\"\n",
    "    - \"R\"\n",
    "    - \"S\"\n",
    "    - \"SH\"\n",
    "    - \"T\"\n",
    "    - \"TH\"\n",
    "    - \"UH\"\n",
    "    - \"UW\"\n",
    "    - \"V\"\n",
    "    - \"W\"\n",
    "    - \"Y\"\n",
    "    - \"Z\"\n",
    "    - \"ZH\"\n",
    "    - \"-\"\n",
    "    - \"!\"\n",
    "    - \"'\"\n",
    "    - \"(\"\n",
    "    - \")\"\n",
    "    - \",\"\n",
    "    - \".\"\n",
    "    - \":\"\n",
    "    - \";\"\n",
    "    - \"?\"\n",
    "    - \" \"\n",
    "\n",
    "n_symbols: 52 #fixed depending on symbols in the lexicon (+1 for a dummy symbol used for padding, +1 for unknown)\n",
    "padding_idx: 0\n",
    "symbols_embedding_dim: 512\n",
    "\n",
    "# Encoder Pre-Net parameters\n",
    "encoder_kernel_size: 5\n",
    "encoder_n_convolutions: 3\n",
    "encoder_embedding_dim: 512\n",
    "encoder_stride: 1\n",
    "encoder_prenet_dropout: 0.1\n",
    "encoder_dilation: 1\n",
    "encoder_bias: True\n",
    "encoder_padding: \"same\"\n",
    "\n",
    "# Decoder Pre-Net parameters\n",
    "# The number of frames in the target per encoder step\n",
    "n_frames_per_step: 1\n",
    "decoder_prenet_dim: 256\n",
    "max_decoder_steps: 1000 \n",
    "p_decoder_dropout: 0.5\n",
    "\n",
    "# Transformer parameters\n",
    "d_model: 256\n",
    "transformer_nhead: 8\n",
    "transformer_num_encoder_layers: 6\n",
    "transformer_num_decoder_layers: 6\n",
    "transformer_d_ffn: 2048\n",
    "transformer_dropout: 0.1\n",
    "transformer_activation: \"relu\"\n",
    "\n",
    "# Mel-post processing network parameters\n",
    "postnet_embedding_dim: 512\n",
    "postnet_kernel_size: 5\n",
    "postnet_n_convolutions: 5\n",
    "gate_threshold: 0.5 # TODO - maybe remove\n",
    "\n",
    "\n",
    "mel_spectogram: !name:models.TransformerTTS.mel_spectogram\n",
    "  sample_rate: !ref <sample_rate>\n",
    "  hop_length: !ref <hop_length>\n",
    "  win_length: !ref <win_length>\n",
    "  n_fft: !ref <n_fft>\n",
    "  n_mels: !ref <n_mel_channels>\n",
    "  f_min: !ref <mel_fmin>\n",
    "  f_max: !ref <mel_fmax>\n",
    "  power: !ref <power>\n",
    "  normalized: !ref <mel_normalized>\n",
    "  norm: !ref <norm>\n",
    "  mel_scale: !ref <mel_scale>\n",
    "  compression: !ref <dynamic_range_compression>\n",
    "\n",
    "#model\n",
    "model: !new:models.TransformerTTS.TransformerTTS\n",
    "  mask_padding: !ref <mask_padding>\n",
    "  n_mel_channels: !ref <n_mel_channels>\n",
    "  # symbols\n",
    "  n_symbols: !ref <n_symbols>\n",
    "  symbols_embedding_dim: !ref <symbols_embedding_dim>\n",
    "  # encoder pre-net\n",
    "  #encoder_embedding_dim: !ref <encoder_embedding_dim>\n",
    "  encoder_prenet_kernel_size: !ref <encoder_kernel_size>\n",
    "  encoder_prenet_n_convolutions: !ref <encoder_n_convolutions>\n",
    "  #encoder_prenet_padding: !ref <encoder_padding>\n",
    "  encoder_prenet_dilation: !ref <encoder_dilation>\n",
    "  encoder_prenet_bias: !ref <encoder_bias>\n",
    "  encoder_prenet_dropout: !ref <encoder_prenet_dropout>\n",
    "  encoder_prenet_stride: !ref <encoder_stride>\n",
    "  # decoder pre-net\n",
    "  n_frames_per_step: !ref <n_frames_per_step>\n",
    "  decoder_prenet_hidden_dims: !ref <decoder_prenet_dim>\n",
    "  decoder_prenet_dropout: !ref <p_decoder_dropout>\n",
    "  # transformer\n",
    "  d_model: !ref <d_model>\n",
    "  transformer_nhead: !ref <transformer_nhead>\n",
    "  transformer_num_encoder_layers: !ref <transformer_num_encoder_layers>\n",
    "  transformer_num_decoder_layers: !ref <transformer_num_decoder_layers>\n",
    "  transformer_d_ffn: !ref <transformer_d_ffn>\n",
    "  transformer_dropout: !ref <transformer_dropout>\n",
    "  transformer_activation: !ref <transformer_activation>\n",
    "  # postnet\n",
    "  postnet_embedding_dim: !ref <postnet_embedding_dim>\n",
    "  postnet_kernel_size: !ref <postnet_kernel_size>\n",
    "  postnet_n_convolutions: !ref <postnet_n_convolutions>\n",
    "  #decoder_no_early_stopping: !ref <decoder_no_early_stopping>\n",
    "  gate_threshold: !ref <gate_threshold>\n",
    "  padding_idx: !ref <padding_idx>\n",
    "\n",
    "criterion: !new:models.TransformerTTS.Loss\n",
    "  gate_loss_weight: !ref <gate_loss_weight>\n",
    "\n",
    "# Masks\n",
    "lookahead_mask: !name:speechbrain.lobes.models.transformer.Transformer.get_lookahead_mask\n",
    "padding_mask: !name:speechbrain.lobes.models.transformer.Transformer.get_mask_from_lengths\n",
    "\n",
    "modules:\n",
    "  model: !ref <model>\n",
    "\n",
    "#optimizer\n",
    "opt_class: !name:torch.optim.Adam\n",
    "  lr: !ref <learning_rate>\n",
    "  weight_decay: !ref <weight_decay>\n",
    "\n",
    "#epoch object\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "  limit: !ref <epochs>\n",
    "\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "  save_file: !ref <train_log>\n",
    "\n",
    "#annealing_function\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.IntervalScheduler\n",
    "  intervals:\n",
    "    - steps: 6000\n",
    "      lr: 0.0005\n",
    "    - steps: 8000\n",
    "      lr: 0.0003\n",
    "    - steps: 10000\n",
    "      lr: 0.0001\n",
    "\n",
    "#checkpointer\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "  checkpoints_dir: !ref <save_folder>\n",
    "  recoverables:\n",
    "    model: !ref <model>\n",
    "    counter: !ref <epoch_counter>\n",
    "    scheduler: !ref <lr_annealing>\n",
    "\n",
    "infer: !name:models.TransformerTTS.infer\n",
    "\n",
    "progress_sample_logger: !new:speechbrain.utils.train_logger.ProgressSampleLogger\n",
    "  output_path: !ref <progress_sample_path>\n",
    "  batch_sample_size: !ref <progress_batch_sample_size>\n",
    "  formats:\n",
    "    raw_batch: raw\n",
    "\n",
    "input_encoder: !new:speechbrain.dataio.encoder.TextEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7935fb-34c1-481b-9509-98987381653d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Brain Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b31ec953-358d-48cd-bd8d-9ca73d4a88c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /notebooks/train.py\n"
     ]
    }
   ],
   "source": [
    "%%file /notebooks/train.py\n",
    "\n",
    "\"\"\"\n",
    " Recipe for training the TransformerTTS Text-To-Speech model, an end-to-end\n",
    " neural text-to-speech (TTS) system\n",
    "\n",
    " To run this recipe, do the following:\n",
    " # python train.py --device=cuda:0 --max_grad_norm=1.0 --data_folder=/your_folder/LJSpeech-1.1 hparams/train.yaml\n",
    "\n",
    " to infer simply load saved model and do\n",
    " savemodel.infer(text_Sequence,len(textsequence))\n",
    "\n",
    " where text_Sequence is the output of the text_to_sequence function from\n",
    " textToSequence.py (from textToSequence import text_to_sequence)\n",
    "\n",
    " Authors\n",
    " * Georges Abous-Rjeili 2021\n",
    " * Artem Ploujnikov 2021\n",
    " * Yingzhi Wang 2022\n",
    "\"\"\"\n",
    "import torch\n",
    "import speechbrain as sb\n",
    "import sys\n",
    "import logging\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "from speechbrain.inference import GraphemeToPhoneme\n",
    "from speechbrain.utils.text_to_sequence import text_to_sequence, _g2p_keep_punctuations, _clean_text\n",
    "from speechbrain.utils.data_utils import scalarize\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TransformerTTSBrain(sb.Brain):\n",
    "    \"\"\"The Brain implementation for TransformerTTS\"\"\"\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"Gets called at the beginning of ``fit()``, on multiple processes\n",
    "        if ``distributed_count > 0`` and backend is ddp and initializes statistics\n",
    "        \"\"\"\n",
    "        self.hparams.progress_sample_logger.reset()\n",
    "        self.last_epoch = 0\n",
    "        self.last_batch = None\n",
    "        self.last_loss_stats = {}\n",
    "        self.g2p = GraphemeToPhoneme.from_hparams(\"speechbrain/soundchoice-g2p\")\n",
    "        return super().on_fit_start()\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Computes the forward pass\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch: str\n",
    "            a single batch\n",
    "        stage: speechbrain.Stage\n",
    "            the training stage\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the model output\n",
    "        \"\"\"\n",
    "        # Batch is the results of TextMelCollate\n",
    "        effective_batch = self.batch_to_device(batch)\n",
    "\n",
    "        inputs, y, num_items, _, _ = effective_batch\n",
    "\n",
    "        _, input_lengths, mel, _, output_lengths = inputs\n",
    "\n",
    "        # Getting target mask (to avoid looking ahead)\n",
    "        mask_size = mel.shape[2]\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(mask_size, device=self.device)\n",
    "        #tgt_mask = torch.triu(torch.ones(mask_size, mask_size) * float('-inf'), diagonal=1).to(self.device, non_blocking=True)\n",
    "        #tgt_mask = self.hparams.lookahead_mask(mel).to(self.device, non_blocking=True)\n",
    "\n",
    "        # Padding masks for source and targets (use padding_mask)\n",
    "        src_key_padding_mask = self.hparams.padding_mask(input_lengths).to(self.device, non_blocking=True)\n",
    "        \n",
    "        tgt_key_padding_mask = self.hparams.padding_mask(output_lengths).to(self.device, non_blocking=True)\n",
    "        \n",
    "        masks = (tgt_mask, None, src_key_padding_mask, tgt_key_padding_mask)\n",
    "\n",
    "        return self.modules.model(inputs,masks)\n",
    "\n",
    "    def on_fit_batch_end(self, batch, outputs, loss, should_step):\n",
    "        \"\"\"At the end of the optimizer step, apply noam annealing.\"\"\"\n",
    "        if should_step:\n",
    "            self.hparams.lr_annealing(self.optimizer)\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions : torch.Tensor\n",
    "            The model generated spectrograms and other metrics from `compute_forward`.\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            A one-element tensor used for backpropagating the gradient.\n",
    "        \"\"\"\n",
    "        effective_batch = self.batch_to_device(batch)\n",
    "        # Hold on to the batch for the inference sample. This is needed because\n",
    "        # the inference sample is run from on_stage_end only, where\n",
    "        # batch information is not available\n",
    "        self.last_batch = effective_batch\n",
    "        # Hold on to a sample (for logging)\n",
    "        self._remember_sample(effective_batch, predictions)\n",
    "        # Compute the loss\n",
    "        loss = self._compute_loss(predictions, effective_batch, stage)\n",
    "        return loss\n",
    "\n",
    "    def _compute_loss(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the value of the loss function and updates stats\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions: tuple\n",
    "            model predictions\n",
    "        batch: PaddedBatch\n",
    "            Inputs for this training iteration.\n",
    "        stage: sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: torch.Tensor\n",
    "            the loss value\n",
    "        \"\"\"\n",
    "        inputs, targets, num_items, labels, wavs = batch\n",
    "        text_padded, input_lengths, _, max_len, output_lengths = inputs\n",
    "        \n",
    "        mel_target, _ = targets\n",
    "        mel_out, mel_out_postnet, gate_out = predictions\n",
    "        \n",
    "        target=self._get_spectrogram_sample(mel_target),\n",
    "        output=self._get_spectrogram_sample(mel_out),\n",
    "\n",
    "        loss_stats = self.hparams.criterion(\n",
    "            predictions, targets\n",
    "        )\n",
    "        self.last_loss_stats[stage] = scalarize(loss_stats)\n",
    "        return loss_stats.loss\n",
    "\n",
    "    def _remember_sample(self, batch, predictions):\n",
    "        \"\"\"Remembers samples of spectrograms and the batch for logging purposes\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch: tuple\n",
    "            a training batch\n",
    "        predictions: tuple\n",
    "            predictions (raw output of the TransformerTTS model)\n",
    "        \"\"\"\n",
    "        inputs, targets, num_items, labels, wavs = batch\n",
    "        text_padded, input_lengths, _, max_len, output_lengths = inputs\n",
    "        mel_target, _ = targets\n",
    "        mel_out, mel_out_postnet, gate_out = predictions\n",
    "\n",
    "        self.hparams.progress_sample_logger.remember(\n",
    "            target=self._get_spectrogram_sample(mel_target),\n",
    "            output=self._get_spectrogram_sample(mel_out),\n",
    "            output_postnet=self._get_spectrogram_sample(mel_out_postnet),\n",
    "            raw_batch=self.hparams.progress_sample_logger.get_batch_sample(\n",
    "                {\n",
    "                    \"text_padded\": text_padded,\n",
    "                    \"input_lengths\": input_lengths,\n",
    "                    \"mel_target\": mel_target,\n",
    "                    \"mel_out\": mel_out,\n",
    "                    \"mel_out_postnet\": mel_out_postnet,\n",
    "                    \"gate_out\": gate_out,\n",
    "                    \"labels\": labels,\n",
    "                    \"wavs\": wavs,\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def batch_to_device(self, batch):\n",
    "        \"\"\"Transfers the batch to the target device\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch: tuple\n",
    "            the batch to use\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        batch: tuple\n",
    "            the batch on the correct device\n",
    "        \"\"\"\n",
    "        (\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            gate_padded,\n",
    "            output_lengths,\n",
    "            len_x,\n",
    "            labels,\n",
    "            wavs,\n",
    "        ) = batch\n",
    "        text_padded = text_padded.to(self.device, non_blocking=True).long()\n",
    "        input_lengths = input_lengths.to(self.device, non_blocking=True).long()\n",
    "        max_len = torch.max(input_lengths.data).item()\n",
    "        mel_padded = mel_padded.to(self.device, non_blocking=True).float()\n",
    "        gate_padded = gate_padded.to(self.device, non_blocking=True).float()\n",
    "\n",
    "        output_lengths = output_lengths.to(\n",
    "            self.device, non_blocking=True\n",
    "        ).long()\n",
    "        x = (text_padded, input_lengths, mel_padded, max_len, output_lengths)\n",
    "        y = (mel_padded, gate_padded)\n",
    "        len_x = torch.sum(output_lengths)\n",
    "        return (x, y, len_x, labels, wavs)\n",
    "\n",
    "    def _get_spectrogram_sample(self, raw):\n",
    "        \"\"\"Converts a raw spectrogram to one that can be saved as an image\n",
    "        sample  = sqrt(exp(raw))\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        raw: torch.Tensor\n",
    "            the raw spectrogram (as used in the model)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample: torch.Tensor\n",
    "            the spectrogram, for image saving purposes\n",
    "        \"\"\"\n",
    "        sample = raw[0]\n",
    "        return torch.sqrt(torch.exp(sample))\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of an epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
    "        stage_loss : float\n",
    "            The average loss for all of the data processed in this stage.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the train loss until the validation stage\n",
    "        \n",
    "        # At the end of validation, we can write\n",
    "        if stage == sb.Stage.VALID:\n",
    "            # Update learning rate\n",
    "            print(\"Stage End\")\n",
    "            lr = self.optimizer.param_groups[-1][\"lr\"]\n",
    "            self.last_epoch = epoch\n",
    "\n",
    "            # The train_logger writes a summary to stdout and to the logfile.\n",
    "            self.hparams.train_logger.log_stats(  # 1#2#\n",
    "                stats_meta={\"Epoch\": epoch, \"lr\": lr},\n",
    "                train_stats=self.last_loss_stats[sb.Stage.TRAIN],\n",
    "                valid_stats=self.last_loss_stats[sb.Stage.VALID],\n",
    "            )\n",
    "\n",
    "            # Save the current checkpoint and delete previous checkpoints.\n",
    "            epoch_metadata = {\n",
    "                **{\"epoch\": epoch},\n",
    "                **self.last_loss_stats[sb.Stage.VALID],\n",
    "            }\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta=epoch_metadata,\n",
    "                min_keys=[\"loss\"],\n",
    "                ckpt_predicate=(\n",
    "                    (\n",
    "                        lambda ckpt: (\n",
    "                            ckpt.meta[\"epoch\"]\n",
    "                            % self.hparams.keep_checkpoint_interval\n",
    "                            != 0\n",
    "                        )\n",
    "                    )\n",
    "                    if self.hparams.keep_checkpoint_interval is not None\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "            output_progress_sample = (\n",
    "                self.hparams.progress_samples\n",
    "                and epoch % self.hparams.progress_samples_interval == 0\n",
    "            )\n",
    "            # if output_progress_sample:\n",
    "            #     self.run_inference_sample()\n",
    "            #     self.hparams.progress_sample_logger.save(epoch)\n",
    "\n",
    "        # We also write statistics about test data to stdout and to the logfile.\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=self.last_loss_stats[sb.Stage.TEST],\n",
    "            )\n",
    "            if self.hparams.progress_samples:\n",
    "                self.run_inference_sample()\n",
    "                self.hparams.progress_sample_logger.save(\"test\")\n",
    "\n",
    "    def run_inference_sample(self):\n",
    "        \"\"\"Produces a sample in inference mode. This is called when producing\n",
    "        samples and can be useful because\"\"\"\n",
    "        if self.last_batch is None:\n",
    "            return\n",
    "        inputs, _, _, _, _ = self.last_batch\n",
    "        text_padded, input_lengths, _, _, _ = inputs\n",
    "        mel_out, _, _ = self.hparams.model.infer(\n",
    "            text_padded[:1], input_lengths[:1]\n",
    "        )\n",
    "        self.hparams.progress_sample_logger.remember(\n",
    "            inference_mel_out=self._get_spectrogram_sample(mel_out)\n",
    "        )\n",
    "        \n",
    "def dataio_prepare(hparams):\n",
    "    \n",
    "    lexicon = hparams[\"lexicon\"]\n",
    "    input_encoder = hparams.get(\"input_encoder\")\n",
    "    \n",
    "    # add a dummy symbol for idx 0 - used for padding.\n",
    "    lexicon = [\"@@\"] + lexicon\n",
    "    input_encoder.update_from_iterable(lexicon, sequence_input=False)\n",
    "    input_encoder.add_unk()\n",
    "    \n",
    "    # Define audio pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wav\",\"phonemes\")\n",
    "    @sb.utils.data_pipeline.provides(\"mel_text_pair\")\n",
    "    def audio_pipeline(wav, phonemes):\n",
    "        # Calculate the mel spectrogram for the audio files\n",
    "        audio = sb.dataio.dataio.read_audio(wav)\n",
    "        mel = hparams[\"mel_spectogram\"](audio=audio)\n",
    "\n",
    "        # Encode phonemes to get the sequence of IDs corresponding to the symbols in the text.\n",
    "        encoded_phonemes = input_encoder.encode_sequence_torch(phonemes).int()\n",
    "\n",
    "        len_phonemes = len(encoded_phonemes)\n",
    "\n",
    "        return encoded_phonemes, mel, len_phonemes\n",
    "\n",
    "\n",
    "    datasets = {}\n",
    "    data_info = {\n",
    "        \"train\": hparams[\"train_json\"],\n",
    "        \"valid\": hparams[\"valid_json\"],\n",
    "        \"test\": hparams[\"test_json\"],\n",
    "    }\n",
    "    for dataset in hparams[\"splits\"]:\n",
    "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "            json_path=data_info[dataset],\n",
    "            replacements={\"data_root\": hparams[\"data_folder\"]},\n",
    "            dynamic_items=[audio_pipeline],\n",
    "            output_keys=[\"mel_text_pair\", \"wav\", \"label\"],\n",
    "        )\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load hyperparameters file with command-line overrides\n",
    "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
    "\n",
    "    with open(hparams_file) as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    # create ddp_group with the right communication protocol\n",
    "    sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "    # Create experiment directory\n",
    "    sb.create_experiment_directory(\n",
    "        experiment_directory=hparams[\"output_folder\"],\n",
    "        hyperparams_to_save=hparams_file,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "    from ljspeech_prepare import prepare_ljspeech\n",
    "\n",
    "    sb.utils.distributed.run_on_main(\n",
    "        prepare_ljspeech,\n",
    "        kwargs={\n",
    "            \"model_name\": hparams[\"model_name\"],\n",
    "            \"data_folder\": hparams[\"data_folder\"],\n",
    "            \"save_folder\": hparams[\"save_folder\"],\n",
    "            \"splits\": hparams[\"splits\"],\n",
    "            \"split_ratio\": hparams[\"split_ratio\"],\n",
    "            \"seed\": hparams[\"seed\"],\n",
    "            \"skip_prep\": hparams[\"skip_prep\"],\n",
    "            \"device\": run_opts[\"device\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    datasets = dataio_prepare(hparams)\n",
    "    \n",
    "    #print(hparams)\n",
    "\n",
    "    # Brain class initialization\n",
    "    TransformerTTS_brain = TransformerTTSBrain(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"opt_class\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    TransformerTTS_brain.fit(\n",
    "        TransformerTTS_brain.hparams.epoch_counter,\n",
    "        train_set=datasets[\"train\"],\n",
    "        valid_set=datasets[\"valid\"],\n",
    "        train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "        valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    "    )\n",
    "\n",
    "    # Test\n",
    "    if \"test\" in datasets:\n",
    "        TransformerTTS_brain.evaluate(\n",
    "            datasets[\"test\"],\n",
    "            test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293c849-ef8e-47cb-a44d-ba4fb9b3a39d",
   "metadata": {},
   "source": [
    "### Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7ae24",
   "metadata": {},
   "source": [
    "Script used to remove previous checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf0f6d24-78ea-48de-acae-32167b523e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /notebooks/results/transformertts/1234/save/CKPT+2024-04-27+06-43-30+00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d950050",
   "metadata": {},
   "source": [
    "Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9bb330-b9f0-4b45-be8a-edc0039c66dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/transformertts/1234\n",
      "ljspeech_prepare - Skipping preparation, completed in previous run.\n",
      "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
      "speechbrain.core - 26.0M trainable parameters in TransformerTTSBrain\n",
      "speechbrain.utils.fetching - Fetch hyperparams.yaml: Using existing file/symlink in pretrained_models/GraphemeToPhoneme-9b27d6eb840bf95c5aedf15ae8ed1172/hyperparams.yaml.\n",
      "speechbrain.utils.fetching - Fetch custom.py: Delegating to Huggingface hub, source speechbrain/soundchoice-g2p.\n",
      "speechbrain.utils.fetching - Fetch model.ckpt: Using existing file/symlink in pretrained_models/GraphemeToPhoneme-9b27d6eb840bf95c5aedf15ae8ed1172/model.ckpt.\n",
      "speechbrain.utils.fetching - Fetch ctc_lin.ckpt: Using existing file/symlink in pretrained_models/GraphemeToPhoneme-9b27d6eb840bf95c5aedf15ae8ed1172/ctc_lin.ckpt.\n",
      "speechbrain.utils.parameter_transfer - Loading pretrained files for: model, ctc_lin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/transformertts/1234/save/CKPT+2024-04-27+20-50-07+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 30\n",
      "  0%|                                                   | 0/367 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "  0%|                          | 1/367 [00:04<29:12,  4.79s/it, train_loss=9.94]/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "100%|| 367/367 [02:15<00:00,  2.70it/s, train_loss=9.39]\n",
      "100%|| 42/42 [00:06<00:00,  6.15it/s]\n",
      "Stage End\n",
      "speechbrain.utils.train_logger - Epoch: 30, lr: 1.00e-04 - train loss: 9.62, train mel_loss: 9.58, train gate_loss: 4.41e-02 - valid loss: 17.25, valid mel_loss: 17.18, valid gate_loss: 6.72e-02\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformertts/1234/save/CKPT+2024-04-27+20-52-59+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformertts/1234/save/CKPT+2024-04-27+20-50-07+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 31\n",
      "100%|| 367/367 [02:11<00:00,  2.79it/s, train_loss=9.38]\n",
      "100%|| 42/42 [00:06<00:00,  6.29it/s]\n",
      "Stage End\n",
      "speechbrain.utils.train_logger - Epoch: 31, lr: 1.00e-04 - train loss: 10.16, train mel_loss: 10.11, train gate_loss: 4.52e-02 - valid loss: 17.71, valid mel_loss: 17.65, valid gate_loss: 6.74e-02\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformertts/1234/save/CKPT+2024-04-27+20-55-18+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformertts/1234/save/CKPT+2024-04-27+20-52-59+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 32\n",
      "100%|| 367/367 [02:12<00:00,  2.78it/s, train_loss=9.36]\n",
      "100%|| 42/42 [00:06<00:00,  6.39it/s]\n",
      "Stage End\n",
      "speechbrain.utils.train_logger - Epoch: 32, lr: 1.00e-04 - train loss: 9.54, train mel_loss: 9.50, train gate_loss: 4.36e-02 - valid loss: 17.87, valid mel_loss: 17.80, valid gate_loss: 6.69e-02\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformertts/1234/save/CKPT+2024-04-27+20-57-37+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformertts/1234/save/CKPT+2024-04-27+20-55-18+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 33\n",
      "100%|| 367/367 [02:11<00:00,  2.79it/s, train_loss=9.37]\n",
      "100%|| 42/42 [00:06<00:00,  6.14it/s]\n",
      "Stage End\n",
      "speechbrain.utils.train_logger - Epoch: 35, lr: 1.00e-04 - train loss: 9.61, train mel_loss: 9.57, train gate_loss: 4.39e-02 - valid loss: 18.08, valid mel_loss: 18.02, valid gate_loss: 6.72e-02\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformertts/1234/save/CKPT+2024-04-27+21-04-33+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformertts/1234/save/CKPT+2024-04-27+21-02-14+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 36\n",
      "100%|| 367/367 [02:11<00:00,  2.78it/s, train_loss=9.34]\n",
      "100%|| 42/42 [00:06<00:00,  6.25it/s]\n",
      "Stage End\n",
      "speechbrain.utils.train_logger - Epoch: 37, lr: 1.00e-04 - train loss: 9.43, train mel_loss: 9.38, train gate_loss: 4.30e-02 - valid loss: 18.14, valid mel_loss: 18.07, valid gate_loss: 6.65e-02\n"
     ]
    }
   ],
   "source": [
    "!python train.py --device=cuda:0 --max_grad_norm=1.0 --data_folder=/notebooks/LJSpeech-1.1 train.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
